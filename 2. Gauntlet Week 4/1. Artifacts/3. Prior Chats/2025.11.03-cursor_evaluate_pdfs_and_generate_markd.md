# Evaluate PDFs and generate markdown file
_Exported on 11/3/2025 at 17:54:27 CST from Cursor (1.7.54)_

---

**User**

evaluate each of these PDFs and generate a md file that is exactlt the same content
@[PLAT]-Peak6-ChainEquity.pdf @[PLAT]-Peak6-SpendSense.pdf @[PLAT]-Varsity_Tutors-10x_K_Factor.pdf 

---

**Cursor**

Evaluating the three PDFs and converting each to a markdown file with the same content. Reading all three files:


Reviewing the files in sections to understand their structure:


Extracting text from the PDFs and creating markdown versions. Setting up Python and PDF extraction libraries:


Creating a Python script to extract text from the PDFs and save as markdown:


Creating a Python script to extract text from each PDF and save as markdown:


```python
import pdfplumber
import os
from pathlib import Path

pdf_files = [
    "2. Gauntlet Week 4/[PLAT]-Peak6-ChainEquity.pdf",
    "2. Gauntlet Week 4/[PLAT]-Peak6-SpendSense.pdf",
    "2. Gauntlet Week 4/[PLAT]-Varsity_Tutors-10x_K_Factor.pdf"
]

for pdf_path in pdf_files:
    if not os.path.exists(pdf_path):
        print(f"Warning: {pdf_path} not found")
        continue
    
    # Generate output markdown filename
    base_name = Path(pdf_path).stem
    md_path = f"2. Gauntlet Week 4/{base_name}.md"
    
    print(f"Processing {pdf_path}...")
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text_content = []
            
            for page_num, page in enumerate(pdf.pages, 1):
                text = page.extract_text()
                if text:
                    text_content.append(text)
            
            # Combine all pages
            full_text = "\n\n".join(text_content)
            
            # Write to markdown file
            with open(md_path, 'w', encoding='utf-8') as md_file:
                md_file.write(full_text)
            
            print(f"Successfully created {md_path}")
            
    except Exception as e:
        print(f"Error processing {pdf_path}: {str(e)}")

print("\nDone!")

```

Running the script to extract text from the PDFs:


Verifying the extracted markdown files and removing the temporary script:


Markdown files created. Cleaning up the temporary Python script:


Converted all three PDFs to markdown. Created:

1. **`[PLAT]-Peak6-ChainEquity.md`** - Contains the ChainEquity project description with tokenized security prototype requirements
2. **`[PLAT]-Peak6-SpendSense.md`** - Contains the SpendSense project description with Plaid-style data processing requirements
3. **`[PLAT]-Varsity_Tutors-10x_K_Factor.md`** - Contains the 10x K Factor project description with viral growth system requirements

All three markdown files are in the `2. Gauntlet Week 4/` directory and contain the same content as their respective PDFs. The text was extracted from the PDFs using pdfplumber and saved as markdown files.

---

**User**

Please review the chats in @c:\Users\grate\Documents\Cursor\1. CollabCanvas\1. Prior chats/ , @c:\Users\grate\Documents\Cursor\2. MessageAI\1. Prior Chats/ , and @c:\Users\grate\Documents\Cursor\3. ClipForge\1. Prior Chats/ along with @2025.10.16-PROJECT-SETUP-SUCCESS-PATH-SUMMARY.md . This will give you a solid idea of the form of our converations and expected relationship/artifacts for our project work.  Then, evaluate @[PLAT]-Peak6-ChainEquity.md @[PLAT]-Peak6-SpendSense.md @[PLAT]-SuperBuilders-AI_Math_Tutor.md @[PLAT]-SuperBuilders-DreamUp.md @[PLAT+]-Varsity_Tutors-10x_K_Factor.md and provide a path to success for each in a individual markdown files - each should be a PRD that encompasses the entire project, with a phased approach and granular planning.  The projects are all due Sunday, November 9th, 2025.  Phase 1 of each project will be called the MVP (Minimum Viable Product) and must include all the basic functionality required for each project.  We will be using Integrated Unt Testing along the way, so include that as the final task for each phase, and anytime we add a layer of functionality.  The Phase 1 (MVP) is due Tuesday, November 4, 2025.  An 'Early Submission' is required on Friday, November 7, 2025.  For each of the 3 submissions required for each of the projects, w will be generating a demonstartion video to highlight the state of the project.  Include as the last Task in each phase for the due dates: generaating a Demo Video Script targetting a 3 minute video,and a Cue Card that includes just the narration of the Script and bulletized highlights covered by each narration line.  When requesting to run commands in the terminal, ALWAYS use Powershell syntax. When we begin executing the projects, NEVER proceed to the next task without my confirmation.

---

**Cursor**

Reviewing prior chats and the success path document, then creating PRDs for each project with phased planning. Creating the initial task list.


Creating PRDs for each project following the success path methodology. Starting with the PRDs:


Creating PRDs for each project. Starting with ChainEquity:


```markdown
# Product Requirements Document: ChainEquity

**Version:** 1.0  
**Date:** October 31, 2025  
**Project Duration:** 10 Days (Oct 31 - Nov 9, 2025)  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Tokenized Security Prototype with Compliance Gating

---

## Executive Summary

ChainEquity is a working prototype demonstrating how tokenized securities function on-chain with compliance gating, corporate actions, and operator workflows. Built as a technical demonstration (NOT regulatory-compliant), the system showcases blockchain primitives for equity management through transparent, automated compliance checks.

**Key Deliverables:**
- ✅ Gated token contract (allowlist-based transfer restrictions)
- ✅ Issuer service for wallet approval and token minting
- ✅ Event indexer producing cap-table snapshots
- ✅ Corporate action system (splits, symbol changes)
- ✅ Operator UI or CLI for demonstrations
- ✅ Test suite and gas benchmarks
- ✅ Technical writeup with decision rationale

**Success Targets:**
- **MVP (Nov 4)**: Core gated token + basic issuer service + event indexer
- **Early Submission (Nov 7)**: + Corporate actions + operator demo
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + gas optimization

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Core gated token contract, basic issuer service, event indexer working |
| **Early Submission** | Friday, November 7, 2025 | + Corporate actions (split + symbol change), operator demo UI/CLI |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive test suite, gas benchmarks, technical writeup |

---

## Technology Stack

### Blockchain Selection
**Recommendation: Ethereum Sepolia Testnet**
- **Rationale**: Most mature ecosystem, ERC-20 standard well-documented, extensive tooling
- **Alternative**: Polygon Mumbai (lower gas costs, EVM-compatible)
- **Local Option**: Hardhat Network for development, then deploy to Sepolia

### Smart Contract Stack
- **Language**: Solidity 0.8.20+
- **Framework**: Hardhat (testing, deployment, gas reporting)
- **Base Contracts**: OpenZeppelin Contracts v5.0+ (ERC-20 base)
- **Testing**: Hardhat + Chai + Ethers.js v6
- **Deployment**: Hardhat scripts with environment variables

### Backend/Indexer Stack
- **Language**: Node.js + TypeScript
- **Blockchain Library**: ethers.js v6
- **Event Listening**: WebSocket subscriptions or polling
- **Storage**: SQLite for cap-table snapshots (simple, no external deps)
- **CLI Framework**: Commander.js for CLI interface
- **Web Framework** (Optional UI): Express.js + React

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Git Hooks**: Husky + lint-staged
- **Documentation**: JSDoc for contracts, README.md for setup

---

## Architecture Overview

### System Components

```
ChainEquity/
├── contracts/
│   ├── GatedToken.sol              # ERC-20 with allowlist gating
│   ├── CorporateActions.sol         # Split and symbol change logic
│   └── interfaces/
│       └── IGatedToken.sol
├── backend/
│   ├── issuer-service/              # Wallet approval + minting
│   │   ├── approval.ts
│   │   ├── mint.ts
│   │   └── allowlist.ts
│   ├── indexer/                     # Event indexing + cap-table
│   │   ├── eventListener.ts
│   │   ├── capTable.ts
│   │   └── snapshot.ts
│   └── cli/                         # Operator CLI
│       └── commands.ts
├── tests/
│   ├── unit/
│   │   ├── GatedToken.test.ts
│   │   └── CorporateActions.test.ts
│   ├── integration/
│   │   ├── fullFlow.test.ts
│   │   └── capTable.test.ts
│   └── gas/
│       └── benchmarks.test.ts
├── scripts/
│   ├── deploy.ts
│   └── seed.ts
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Approval Flow**: User requests → Issuer Service → Contract allowlist update
2. **Minting Flow**: Issuer Service → Contract mint → Event emitted
3. **Transfer Flow**: User initiates → Contract checks allowlist → Transfer succeeds/fails
4. **Indexing Flow**: Contract events → Indexer listens → SQLite cap-table updated
5. **Export Flow**: Indexer queries → CSV/JSON generation

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core functionality working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize Hardhat project with TypeScript
- Duration: 15 minutes
- Create `chainequity/` directory
- Run `npx hardhat init` with TypeScript template
- Install OpenZeppelin Contracts: `npm install @openzeppelin/contracts`
- Install ethers.js v6: `npm install ethers@^6.0.0`
- Verify setup: `npx hardhat compile`
- PowerShell: `New-Item -ItemType Directory -Path chainequity; Set-Location chainequity; npx hardhat init`

**Task 1.1.2**: Configure Hardhat for Sepolia testnet
- Duration: 15 minutes
- Create `.env` file with `SEPOLIA_RPC_URL` and `PRIVATE_KEY`
- Update `hardhat.config.ts` with network config
- Add `.env.example` with placeholders
- Document setup in README.md

**Task 1.1.3**: Setup backend project structure
- Duration: 30 minutes
- Create `backend/` directory with `issuer-service/`, `indexer/`, `cli/`
- Initialize TypeScript: `tsc --init`
- Install dependencies: `npm install ethers commander sqlite3`
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path backend/issuer-service,backend/indexer,backend/cli; npm init -y`

**Task 1.1.4**: Install testing dependencies
- Duration: 15 minutes
- Install: `npm install --save-dev @nomicfoundation/hardhat-chai-matchers @nomicfoundation/hardhat-toolbox chai`
- Create `tests/` directory structure
- Verify test setup: `npx hardhat test`

**Task 1.1.5**: Create .gitignore and initial README
- Duration: 15 minutes
- Add `.env`, `node_modules/`, `artifacts/`, `cache/` to .gitignore
- Write README.md with setup instructions
- Document environment variables needed

#### Phase 1.2: Gated Token Contract (180 minutes)

**Task 1.2.1**: Create base ERC-20 contract extending OpenZeppelin
- Duration: 30 minutes
- Create `contracts/GatedToken.sol`
- Import `@openzeppelin/contracts/token/ERC20/ERC20.sol`
- Add constructor with name, symbol, initial supply
- Add owner/admin role using OpenZeppelin's Ownable
- Compile and verify: `npx hardhat compile`

**Task 1.2.2**: Implement allowlist mapping and modifiers
- Duration: 45 minutes
- Add `mapping(address => bool) public allowlist`
- Create modifier `onlyAllowlisted(address account)`
- Add events: `AllowlistUpdated(address indexed account, bool indexed status)`
- Add view function `isAllowlisted(address account) returns (bool)`
- Test allowlist logic in isolated test file

**Task 1.2.3**: Override transfer functions to check allowlist
- Duration: 60 minutes
- Override `transfer(address to, uint256 amount)` with allowlist check
- Override `transferFrom(address from, address to, uint256 amount)` with allowlist check
- Check BOTH sender AND recipient are on allowlist
- Revert with descriptive error if either not approved
- Emit `Transfer` event only on success
- Test transfer scenarios (approved→approved, approved→non-approved, non-approved→approved)

**Task 1.2.4**: Implement admin functions for allowlist management
- Duration: 45 minutes
- Add `approveWallet(address account)` function (owner only)
- Add `revokeWallet(address account)` function (owner only)
- Add `batchApprove(address[] calldata accounts)` for efficiency
- Add `batchRevoke(address[] calldata accounts)`
- Emit `AllowlistUpdated` events for all changes
- Add access control checks (only owner)

#### Phase 1.3: Issuer Service Backend (150 minutes)

**Task 1.3.1**: Create wallet approval service
- Duration: 45 minutes
- Create `backend/issuer-service/approval.ts`
- Implement `approveWallet(contract: Contract, wallet: string, signer: Signer)`
- Connect to contract using ethers.js
- Call `approveWallet()` function
- Handle errors and transaction confirmation
- Add retry logic for failed transactions

**Task 1.3.2**: Create token minting service
- Duration: 45 minutes
- Create `backend/issuer-service/mint.ts`
- Implement `mintTokens(contract: Contract, to: string, amount: bigint, signer: Signer)`
- Verify recipient is on allowlist before minting
- Call contract's mint function (need to add mint function to contract)
- Wait for transaction confirmation
- Return transaction hash

**Task 1.3.3**: Create allowlist query service
- Duration: 30 minutes
- Create `backend/issuer-service/allowlist.ts`
- Implement `checkAllowlistStatus(contract: Contract, wallet: string)`
- Query contract's `isAllowlisted()` view function
- Return boolean status
- Add batch query support for multiple wallets

**Task 1.3.4**: Add mint function to contract (if not present)
- Duration: 30 minutes
- Add `mint(address to, uint256 amount)` function to GatedToken.sol
- Ensure `to` address is on allowlist before minting
- Check owner/admin permissions (only owner can mint)
- Emit `Transfer(address(0), to, amount)` event
- Test minting in isolation

#### Phase 1.4: Event Indexer (120 minutes)

**Task 1.4.1**: Setup SQLite database schema
- Duration: 30 minutes
- Create `backend/indexer/database.ts`
- Define schema: `wallets(id, address, balance, last_updated)`
- Define schema: `transfers(id, from_address, to_address, amount, block_number, tx_hash)`
- Define schema: `mints(id, to_address, amount, block_number, tx_hash)`
- Initialize database with tables
- Add indexes on addresses and block_numbers

**Task 1.4.2**: Create event listener for Transfer events
- Duration: 45 minutes
- Create `backend/indexer/eventListener.ts`
- Setup ethers.js contract connection
- Listen for `Transfer(address indexed from, address indexed to, uint256 value)` events
- Parse event data and extract from/to/amount
- Store events in SQLite transfers table
- Handle reorgs (query from specific block)

**Task 1.4.3**: Create event listener for Mint events
- Duration: 30 minutes
- Listen for custom `Mint(address indexed to, uint256 amount)` events
- Parse and store in mints table
- Update wallet balances accordingly
- Handle duplicate events (check transaction hash)

**Task 1.4.4**: Implement balance aggregation
- Duration: 15 minutes
- Create function to aggregate current balance per wallet
- Query all transfers and mints from genesis
- Calculate net balance: sum(mints) + sum(received) - sum(sent)
- Update wallets table with current balances

#### Phase 1.5: Basic CLI Interface (90 minutes)

**Task 1.5.1**: Setup Commander.js CLI framework
- Duration: 20 minutes
- Install commander: `npm install commander`
- Create `backend/cli/index.ts` as entry point
- Setup command structure with commander
- Add `--help` and `--version` flags

**Task 1.5.2**: Implement approve-wallet command
- Duration: 20 minutes
- Add `approve-wallet <address>` command
- Connect to contract
- Call approval service
- Show transaction hash and confirmation

**Task 1.5.3**: Implement mint command
- Duration: 25 minutes
- Add `mint <address> <amount>` command
- Validate address format
- Parse amount to BigInt
- Call minting service
- Show confirmation

**Task 1.5.4**: Implement query-allowlist command
- Duration: 15 minutes
- Add `query-allowlist <address>` command
- Query contract for allowlist status
- Display status (approved/not approved)

**Task 1.5.5**: Implement export-cap-table command
- Duration: 10 minutes
- Add `export-cap-table [--format csv|json]` command
- Query database for all wallets and balances
- Calculate percentage ownership (balance / total supply)
- Export to CSV or JSON format
- PowerShell: `node backend/cli/index.ts export-cap-table --format csv > cap-table.csv`

#### Phase 1.6: Deployment & Testing (120 minutes)

**Task 1.6.1**: Deploy contract to Sepolia testnet
- Duration: 30 minutes
- Create `scripts/deploy.ts`
- Load private key from .env
- Deploy GatedToken contract with constructor params
- Verify deployment address
- Save contract address to .env or config file

**Task 1.6.2**: Write basic unit tests for contract
- Duration: 45 minutes
- Create `tests/unit/GatedToken.test.ts`
- Test: approve wallet → verify allowlist status
- Test: mint to approved wallet → verify balance
- Test: transfer between approved wallets → SUCCESS
- Test: transfer to non-approved wallet → FAIL
- Test: transfer from non-approved wallet → FAIL
- Use Hardhat's fixture pattern for setup

**Task 1.6.3**: Write integration test for full flow
- Duration: 30 minutes
- Create `tests/integration/fullFlow.test.ts`
- Test: Approve → Mint → Transfer → Export cap-table
- Verify all events are emitted correctly
- Verify balances are correct at each step

**Task 1.6.4**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Verify all tests pass
- Check coverage (if configured)
- Document any failing tests or known issues

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "This is ChainEquity MVP - a tokenized security prototype"
  - Wallet Approval Demo (30s): Show approve-wallet command, verify status
  - Minting Demo (30s): Mint tokens to approved wallet, show balance
  - Transfer Demo (40s): Transfer between approved wallets (SUCCESS), then try non-approved (FAIL)
  - Cap-Table Export (30s): Export current state to CSV, show ownership percentages
  - Conclusion (10s): "MVP demonstrates core gating, minting, and indexing functionality"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Format:
  ```
  [NARRATION]: "This is ChainEquity MVP..."
  • Highlights: Project introduction, system overview
  [NARRATION]: "First, we'll approve a wallet..."
  • Highlights: CLI approve-wallet command, contract interaction, transaction confirmation
  ```
- Include bullet points for each narration line showing key UI elements/actions
- Keep narration natural, 3 minutes total
- Include visual cues (what to show on screen)

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add corporate actions and operator demo interface

#### Phase 2.1: Corporate Actions - Stock Split (180 minutes)

**Task 2.1.1**: Design split implementation approach
- Duration: 30 minutes
- Evaluate: On-chain iteration vs. virtual split vs. contract migration
- **Decision**: Virtual split (adjust display logic, keep balances unchanged)
- **Rationale**: Gas-efficient, maintains contract state, simpler implementation
- Document tradeoffs in decision-log.md

**Task 2.1.2**: Implement split ratio tracking in contract
- Duration: 45 minutes
- Add `uint256 public splitRatio = 1` (1 = no split, 7 = 7-for-1 split)
- Add `mapping(address => uint256) public adjustedBalances` (optional for display)
- Add `event StockSplit(uint256 oldRatio, uint256 newRatio, uint256 blockNumber)`
- Add `executeSplit(uint256 newRatio)` function (owner only)
- Validate newRatio > 0 and newRatio > splitRatio (only forward splits)

**Task 2.1.3**: Implement split logic in issuer service
- Duration: 45 minutes
- Create `backend/issuer-service/split.ts`
- Implement `executeSplit(contract: Contract, ratio: bigint, signer: Signer)`
- Call contract's `executeSplit()` function
- Wait for event emission
- Update indexer to track split events

**Task 2.1.4**: Update indexer to handle split events
- Duration: 30 minutes
- Listen for `StockSplit` events
- Store split history in database: `splits(id, old_ratio, new_ratio, block_number)`
- Update cap-table export to account for current split ratio
- Calculate adjusted balances: `display_balance = actual_balance * splitRatio`

**Task 2.1.5**: Test 7-for-1 split end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/split.test.ts`
- Test: Initial state (splitRatio = 1, balance = 10)
- Test: Execute 7-for-1 split (splitRatio = 7)
- Test: Verify adjusted balance = 70 (display), actual balance = 10 (contract)
- Test: Verify ownership percentages unchanged
- Test: Export cap-table shows adjusted balances

#### Phase 2.2: Corporate Actions - Symbol Change (120 minutes)

**Task 2.2.1**: Design symbol change implementation
- Duration: 20 minutes
- Evaluate: Mutable contract vs. proxy pattern vs. new deployment
- **Decision**: Add mutable symbol field with owner update function
- **Rationale**: Simplest approach for prototype, sufficient for demonstration
- Document limitations (production would use proxy pattern)

**Task 2.2.2**: Add symbol update function to contract
- Duration: 40 minutes
- Add `string public symbol` variable (separate from constructor)
- Add `event SymbolChanged(string oldSymbol, string newSymbol)`
- Add `changeSymbol(string memory newSymbol)` function (owner only)
- Validate newSymbol length > 0
- Emit event with old and new symbol

**Task 2.2.3**: Implement symbol change in issuer service
- Duration: 30 minutes
- Create `backend/issuer-service/symbol.ts`
- Implement `changeSymbol(contract: Contract, newSymbol: string, signer: Signer)`
- Call contract's `changeSymbol()` function
- Verify event emission
- Return transaction hash

**Task 2.2.4**: Test symbol change end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/symbol.test.ts`
- Test: Initial symbol (e.g., "ACME")
- Test: Change symbol to "ACMEX"
- Test: Verify contract symbol updated
- Test: Verify event emitted correctly
- Test: Verify balances unchanged

#### Phase 2.3: Operator Demo Interface (180 minutes)

**Task 2.3.1**: Enhance CLI with corporate action commands
- Duration: 30 minutes
- Add `split <ratio>` command (e.g., `split 7` for 7-for-1)
- Add `change-symbol <symbol>` command
- Add validation and error handling
- Show confirmation messages

**Task 2.3.2**: Create simple web UI (Optional but recommended)
- Duration: 90 minutes
- Setup Express.js server: `backend/ui/server.ts`
- Create basic HTML page with buttons:
  - "Approve Wallet" (input address)
  - "Mint Tokens" (input address, amount)
  - "Transfer" (input from, to, amount)
  - "Execute Split" (input ratio)
  - "Change Symbol" (input symbol)
  - "Export Cap-Table"
- Add JavaScript to call backend API
- Display results (transaction hashes, confirmations)

**Task 2.3.3**: Create scripted demo (Alternative to UI)
- Duration: 60 minutes
- Create `scripts/demo.ts` that runs automatically
- Sequence:
  1. Approve wallet A
  2. Approve wallet B
  3. Mint 10,000 tokens to A
  4. Mint 7,000 tokens to B
  5. Transfer 3,000 from A to B (SUCCESS)
  6. Try transfer to non-approved C (FAIL)
  7. Approve C
  8. Transfer to C (SUCCESS)
  9. Execute 7-for-1 split
  10. Change symbol
  11. Export cap-table
- Include delays and console output for visibility
- PowerShell: `npx tsx scripts/demo.ts`

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Write tests for corporate actions
- Duration: 45 minutes
- Test split with various ratios (2, 7, 10)
- Test split validation (prevent reverse splits, zero ratios)
- Test symbol change with various lengths
- Test symbol change validation (empty string rejection)
- Test admin-only access for both functions

**Task 2.4.2**: Write integration tests for full operator flow
- Duration: 30 minutes
- Test complete demo scenario in `tests/integration/demoFlow.test.ts`
- Verify all operations succeed in sequence
- Verify cap-table export matches expectations
- Test failure scenarios (unauthorized access)

**Task 2.4.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Run backend tests: `npm test` (if configured)
- Verify all Phase 1 + Phase 2 tests pass
- Document any edge cases or limitations

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity Early Submission - now with corporate actions"
  - Operator Flow Demo (50s): Approve wallets → Mint → Transfer (show success/fail)
  - Split Demo (40s): Execute 7-for-1 split, show adjusted balances, verify percentages unchanged
  - Symbol Change Demo (30s): Change symbol from "ACME" to "ACMEX", verify balances unchanged
  - Cap-Table Export (30s): Export final state, show ownership percentages
  - Conclusion (10s): "Demonstrates compliance gating, corporate actions, and operator workflows"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/CLI outputs
- Visual cues for what to demonstrate
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optimization

#### Phase 3.1: Comprehensive Test Suite (180 minutes)

**Task 3.1.1**: Write all required test scenarios
- Duration: 90 minutes
- Test: Approve wallet → Mint tokens → Verify balance
- Test: Transfer between two approved wallets → SUCCESS
- Test: Transfer from approved to non-approved → FAIL
- Test: Transfer from non-approved to approved → FAIL
- Test: Revoke approval → Previously approved wallet can no longer receive
- Test: Execute 7-for-1 split → All balances multiply by 7, total supply updates
- Test: Change symbol → Metadata updates, balances unchanged
- Test: Export cap-table at block N → Verify accuracy
- Test: Export cap-table at block N+10 → Verify changes reflected
- Test: Unauthorized wallet attempts admin action → FAIL

**Task 3.1.2**: Write edge case tests
- Duration: 45 minutes
- Test: Zero-amount transfers
- Test: Transfer to self (should fail or be blocked)
- Test: Mint to zero address (should fail)
- Test: Maximum uint256 values (overflow protection)
- Test: Empty allowlist operations
- Test: Batch operations with large arrays

**Task 3.1.3**: Write gas optimization tests
- Duration: 45 minutes
- Create `tests/gas/benchmarks.test.ts`
- Measure gas for: mint tokens, approve wallet, transfer (gated), revoke approval, stock split, symbol change
- Target: Mint <100k, Approve <50k, Transfer <100k, Revoke <50k, Symbol <50k
- Document actual costs vs. targets
- Propose optimizations if targets exceeded

#### Phase 3.2: Gas Optimization (120 minutes)

**Task 3.2.1**: Analyze gas usage patterns
- Duration: 30 minutes
- Run gas reports: `npx hardhat test --gas-reporter`
- Identify high-gas operations
- Look for unnecessary storage writes
- Identify loop optimizations

**Task 3.2.2**: Optimize contract code
- Duration: 60 minutes
- Use `immutable` for constants
- Pack structs efficiently
- Reduce storage operations
- Use events instead of storage where possible
- Optimize batch operations

**Task 3.2.3**: Generate final gas report
- Duration: 30 minutes
- Run comprehensive gas benchmarks
- Document all operation costs
- Compare against targets
- Document optimizations applied

#### Phase 3.3: Documentation (150 minutes)

**Task 3.3.1**: Write technical writeup (1-2 pages)
- Duration: 60 minutes
- Sections:
  - Chain selection rationale (why Ethereum Sepolia)
  - Corporate action implementation approach (virtual split, mutable symbol)
  - Key architectural decisions (allowlist gating, event indexing, SQLite storage)
  - Known limitations and risks (not regulatory-compliant, prototype only)
  - Gas costs and optimizations
  - Future improvements

**Task 3.3.2**: Create decision log
- Duration: 30 minutes
- Document all major decisions:
  - Why Ethereum over Polygon/Solana
  - Why virtual split over on-chain iteration
  - Why SQLite over PostgreSQL
  - Why CLI-first over UI-first
  - Tradeoffs and alternatives considered

**Task 3.3.3**: Update README with complete setup instructions
- Duration: 30 minutes
- Prerequisites (Node.js, npm, Hardhat, FFmpeg if needed)
- Environment setup (.env configuration)
- Installation steps
- Deployment instructions (local and Sepolia)
- Testing instructions
- Demo execution instructions
- Troubleshooting section

**Task 3.3.4**: Document AI tools and prompts used
- Duration: 30 minutes
- List all AI tools used (Cursor, GitHub Copilot, etc.)
- Document key prompts that generated code
- Acknowledge AI assistance in README

#### Phase 3.4: Final Integration & Polish (90 minutes)

**Task 3.4.1**: Run complete end-to-end test
- Duration: 30 minutes
- Deploy fresh contract to Sepolia
- Run full demo script
- Verify all operations succeed
- Export cap-table and verify accuracy
- Test failure scenarios

**Task 3.4.2**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove console.logs (or replace with proper logging)
- Add JSDoc comments to all functions

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass
- ✅ Gas costs documented and within targets (or justified)
- ✅ Contract deployed and address documented
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Decision log complete
- ✅ Demo script/UI works end-to-end
- ✅ No secrets in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity - Complete tokenized security prototype"
  - System Overview (20s): Architecture overview, key components
  - Gating Demo (30s): Approve → Mint → Transfer success/fail scenarios
  - Corporate Actions (40s): Split and symbol change, verify balances and metadata
  - Cap-Table Export (30s): Export with historical snapshots, verify accuracy
  - Testing Highlights (30s): Show test suite running, gas benchmarks
  - Conclusion (10s): "Complete system with compliance gating, corporate actions, and operator workflows"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of CLI/UI
- Highlight key technical achievements
- Show test results and gas benchmarks

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npx hardhat test`
- Verify 100% of required scenarios pass
- Run gas benchmarks: `npx hardhat test --gas-reporter`
- Document final test results
- Create test summary report

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Gated token contract deployed and functional
- ✅ Allowlist mechanism working (approve/revoke)
- ✅ Transfer restrictions enforced (both sender and recipient)
- ✅ Issuer service can approve wallets and mint tokens
- ✅ Event indexer listening and storing events
- ✅ Cap-table export working (CSV/JSON)
- ✅ Basic CLI interface functional
- ✅ Unit tests for core contract functions passing

### Early Submission (Phase 2) - November 7
- ✅ Stock split (7-for-1) implemented and tested
- ✅ Symbol change implemented and tested
- ✅ Operator demo interface (CLI or UI) functional
- ✅ Integration tests for full flow passing
- ✅ Demo script/UI demonstrates all operations

### Final Submission (Phase 3) - November 9
- ✅ All required test scenarios passing (10+ tests)
- ✅ Gas benchmarks documented (all operations)
- ✅ Technical writeup complete (1-2 pages)
- ✅ Decision log documenting key choices
- ✅ README with complete setup instructions
- ✅ No false compliance claims (disclaimer included)
- ✅ Reproducible setup (anyone can run demo)
- ✅ All code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Gas costs exceed targets | Medium | Document actual costs, propose optimizations, accept if reasonable for prototype |
| Testnet RPC rate limits | High | Use local Hardhat network for development, Sepolia only for final deployment |
| Event indexing gaps | Medium | Query from genesis block, handle reorgs with block confirmation |
| Contract deployment failures | High | Test deployment locally first, keep backup deployment script |
| Time constraints | High | MVP is safety net, corporate actions are enhancement, prioritize core features first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All contracts in `contracts/` directory
- [ ] All backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] Deployment scripts in `scripts/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation
- [ ] Gas report with benchmarks
- [ ] Test results summary

### Testing
- [ ] Unit tests for all contract functions
- [ ] Integration tests for full flows
- [ ] Gas benchmarks for all operations
- [ ] Test coverage for required scenarios

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployment addresses documented (if testnet)

---

## Notes

- **No Compliance Claims**: This is a technical prototype. Include disclaimer that it is NOT regulatory-compliant.
- **Testnet Only**: Never use real funds. Sepolia testnet or local Hardhat network only.
- **Secrets Management**: Use .env for private keys, never commit secrets.
- **Gas Optimization**: Targets are guidelines. Document actual costs and justify if above targets.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology

```

Continuing with the SpendSense PRD:


```markdown
# Product Requirements Document: SpendSense

**Version:** 1.0  
**Date:** October 31, 2025  
**Project Duration:** 10 Days (Oct 31 - Nov 9, 2025)  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Final Polish (60 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.6.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology

```

Continuing with the remaining 3 PRDs, starting with AI Math Tutor and DreamUp, then Varsity Tutors. Creating them systematically:


Creating PRDs for the remaining 3 projects. Starting with AI Math Tutor, then DreamUp, then Varsity Tutors:


```markdown
# Product Requirements Document: AI Math Tutor

**Version:** 1.0  
**Date:** October 31, 2025  
**Project Duration:** 10 Days (Oct 31 - Nov 9, 2025)  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Socratic Learning Assistant with OCR/Vision LLM Parsing

---

## Executive Summary

AI Math Tutor is an AI-powered educational assistant that guides students through math problems using Socratic questioning. The system accepts problems via screenshot or text input, parses them using OCR/Vision LLM, and maintains multi-turn conversations that guide students to discover solutions without providing direct answers.

**Key Deliverables:**
- ✅ Problem Input: Text entry + image upload with OCR/Vision LLM parsing
- ✅ Socratic Dialogue: Multi-turn conversation with guiding questions
- ✅ Math Rendering: Display equations properly (LaTeX/KaTeX)
- ✅ Web Interface: Clean chat UI with image upload and conversation history
- ✅ Context Management: Maintains conversation context across turns
- ✅ Adaptive Scaffolding: Adjusts to student understanding level

**Success Targets:**
- **MVP (Nov 4)**: Basic chat + LLM integration + Socratic prompting + image parsing
- **Early Submission (Nov 7)**: + Math rendering + conversation history + UI polish
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + optional stretch features

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Basic chat UI, LLM integration, Socratic prompting, image parsing working |
| **Early Submission** | Friday, November 7, 2025 | + Math rendering (LaTeX), conversation history, UI polish |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive testing, documentation, optional stretch features |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Math Rendering**: KaTeX or MathJax
- **Image Upload**: React Dropzone or similar
- **Chat UI**: Custom chat component or react-chat-elements
- **Build Tool**: Vite or Next.js (if SSR needed)

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API routes
- **Language**: TypeScript
- **AI Integration**: OpenAI GPT-4 with Vision API or Anthropic Claude with vision
- **Image Processing**: Sharp (for image optimization)
- **Storage**: SQLite (local) or PostgreSQL (if deployed)
- **Session Management**: In-memory or Redis (if deployed)

### AI Integration
- **LLM**: OpenAI GPT-4 Turbo with Vision or Anthropic Claude 3.5 Sonnet with vision
- **SDK**: OpenAI Node.js SDK or Anthropic SDK
- **Image Processing**: OpenAI Vision API for OCR or Tesseract.js (local OCR)
- **Prompt Engineering**: Structured prompts for Socratic method

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
AI-Math-Tutor/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── Chat.tsx              # Main chat interface
│   │   │   ├── MessageBubble.tsx     # Message display
│   │   │   ├── ImageUpload.tsx       # Image upload component
│   │   │   ├── MathRenderer.tsx      # LaTeX math rendering
│   │   │   └── ConversationHistory.tsx # History sidebar
│   │   ├── hooks/
│   │   │   ├── useChat.ts            # Chat state management
│   │   │   └── useImageUpload.ts     # Image upload logic
│   │   ├── services/
│   │   │   ├── api.ts                # API client
│   │   │   └── mathParser.ts         # Math parsing utilities
│   │   └── App.tsx
│   └── public/
├── backend/
│   ├── api/
│   │   ├── chat.ts                   # Chat endpoint
│   │   ├── image-upload.ts            # Image upload endpoint
│   │   └── health.ts                  # Health check
│   ├── services/
│   │   ├── llm.ts                     # LLM service
│   │   ├── imageProcessor.ts          # Image processing
│   │   ├── socratic.ts                # Socratic method logic
│   │   └── contextManager.ts         # Conversation context
│   ├── prompts/
│   │   └── socratic.ts                # Socratic prompt templates
│   └── server.ts                      # Express server
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── prompt-engineering-notes.md
```

### Data Flow

1. **Problem Input**: User uploads image or types text → Image processed via Vision API → Problem text extracted
2. **Socratic Dialogue**: Problem text → LLM with Socratic prompt → Guiding question → User response → Context updated
3. **Math Rendering**: Mathematical expressions → LaTeX parser → KaTeX renderer → Displayed in UI
4. **Context Management**: Each turn → Context stored → Previous turns retrieved → Full conversation context maintained

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic chat with LLM integration and Socratic prompting working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 20 minutes
- Create `ai-math-tutor/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install dependencies: React, TypeScript, Tailwind CSS
- Verify setup: `npm run dev`
- PowerShell: `New-Item -ItemType Directory -Path ai-math-tutor; Set-Location ai-math-tutor; npm create vite@latest frontend -- --template react-ts`

**Task 1.1.2**: Setup backend with Express
- Duration: 20 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, tsx, OpenAI SDK
- Create `server.ts` with basic Express setup
- Add TypeScript config: `tsconfig.json`
- PowerShell: `New-Item -ItemType Directory -Path backend; Set-Location backend; npm init -y; npm install express typescript tsx @types/express @types/node`

**Task 1.1.3**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files (frontend and backend)
- Add: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries
- Document setup in README

**Task 1.1.4**: Setup testing framework
- Duration: 20 minutes
- Install Vitest: `npm install -D vitest @testing-library/react`
- Create `tests/` directory structure
- Create sample test to verify setup
- Run: `npm test` to verify working

**Task 1.1.5**: Create project structure
- Duration: 15 minutes
- Create directory structure (components, services, hooks)
- Create placeholder files for main components
- Add basic README with setup instructions

#### Phase 1.2: Basic Chat UI (120 minutes)

**Task 1.2.1**: Create chat interface component
- Duration: 45 minutes
- Create `frontend/src/components/Chat.tsx`
- Basic chat UI with:
  - Message input area
  - Send button
  - Message list area
  - Simple styling with Tailwind
- Add state management for messages

**Task 1.2.2**: Create message bubble component
- Duration: 30 minutes
- Create `frontend/src/components/MessageBubble.tsx`
- Display: User messages (right-aligned), Tutor messages (left-aligned)
- Add basic styling and animations
- Show timestamps

**Task 1.2.3**: Create chat hook for state management
- Duration: 30 minutes
- Create `frontend/src/hooks/useChat.ts`
- Manage: Messages array, current input, loading state
- Functions: `sendMessage()`, `addMessage()`, `clearChat()`
- Return: Messages, loading, sendMessage function

**Task 1.2.4**: Create API client
- Duration: 15 minutes
- Create `frontend/src/services/api.ts`
- Function: `sendMessage(message: string) => Promise<string>`
- POST to `/api/chat` endpoint
- Handle errors and loading states

#### Phase 1.3: Backend API - Chat Endpoint (120 minutes)

**Task 1.3.1**: Create Express server with chat endpoint
- Duration: 30 minutes
- Create `backend/server.ts`
- Setup Express with CORS middleware
- Create `/api/chat` POST endpoint
- Receive: `{ message: string, conversationHistory?: Message[] }`
- Return: `{ response: string }`

**Task 1.3.2**: Integrate OpenAI SDK
- Duration: 30 minutes
- Create `backend/services/llm.ts`
- Setup OpenAI client with API key
- Function: `generateResponse(messages: Message[]) => Promise<string>`
- Call `openai.chat.completions.create()` with messages
- Return assistant message

**Task 1.3.3**: Create Socratic prompt template
- Duration: 30 minutes
- Create `backend/prompts/socratic.ts`
- System prompt: "You are a patient math tutor. NEVER give direct answers. Guide through questions: 'What information do we have?' 'What method might help?' If stuck >2 turns, provide concrete hint. Use encouraging language."
- Format: System message + conversation history
- Add prompt validation

**Task 1.3.4**: Implement Socratic logic
- Duration: 30 minutes
- Create `backend/services/socratic.ts`
- Function: `buildPrompt(conversationHistory: Message[]) => Message[]`
- Logic:
  - If first message: Parse problem → Inventory knowns → Identify goal
  - If subsequent: Guide method selection → Step through solution → Validate answer
- Add validation for Socratic method (no direct answers)

#### Phase 1.4: Image Upload & OCR/Vision LLM (150 minutes)

**Task 1.4.1**: Create image upload component
- Duration: 45 minutes
- Create `frontend/src/components/ImageUpload.tsx`
- Use React Dropzone or similar
- Features: Drag & drop, click to upload, preview image
- Accept: PNG, JPG, JPEG formats
- Display upload progress

**Task 1.4.2**: Create image upload API endpoint
- Duration: 30 minutes
- Create `/api/image-upload` POST endpoint
- Use Multer or similar for file handling
- Save uploaded image temporarily
- Return: Image URL or base64 data

**Task 1.4.3**: Integrate Vision API for OCR
- Duration: 45 minutes
- Create `backend/services/imageProcessor.ts`
- Function: `extractTextFromImage(imagePath: string) => Promise<string>`
- Option A: OpenAI Vision API:
  - Use `openai.chat.completions.create()` with vision model
  - Prompt: "Extract all mathematical expressions and text from this image. Return the problem statement."
  - Parse response for problem text
- Option B: Tesseract.js (local OCR):
  - Install Tesseract.js
  - Process image locally
  - Extract text (less accurate for math)

**Task 1.4.4**: Integrate image upload into chat flow
- Duration: 30 minutes
- Modify chat UI to include image upload
- When image uploaded: Extract text → Send to LLM as first message
- Handle: Image processing → Text extraction → Problem parsing → Chat start
- Add loading state for image processing

#### Phase 1.5: Conversation Context Management (90 minutes)

**Task 1.5.1**: Implement context manager
- Duration: 45 minutes
- Create `backend/services/contextManager.ts`
- Function: `buildContext(userId: string, conversationId: string) => Message[]`
- Store: Messages in-memory (Map<conversationId, Message[]>)
- Or: Use SQLite for persistence
- Retrieve: Full conversation history for context

**Task 1.5.2**: Add context to chat endpoint
- Duration: 30 minutes
- Modify `/api/chat` endpoint to accept `conversationId`
- Retrieve conversation history from context manager
- Pass full history to LLM (last 10-20 messages for token efficiency)
- Update context after each message

**Task 1.5.3**: Test context management
- Duration: 15 minutes
- Test: Multi-turn conversation maintains context
- Test: LLM references previous messages
- Test: Context persists across API calls
- Verify: Conversation history is accurate

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Write unit tests for chat logic
- Duration: 45 minutes
- Test: Message sending and receiving
- Test: Context management (add/retrieve messages)
- Test: Socratic prompt building
- Test: Image text extraction (mock)
- Run: `npm test` to verify

**Task 1.6.2**: Test with hardcoded problems
- Duration: 30 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problem: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
- Verify: Socratic method works (no direct answers)
- Verify: Context maintained across turns

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All unit tests pass
- Verify: Chat flow works end-to-end
- Document any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor MVP - Socratic learning assistant"
  - Problem Input (30s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (60s): Show multi-turn conversation, demonstrate guiding questions (no direct answers)
  - Context Management (30s): Show how tutor references previous messages
  - Conclusion (10s): "MVP demonstrates core Socratic method with image parsing and context management"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for chat interface
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add math rendering, conversation history, UI polish

#### Phase 2.1: Math Rendering with LaTeX/KaTeX (120 minutes)

**Task 2.1.1**: Install and configure KaTeX
- Duration: 20 minutes
- Install: `npm install katex react-katex`
- Add KaTeX CSS to main CSS file
- Create `frontend/src/components/MathRenderer.tsx`
- Function: `renderMath(text: string) => JSX.Element`
- Support: Inline math `$...$` and display math `$$...$$`

**Task 2.1.2**: Parse mathematical expressions from messages
- Duration: 45 minutes
- Create `frontend/src/services/mathParser.ts`
- Function: `parseMath(text: string) => string`
- Extract: LaTeX expressions from text
- Handle: Inline math and display math
- Preserve: Non-math text

**Task 2.1.3**: Integrate math rendering into message bubbles
- Duration: 35 minutes
- Modify `MessageBubble.tsx` to render math
- Parse message text for math expressions
- Render math with KaTeX component
- Handle: Error cases (invalid LaTeX)
- Style: Math expressions appropriately

**Task 2.1.4**: Test math rendering
- Duration: 20 minutes
- Test: Simple equations (`x + 5 = 13`)
- Test: Complex equations (`x² + 5x + 6 = 0`)
- Test: Fractions, exponents, integrals
- Test: Mixed text and math
- Verify: All expressions render correctly

#### Phase 2.2: Conversation History (90 minutes)

**Task 2.2.1**: Create conversation history UI
- Duration: 45 minutes
- Create `frontend/src/components/ConversationHistory.tsx`
- Display: List of past conversations
- Features:
  - Conversation titles (first problem statement)
  - Timestamp
  - Click to load conversation
  - Delete conversation
- Sidebar or dropdown menu

**Task 2.2.2**: Persist conversations in backend
- Duration: 30 minutes
- Create SQLite database for conversations
- Schema: `conversations(id, userId, title, createdAt)`, `messages(conversationId, role, content, timestamp)`
- Store: Full conversation history
- Retrieve: Conversations by user

**Task 2.2.3**: Load conversation from history
- Duration: 15 minutes
- Add API endpoint: `GET /api/conversations/:id`
- Load conversation messages into chat
- Update context manager with loaded conversation
- Test: Loading conversation works correctly

#### Phase 2.3: UI Polish & Enhancements (120 minutes)

**Task 2.3.1**: Improve chat UI design
- Duration: 45 minutes
- Enhance: Message bubble styling
- Add: Typing indicator
- Add: Loading states for LLM responses
- Add: Smooth scroll to latest message
- Add: Dark mode (optional)

**Task 2.3.2**: Add error handling and user feedback
- Duration: 30 minutes
- Display: Error messages for failed API calls
- Add: Retry button for failed requests
- Show: Network errors clearly
- Add: Input validation (empty messages, file size limits)

**Task 2.3.3**: Optimize image upload UX
- Duration: 30 minutes
- Add: Image preview before upload
- Add: Image compression (if needed)
- Add: Progress bar for upload
- Add: Error handling for unsupported formats
- Add: Maximum file size validation

**Task 2.3.4**: Add keyboard shortcuts
- Duration: 15 minutes
- Add: Enter to send message
- Add: Shift+Enter for new line
- Add: Escape to clear input
- Test: Keyboard shortcuts work correctly

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Test math rendering with various problems
- Duration: 30 minutes
- Test: 5+ problem types (arithmetic, algebra, geometry, word problems, multi-step)
- Verify: All math expressions render correctly
- Verify: LaTeX syntax is handled properly
- Document: Any rendering issues

**Task 2.4.2**: Test conversation history
- Duration: 30 minutes
- Test: Saving conversations
- Test: Loading conversations
- Test: Deleting conversations
- Test: Multiple conversations per user
- Verify: Context maintained when loading

**Task 2.4.3**: Integration tests for full flow
- Duration: 30 minutes
- Test: Upload image → Extract text → Start chat → Socratic dialogue → Math rendering
- Test: Load conversation → Continue chat → Context maintained
- Test: Error scenarios (API failures, invalid images)
- Verify: All features work together

**Task 2.4.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor Early Submission - now with math rendering and history"
  - Problem Input (25s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (50s): Show multi-turn conversation, demonstrate guiding questions, show math rendering
  - Conversation History (30s): Show past conversations, load conversation, continue dialogue
  - UI Polish (25s): Show improved UI, error handling, keyboard shortcuts
  - Conclusion (10s): "Demonstrates complete Socratic learning experience with math rendering"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for improved interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optional stretch features

#### Phase 3.1: Comprehensive Testing (120 minutes)

**Task 3.1.1**: Test all 5+ problem types
- Duration: 60 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problems: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
  6. Fractions: "1/2 + 1/3 = ?"
  7. Exponents: "2³ × 2² = ?"
- Verify: Socratic method works for all types
- Verify: No direct answers given
- Verify: Context maintained across turns

**Task 3.1.2**: Test edge cases and error scenarios
- Duration: 45 minutes
- Test: Invalid image formats
- Test: Very large images
- Test: Blurry images (OCR failure)
- Test: No text in image
- Test: Network errors
- Test: LLM API errors
- Test: Empty messages
- Test: Very long conversations (>20 turns)

**Task 3.1.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.2: Optional Stretch Features (180 minutes)

**Task 3.2.1**: Interactive Whiteboard (Optional - High Value)
- Duration: 90 minutes
- Install: react-sketch-canvas or similar
- Create: `Whiteboard.tsx` component
- Features:
  - Draw diagrams and equations
  - Share with tutor
  - Tutor can draw explanations
- Integrate: Into chat flow
- Test: Whiteboard functionality

**Task 3.2.2**: Step Visualization (Optional - High Value)
- Duration: 60 minutes
- Create: `StepVisualization.tsx` component
- Display: Solution steps animated or sequentially
- Parse: LLM response for steps
- Render: Each step with math rendering
- Test: Step visualization works

**Task 3.2.3**: Voice Interface (Optional - High Value)
- Duration: 30 minutes
- Add: Text-to-speech for tutor responses
- Add: Speech-to-text for student input
- Use: Web Speech API
- Test: Voice interface works

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - LLM Integration (OpenAI/Anthropic)
  - Socratic Method Implementation
  - Image Processing & OCR
  - Math Rendering (KaTeX)
  - Context Management
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Write prompt engineering notes
- Duration: 30 minutes
- Document: Socratic prompt design
- Document: Prompt iterations and improvements
- Document: Techniques for preventing direct answers
- Document: Context management strategies
- Include: Example prompts and responses

**Task 3.3.3**: Update README with complete setup
- Duration: 30 minutes
- Prerequisites (Node.js, npm, API keys)
- Installation steps
- Environment setup
- Running locally
- Testing instructions
- Deployment instructions (if applicable)
- Troubleshooting section

**Task 3.3.4**: Document 5+ example problem walkthroughs
- Duration: 15 minutes
- Document: 5+ complete problem walkthroughs
- Include: Problem statement, conversation flow, final answer
- Show: Socratic method in action
- Show: Context maintained across turns

**Task 3.3.5**: Document AI tools and prompts used
- Duration: 15 minutes
- List: AI tools used (Cursor, Copilot, etc.)
- Document: Key prompts that generated code
- Acknowledge: AI assistance in README

#### Phase 3.4: Final Polish (90 minutes)

**Task 3.4.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove: Debug code and console.logs
- Add: JSDoc comments to all functions

**Task 3.4.2**: Performance optimization
- Duration: 30 minutes
- Optimize: Image upload (compression if needed)
- Optimize: Math rendering (cache if possible)
- Optimize: Context management (limit history length)
- Profile: API response times
- Optimize: Re-renders in React

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All 5+ problem types tested
- ✅ Socratic method works (no direct answers)
- ✅ Context maintained across turns
- ✅ Math rendering works correctly
- ✅ Image upload and OCR working
- ✅ Conversation history working
- ✅ README complete and accurate
- ✅ Documentation complete
- ✅ Tests passing
- ✅ No API keys in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor - Complete Socratic learning assistant"
  - System Overview (15s): Architecture, key features
  - Problem Input (25s): Text input and image upload, OCR demonstration
  - Socratic Dialogue (60s): Multi-turn conversation, guiding questions, math rendering, no direct answers
  - Conversation History (20s): Past conversations, loading and continuing
  - Stretch Features (20s): Whiteboard/step visualization/voice (if built)
  - Conclusion (10s): "Complete Socratic learning experience with comprehensive features"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all features
- Highlight key technical achievements
- Show Socratic method in action

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Check: Test coverage (aim for >80%)
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Problem input working (text + image upload)
- ✅ OCR/Vision LLM parsing working
- ✅ Basic chat UI functional
- ✅ LLM integration with Socratic prompting
- ✅ Socratic method working (no direct answers)
- ✅ Context management working
- ✅ Multi-turn conversations functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Math rendering with LaTeX/KaTeX working
- ✅ Conversation history implemented
- ✅ UI polish and enhancements complete
- ✅ Integration tests passing
- ✅ Tested on 5+ problem types

### Final Submission (Phase 3) - November 9
- ✅ Comprehensive testing (5+ problem types)
- ✅ Edge cases handled gracefully
- ✅ Documentation complete (technical writeup, prompt notes, README, walkthroughs)
- ✅ Optional stretch features implemented (if time permits)
- ✅ Code clean, linted, and documented
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM gives direct answers | High | Strict Socratic prompt, validation in code, test thoroughly |
| OCR fails on images | Medium | Fallback to text input, clear error messages, test with various images |
| Context too long for LLM | Medium | Limit conversation history (last 10-20 messages), summarize if needed |
| API costs too high | Medium | Cache responses if possible, use cheaper models for iteration |
| Math rendering errors | Low | Validate LaTeX syntax, handle errors gracefully |
| Time constraints | High | MVP is safety net, stretch features are optional, prioritize core Socratic method |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Prompt engineering notes
- [ ] README with complete setup and usage
- [ ] 5+ example problem walkthroughs
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for chat logic
- [ ] Integration tests for full flow
- [ ] Tests for 5+ problem types
- [ ] Edge case tests

### Demo
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployed app or local setup instructions

---

## Notes

- **Socratic Method**: NEVER give direct answers. Guide through questions only.
- **Context Management**: Maintain full conversation history for proper context.
- **Math Rendering**: Use KaTeX for accurate math display.
- **Image Processing**: Use Vision API for best OCR results (or Tesseract.js for local).
- **Testing**: Test with diverse problem types to ensure Socratic method works.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology

```

Creating PRDs for DreamUp and Varsity Tutors:


```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.0  
**Date:** October 31, 2025  
**Project Duration:** 10 Days (Oct 31 - Nov 9, 2025)  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Final Polish (90 minutes)

**Task 3.5.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.5.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.5.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.6: Final Submission Demo Video Preparation (45 minutes)

**Task 3.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology

```

Creating the final PRD for Varsity Tutors:


```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.0  
**Date:** October 31, 2025  
**Project Duration:** 10 Days (Oct 31 - Nov 9, 2025)  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology

```

Marking the final task complete:


## Summary

Created 5 PRDs for your Gauntlet Week 4 projects:

### 1. **PRD-Peak6-ChainEquity.md**
   - Tokenized security prototype with compliance gating
   - Phases: Gated token contract → Corporate actions → Operator demo
   - Tech: Solidity, Hardhat, Node.js, ethers.js, SQLite

### 2. **PRD-Peak6-SpendSense.md**
   - Financial education system with behavioral detection
   - Phases: Data ingestion → Signal detection → Persona assignment → Recommendations → Evaluation
   - Tech: Python/Pandas, SQLite, FastAPI (optional React UI)

### 3. **PRD-SuperBuilders-AI_Math_Tutor.md**
   - Socratic learning assistant with OCR/Vision LLM
   - Phases: Basic chat → Socratic prompting → Math rendering → History
   - Tech: React, TypeScript, OpenAI Vision API, KaTeX

### 4. **PRD-SuperBuilders-DreamUp.md**
   - Autonomous browser game QA pipeline
   - Phases: Browser automation → Evidence capture → AI evaluation → Structured output
   - Tech: TypeScript, Browserbase/Stagehand, OpenAI GPT-4 Vision

### 5. **PRD-Varsity_Tutors-10x_K_Factor.md**
   - Viral growth system with agentic actions
   - Phases: Viral loops → Agentic actions → Smart links → Analytics → Compliance
   - Tech: React, Node.js, Socket.io, MCP agents, OpenAI, PostgreSQL/SQLite

## Features across all PRDs:

- Phase structure: MVP (Nov 4) → Early Submission (Nov 7) → Final (Nov 9)
- Granular tasks: 15–90 minute tasks with time estimates
- Integrated testing: Final task in each phase
- Demo videos: 3-minute scripts and cue cards for each milestone
- PowerShell syntax: All terminal commands use PowerShell format
- Wait-for-confirmation: Explicit reminders to wait for approval before proceeding

All PRDs follow the October 16 success path methodology with:
- Documentation-first approach
- Granular task breakdowns
- Risk mitigation strategies
- Success criteria checklists
- Deliverables tracking

Ready for execution. Each PRD includes detailed phases, tasks, and implementation guidance aligned with your working style and requirements.

---

**User**

Update each of our new PRDs to show that today (November 3rd, 2025) is the start of each project.  Ensure there is a near-final task to evaluate and refactor code and run security assessment. Then, procide a new markdown for each that provides a risk assessment, summing the assessment into a probability of success rating (highlighted on page 1)

---

**Cursor**

Updating all PRDs to start on November 3, 2025, adding a near-final code evaluation and security assessment task, and creating a risk assessment document for each project. Starting with the updates:


Updating all 5 PRDs to start November 3, 2025, adding the code evaluation/security assessment task, and creating a risk assessment document for each. Starting with the updates:


```markdown
# Product Requirements Document: ChainEquity

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Tokenized Security Prototype with Compliance Gating

---

## Executive Summary

ChainEquity is a working prototype demonstrating how tokenized securities function on-chain with compliance gating, corporate actions, and operator workflows. Built as a technical demonstration (NOT regulatory-compliant), the system showcases blockchain primitives for equity management through transparent, automated compliance checks.

**Key Deliverables:**
- ✅ Gated token contract (allowlist-based transfer restrictions)
- ✅ Issuer service for wallet approval and token minting
- ✅ Event indexer producing cap-table snapshots
- ✅ Corporate action system (splits, symbol changes)
- ✅ Operator UI or CLI for demonstrations
- ✅ Test suite and gas benchmarks
- ✅ Technical writeup with decision rationale

**Success Targets:**
- **MVP (Nov 4)**: Core gated token + basic issuer service + event indexer
- **Early Submission (Nov 7)**: + Corporate actions + operator demo
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + gas optimization

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Core gated token contract, basic issuer service, event indexer working |
| **Early Submission** | Friday, November 7, 2025 | + Corporate actions (split + symbol change), operator demo UI/CLI |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive test suite, gas benchmarks, technical writeup |

---

## Technology Stack

### Blockchain Selection
**Recommendation: Ethereum Sepolia Testnet**
- **Rationale**: Most mature ecosystem, ERC-20 standard well-documented, extensive tooling
- **Alternative**: Polygon Mumbai (lower gas costs, EVM-compatible)
- **Local Option**: Hardhat Network for development, then deploy to Sepolia

### Smart Contract Stack
- **Language**: Solidity 0.8.20+
- **Framework**: Hardhat (testing, deployment, gas reporting)
- **Base Contracts**: OpenZeppelin Contracts v5.0+ (ERC-20 base)
- **Testing**: Hardhat + Chai + Ethers.js v6
- **Deployment**: Hardhat scripts with environment variables

### Backend/Indexer Stack
- **Language**: Node.js + TypeScript
- **Blockchain Library**: ethers.js v6
- **Event Listening**: WebSocket subscriptions or polling
- **Storage**: SQLite for cap-table snapshots (simple, no external deps)
- **CLI Framework**: Commander.js for CLI interface
- **Web Framework** (Optional UI): Express.js + React

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Git Hooks**: Husky + lint-staged
- **Documentation**: JSDoc for contracts, README.md for setup

---

## Architecture Overview

### System Components

```
ChainEquity/
├── contracts/
│   ├── GatedToken.sol              # ERC-20 with allowlist gating
│   ├── CorporateActions.sol         # Split and symbol change logic
│   └── interfaces/
│       └── IGatedToken.sol
├── backend/
│   ├── issuer-service/              # Wallet approval + minting
│   │   ├── approval.ts
│   │   ├── mint.ts
│   │   └── allowlist.ts
│   ├── indexer/                     # Event indexing + cap-table
│   │   ├── eventListener.ts
│   │   ├── capTable.ts
│   │   └── snapshot.ts
│   └── cli/                         # Operator CLI
│       └── commands.ts
├── tests/
│   ├── unit/
│   │   ├── GatedToken.test.ts
│   │   └── CorporateActions.test.ts
│   ├── integration/
│   │   ├── fullFlow.test.ts
│   │   └── capTable.test.ts
│   └── gas/
│       └── benchmarks.test.ts
├── scripts/
│   ├── deploy.ts
│   └── seed.ts
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Approval Flow**: User requests → Issuer Service → Contract allowlist update
2. **Minting Flow**: Issuer Service → Contract mint → Event emitted
3. **Transfer Flow**: User initiates → Contract checks allowlist → Transfer succeeds/fails
4. **Indexing Flow**: Contract events → Indexer listens → SQLite cap-table updated
5. **Export Flow**: Indexer queries → CSV/JSON generation

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core functionality working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize Hardhat project with TypeScript
- Duration: 15 minutes
- Create `chainequity/` directory
- Run `npx hardhat init` with TypeScript template
- Install OpenZeppelin Contracts: `npm install @openzeppelin/contracts`
- Install ethers.js v6: `npm install ethers@^6.0.0`
- Verify setup: `npx hardhat compile`
- PowerShell: `New-Item -ItemType Directory -Path chainequity; Set-Location chainequity; npx hardhat init`

**Task 1.1.2**: Configure Hardhat for Sepolia testnet
- Duration: 15 minutes
- Create `.env` file with `SEPOLIA_RPC_URL` and `PRIVATE_KEY`
- Update `hardhat.config.ts` with network config
- Add `.env.example` with placeholders
- Document setup in README.md

**Task 1.1.3**: Setup backend project structure
- Duration: 30 minutes
- Create `backend/` directory with `issuer-service/`, `indexer/`, `cli/`
- Initialize TypeScript: `tsc --init`
- Install dependencies: `npm install ethers commander sqlite3`
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path backend/issuer-service,backend/indexer,backend/cli; npm init -y`

**Task 1.1.4**: Install testing dependencies
- Duration: 15 minutes
- Install: `npm install --save-dev @nomicfoundation/hardhat-chai-matchers @nomicfoundation/hardhat-toolbox chai`
- Create `tests/` directory structure
- Verify test setup: `npx hardhat test`

**Task 1.1.5**: Create .gitignore and initial README
- Duration: 15 minutes
- Add `.env`, `node_modules/`, `artifacts/`, `cache/` to .gitignore
- Write README.md with setup instructions
- Document environment variables needed

#### Phase 1.2: Gated Token Contract (180 minutes)

**Task 1.2.1**: Create base ERC-20 contract extending OpenZeppelin
- Duration: 30 minutes
- Create `contracts/GatedToken.sol`
- Import `@openzeppelin/contracts/token/ERC20/ERC20.sol`
- Add constructor with name, symbol, initial supply
- Add owner/admin role using OpenZeppelin's Ownable
- Compile and verify: `npx hardhat compile`

**Task 1.2.2**: Implement allowlist mapping and modifiers
- Duration: 45 minutes
- Add `mapping(address => bool) public allowlist`
- Create modifier `onlyAllowlisted(address account)`
- Add events: `AllowlistUpdated(address indexed account, bool indexed status)`
- Add view function `isAllowlisted(address account) returns (bool)`
- Test allowlist logic in isolated test file

**Task 1.2.3**: Override transfer functions to check allowlist
- Duration: 60 minutes
- Override `transfer(address to, uint256 amount)` with allowlist check
- Override `transferFrom(address from, address to, uint256 amount)` with allowlist check
- Check BOTH sender AND recipient are on allowlist
- Revert with descriptive error if either not approved
- Emit `Transfer` event only on success
- Test transfer scenarios (approved→approved, approved→non-approved, non-approved→approved)

**Task 1.2.4**: Implement admin functions for allowlist management
- Duration: 45 minutes
- Add `approveWallet(address account)` function (owner only)
- Add `revokeWallet(address account)` function (owner only)
- Add `batchApprove(address[] calldata accounts)` for efficiency
- Add `batchRevoke(address[] calldata accounts)`
- Emit `AllowlistUpdated` events for all changes
- Add access control checks (only owner)

#### Phase 1.3: Issuer Service Backend (150 minutes)

**Task 1.3.1**: Create wallet approval service
- Duration: 45 minutes
- Create `backend/issuer-service/approval.ts`
- Implement `approveWallet(contract: Contract, wallet: string, signer: Signer)`
- Connect to contract using ethers.js
- Call `approveWallet()` function
- Handle errors and transaction confirmation
- Add retry logic for failed transactions

**Task 1.3.2**: Create token minting service
- Duration: 45 minutes
- Create `backend/issuer-service/mint.ts`
- Implement `mintTokens(contract: Contract, to: string, amount: bigint, signer: Signer)`
- Verify recipient is on allowlist before minting
- Call contract's mint function (need to add mint function to contract)
- Wait for transaction confirmation
- Return transaction hash

**Task 1.3.3**: Create allowlist query service
- Duration: 30 minutes
- Create `backend/issuer-service/allowlist.ts`
- Implement `checkAllowlistStatus(contract: Contract, wallet: string)`
- Query contract's `isAllowlisted()` view function
- Return boolean status
- Add batch query support for multiple wallets

**Task 1.3.4**: Add mint function to contract (if not present)
- Duration: 30 minutes
- Add `mint(address to, uint256 amount)` function to GatedToken.sol
- Ensure `to` address is on allowlist before minting
- Check owner/admin permissions (only owner can mint)
- Emit `Transfer(address(0), to, amount)` event
- Test minting in isolation

#### Phase 1.4: Event Indexer (120 minutes)

**Task 1.4.1**: Setup SQLite database schema
- Duration: 30 minutes
- Create `backend/indexer/database.ts`
- Define schema: `wallets(id, address, balance, last_updated)`
- Define schema: `transfers(id, from_address, to_address, amount, block_number, tx_hash)`
- Define schema: `mints(id, to_address, amount, block_number, tx_hash)`
- Initialize database with tables
- Add indexes on addresses and block_numbers

**Task 1.4.2**: Create event listener for Transfer events
- Duration: 45 minutes
- Create `backend/indexer/eventListener.ts`
- Setup ethers.js contract connection
- Listen for `Transfer(address indexed from, address indexed to, uint256 value)` events
- Parse event data and extract from/to/amount
- Store events in SQLite transfers table
- Handle reorgs (query from specific block)

**Task 1.4.3**: Create event listener for Mint events
- Duration: 30 minutes
- Listen for custom `Mint(address indexed to, uint256 amount)` events
- Parse and store in mints table
- Update wallet balances accordingly
- Handle duplicate events (check transaction hash)

**Task 1.4.4**: Implement balance aggregation
- Duration: 15 minutes
- Create function to aggregate current balance per wallet
- Query all transfers and mints from genesis
- Calculate net balance: sum(mints) + sum(received) - sum(sent)
- Update wallets table with current balances

#### Phase 1.5: Basic CLI Interface (90 minutes)

**Task 1.5.1**: Setup Commander.js CLI framework
- Duration: 20 minutes
- Install commander: `npm install commander`
- Create `backend/cli/index.ts` as entry point
- Setup command structure with commander
- Add `--help` and `--version` flags

**Task 1.5.2**: Implement approve-wallet command
- Duration: 20 minutes
- Add `approve-wallet <address>` command
- Connect to contract
- Call approval service
- Show transaction hash and confirmation

**Task 1.5.3**: Implement mint command
- Duration: 25 minutes
- Add `mint <address> <amount>` command
- Validate address format
- Parse amount to BigInt
- Call minting service
- Show confirmation

**Task 1.5.4**: Implement query-allowlist command
- Duration: 15 minutes
- Add `query-allowlist <address>` command
- Query contract for allowlist status
- Display status (approved/not approved)

**Task 1.5.5**: Implement export-cap-table command
- Duration: 10 minutes
- Add `export-cap-table [--format csv|json]` command
- Query database for all wallets and balances
- Calculate percentage ownership (balance / total supply)
- Export to CSV or JSON format
- PowerShell: `node backend/cli/index.ts export-cap-table --format csv > cap-table.csv`

#### Phase 1.6: Deployment & Testing (120 minutes)

**Task 1.6.1**: Deploy contract to Sepolia testnet
- Duration: 30 minutes
- Create `scripts/deploy.ts`
- Load private key from .env
- Deploy GatedToken contract with constructor params
- Verify deployment address
- Save contract address to .env or config file

**Task 1.6.2**: Write basic unit tests for contract
- Duration: 45 minutes
- Create `tests/unit/GatedToken.test.ts`
- Test: approve wallet → verify allowlist status
- Test: mint to approved wallet → verify balance
- Test: transfer between approved wallets → SUCCESS
- Test: transfer to non-approved wallet → FAIL
- Test: transfer from non-approved wallet → FAIL
- Use Hardhat's fixture pattern for setup

**Task 1.6.3**: Write integration test for full flow
- Duration: 30 minutes
- Create `tests/integration/fullFlow.test.ts`
- Test: Approve → Mint → Transfer → Export cap-table
- Verify all events are emitted correctly
- Verify balances are correct at each step

**Task 1.6.4**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Verify all tests pass
- Check coverage (if configured)
- Document any failing tests or known issues

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "This is ChainEquity MVP - a tokenized security prototype"
  - Wallet Approval Demo (30s): Show approve-wallet command, verify status
  - Minting Demo (30s): Mint tokens to approved wallet, show balance
  - Transfer Demo (40s): Transfer between approved wallets (SUCCESS), then try non-approved (FAIL)
  - Cap-Table Export (30s): Export current state to CSV, show ownership percentages
  - Conclusion (10s): "MVP demonstrates core gating, minting, and indexing functionality"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Format:
  ```
  [NARRATION]: "This is ChainEquity MVP..."
  • Highlights: Project introduction, system overview
  [NARRATION]: "First, we'll approve a wallet..."
  • Highlights: CLI approve-wallet command, contract interaction, transaction confirmation
  ```
- Include bullet points for each narration line showing key UI elements/actions
- Keep narration natural, 3 minutes total
- Include visual cues (what to show on screen)

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add corporate actions and operator demo interface

#### Phase 2.1: Corporate Actions - Stock Split (180 minutes)

**Task 2.1.1**: Design split implementation approach
- Duration: 30 minutes
- Evaluate: On-chain iteration vs. virtual split vs. contract migration
- **Decision**: Virtual split (adjust display logic, keep balances unchanged)
- **Rationale**: Gas-efficient, maintains contract state, simpler implementation
- Document tradeoffs in decision-log.md

**Task 2.1.2**: Implement split ratio tracking in contract
- Duration: 45 minutes
- Add `uint256 public splitRatio = 1` (1 = no split, 7 = 7-for-1 split)
- Add `mapping(address => uint256) public adjustedBalances` (optional for display)
- Add `event StockSplit(uint256 oldRatio, uint256 newRatio, uint256 blockNumber)`
- Add `executeSplit(uint256 newRatio)` function (owner only)
- Validate newRatio > 0 and newRatio > splitRatio (only forward splits)

**Task 2.1.3**: Implement split logic in issuer service
- Duration: 45 minutes
- Create `backend/issuer-service/split.ts`
- Implement `executeSplit(contract: Contract, ratio: bigint, signer: Signer)`
- Call contract's `executeSplit()` function
- Wait for event emission
- Update indexer to track split events

**Task 2.1.4**: Update indexer to handle split events
- Duration: 30 minutes
- Listen for `StockSplit` events
- Store split history in database: `splits(id, old_ratio, new_ratio, block_number)`
- Update cap-table export to account for current split ratio
- Calculate adjusted balances: `display_balance = actual_balance * splitRatio`

**Task 2.1.5**: Test 7-for-1 split end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/split.test.ts`
- Test: Initial state (splitRatio = 1, balance = 10)
- Test: Execute 7-for-1 split (splitRatio = 7)
- Test: Verify adjusted balance = 70 (display), actual balance = 10 (contract)
- Test: Verify ownership percentages unchanged
- Test: Export cap-table shows adjusted balances

#### Phase 2.2: Corporate Actions - Symbol Change (120 minutes)

**Task 2.2.1**: Design symbol change implementation
- Duration: 20 minutes
- Evaluate: Mutable contract vs. proxy pattern vs. new deployment
- **Decision**: Add mutable symbol field with owner update function
- **Rationale**: Simplest approach for prototype, sufficient for demonstration
- Document limitations (production would use proxy pattern)

**Task 2.2.2**: Add symbol update function to contract
- Duration: 40 minutes
- Add `string public symbol` variable (separate from constructor)
- Add `event SymbolChanged(string oldSymbol, string newSymbol)`
- Add `changeSymbol(string memory newSymbol)` function (owner only)
- Validate newSymbol length > 0
- Emit event with old and new symbol

**Task 2.2.3**: Implement symbol change in issuer service
- Duration: 30 minutes
- Create `backend/issuer-service/symbol.ts`
- Implement `changeSymbol(contract: Contract, newSymbol: string, signer: Signer)`
- Call contract's `changeSymbol()` function
- Verify event emission
- Return transaction hash

**Task 2.2.4**: Test symbol change end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/symbol.test.ts`
- Test: Initial symbol (e.g., "ACME")
- Test: Change symbol to "ACMEX"
- Test: Verify contract symbol updated
- Test: Verify event emitted correctly
- Test: Verify balances unchanged

#### Phase 2.3: Operator Demo Interface (180 minutes)

**Task 2.3.1**: Enhance CLI with corporate action commands
- Duration: 30 minutes
- Add `split <ratio>` command (e.g., `split 7` for 7-for-1)
- Add `change-symbol <symbol>` command
- Add validation and error handling
- Show confirmation messages

**Task 2.3.2**: Create simple web UI (Optional but recommended)
- Duration: 90 minutes
- Setup Express.js server: `backend/ui/server.ts`
- Create basic HTML page with buttons:
  - "Approve Wallet" (input address)
  - "Mint Tokens" (input address, amount)
  - "Transfer" (input from, to, amount)
  - "Execute Split" (input ratio)
  - "Change Symbol" (input symbol)
  - "Export Cap-Table"
- Add JavaScript to call backend API
- Display results (transaction hashes, confirmations)

**Task 2.3.3**: Create scripted demo (Alternative to UI)
- Duration: 60 minutes
- Create `scripts/demo.ts` that runs automatically
- Sequence:
  1. Approve wallet A
  2. Approve wallet B
  3. Mint 10,000 tokens to A
  4. Mint 7,000 tokens to B
  5. Transfer 3,000 from A to B (SUCCESS)
  6. Try transfer to non-approved C (FAIL)
  7. Approve C
  8. Transfer to C (SUCCESS)
  9. Execute 7-for-1 split
  10. Change symbol
  11. Export cap-table
- Include delays and console output for visibility
- PowerShell: `npx tsx scripts/demo.ts`

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Write tests for corporate actions
- Duration: 45 minutes
- Test split with various ratios (2, 7, 10)
- Test split validation (prevent reverse splits, zero ratios)
- Test symbol change with various lengths
- Test symbol change validation (empty string rejection)
- Test admin-only access for both functions

**Task 2.4.2**: Write integration tests for full operator flow
- Duration: 30 minutes
- Test complete demo scenario in `tests/integration/demoFlow.test.ts`
- Verify all operations succeed in sequence
- Verify cap-table export matches expectations
- Test failure scenarios (unauthorized access)

**Task 2.4.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Run backend tests: `npm test` (if configured)
- Verify all Phase 1 + Phase 2 tests pass
- Document any edge cases or limitations

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity Early Submission - now with corporate actions"
  - Operator Flow Demo (50s): Approve wallets → Mint → Transfer (show success/fail)
  - Split Demo (40s): Execute 7-for-1 split, show adjusted balances, verify percentages unchanged
  - Symbol Change Demo (30s): Change symbol from "ACME" to "ACMEX", verify balances unchanged
  - Cap-Table Export (30s): Export final state, show ownership percentages
  - Conclusion (10s): "Demonstrates compliance gating, corporate actions, and operator workflows"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/CLI outputs
- Visual cues for what to demonstrate
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optimization

#### Phase 3.1: Comprehensive Test Suite (180 minutes)

**Task 3.1.1**: Write all required test scenarios
- Duration: 90 minutes
- Test: Approve wallet → Mint tokens → Verify balance
- Test: Transfer between two approved wallets → SUCCESS
- Test: Transfer from approved to non-approved → FAIL
- Test: Transfer from non-approved to approved → FAIL
- Test: Revoke approval → Previously approved wallet can no longer receive
- Test: Execute 7-for-1 split → All balances multiply by 7, total supply updates
- Test: Change symbol → Metadata updates, balances unchanged
- Test: Export cap-table at block N → Verify accuracy
- Test: Export cap-table at block N+10 → Verify changes reflected
- Test: Unauthorized wallet attempts admin action → FAIL

**Task 3.1.2**: Write edge case tests
- Duration: 45 minutes
- Test: Zero-amount transfers
- Test: Transfer to self (should fail or be blocked)
- Test: Mint to zero address (should fail)
- Test: Maximum uint256 values (overflow protection)
- Test: Empty allowlist operations
- Test: Batch operations with large arrays

**Task 3.1.3**: Write gas optimization tests
- Duration: 45 minutes
- Create `tests/gas/benchmarks.test.ts`
- Measure gas for: mint tokens, approve wallet, transfer (gated), revoke approval, stock split, symbol change
- Target: Mint <100k, Approve <50k, Transfer <100k, Revoke <50k, Symbol <50k
- Document actual costs vs. targets
- Propose optimizations if targets exceeded

#### Phase 3.2: Gas Optimization (120 minutes)

**Task 3.2.1**: Analyze gas usage patterns
- Duration: 30 minutes
- Run gas reports: `npx hardhat test --gas-reporter`
- Identify high-gas operations
- Look for unnecessary storage writes
- Identify loop optimizations

**Task 3.2.2**: Optimize contract code
- Duration: 60 minutes
- Use `immutable` for constants
- Pack structs efficiently
- Reduce storage operations
- Use events instead of storage where possible
- Optimize batch operations

**Task 3.2.3**: Generate final gas report
- Duration: 30 minutes
- Run comprehensive gas benchmarks
- Document all operation costs
- Compare against targets
- Document optimizations applied

#### Phase 3.3: Documentation (150 minutes)

**Task 3.3.1**: Write technical writeup (1-2 pages)
- Duration: 60 minutes
- Sections:
  - Chain selection rationale (why Ethereum Sepolia)
  - Corporate action implementation approach (virtual split, mutable symbol)
  - Key architectural decisions (allowlist gating, event indexing, SQLite storage)
  - Known limitations and risks (not regulatory-compliant, prototype only)
  - Gas costs and optimizations
  - Future improvements

**Task 3.3.2**: Create decision log
- Duration: 30 minutes
- Document all major decisions:
  - Why Ethereum over Polygon/Solana
  - Why virtual split over on-chain iteration
  - Why SQLite over PostgreSQL
  - Why CLI-first over UI-first
  - Tradeoffs and alternatives considered

**Task 3.3.3**: Update README with complete setup instructions
- Duration: 30 minutes
- Prerequisites (Node.js, npm, Hardhat, FFmpeg if needed)
- Environment setup (.env configuration)
- Installation steps
- Deployment instructions (local and Sepolia)
- Testing instructions
- Demo execution instructions
- Troubleshooting section

**Task 3.3.4**: Document AI tools and prompts used
- Duration: 30 minutes
- List all AI tools used (Cursor, GitHub Copilot, etc.)
- Document key prompts that generated code
- Acknowledge AI assistance in README

#### Phase 3.4: Final Integration & Polish (90 minutes)

**Task 3.4.1**: Run complete end-to-end test
- Duration: 30 minutes
- Deploy fresh contract to Sepolia
- Run full demo script
- Verify all operations succeed
- Export cap-table and verify accuracy
- Test failure scenarios

**Task 3.4.2**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove console.logs (or replace with proper logging)
- Add JSDoc comments to all functions

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass
- ✅ Gas costs documented and within targets (or justified)
- ✅ Contract deployed and address documented
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Decision log complete
- ✅ Demo script/UI works end-to-end
- ✅ No secrets in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity - Complete tokenized security prototype"
  - System Overview (20s): Architecture overview, key components
  - Gating Demo (30s): Approve → Mint → Transfer success/fail scenarios
  - Corporate Actions (40s): Split and symbol change, verify balances and metadata
  - Cap-Table Export (30s): Export with historical snapshots, verify accuracy
  - Testing Highlights (30s): Show test suite running, gas benchmarks
  - Conclusion (10s): "Complete system with compliance gating, corporate actions, and operator workflows"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of CLI/UI
- Highlight key technical achievements
- Show test results and gas benchmarks

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npx hardhat test`
- Verify 100% of required scenarios pass
- Run gas benchmarks: `npx hardhat test --gas-reporter`
- Document final test results
- Create test summary report

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Gated token contract deployed and functional
- ✅ Allowlist mechanism working (approve/revoke)
- ✅ Transfer restrictions enforced (both sender and recipient)
- ✅ Issuer service can approve wallets and mint tokens
- ✅ Event indexer listening and storing events
- ✅ Cap-table export working (CSV/JSON)
- ✅ Basic CLI interface functional
- ✅ Unit tests for core contract functions passing

### Early Submission (Phase 2) - November 7
- ✅ Stock split (7-for-1) implemented and tested
- ✅ Symbol change implemented and tested
- ✅ Operator demo interface (CLI or UI) functional
- ✅ Integration tests for full flow passing
- ✅ Demo script/UI demonstrates all operations

### Final Submission (Phase 3) - November 9
- ✅ All required test scenarios passing (10+ tests)
- ✅ Gas benchmarks documented (all operations)
- ✅ Technical writeup complete (1-2 pages)
- ✅ Decision log documenting key choices
- ✅ README with complete setup instructions
- ✅ No false compliance claims (disclaimer included)
- ✅ Reproducible setup (anyone can run demo)
- ✅ All code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Gas costs exceed targets | Medium | Document actual costs, propose optimizations, accept if reasonable for prototype |
| Testnet RPC rate limits | High | Use local Hardhat network for development, Sepolia only for final deployment |
| Event indexing gaps | Medium | Query from genesis block, handle reorgs with block confirmation |
| Contract deployment failures | High | Test deployment locally first, keep backup deployment script |
| Time constraints | High | MVP is safety net, corporate actions are enhancement, prioritize core features first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All contracts in `contracts/` directory
- [ ] All backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] Deployment scripts in `scripts/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation
- [ ] Gas report with benchmarks
- [ ] Test results summary

### Testing
- [ ] Unit tests for all contract functions
- [ ] Integration tests for full flows
- [ ] Gas benchmarks for all operations
- [ ] Test coverage for required scenarios

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployment addresses documented (if testnet)

---

## Notes

- **No Compliance Claims**: This is a technical prototype. Include disclaimer that it is NOT regulatory-compliant.
- **Testnet Only**: Never use real funds. Sepolia testnet or local Hardhat network only.
- **Secrets Management**: Use .env for private keys, never commit secrets.
- **Gas Optimization**: Targets are guidelines. Document actual costs and justify if above targets.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All contract code for optimizations
- Review: All backend code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Solidity best practices
- Verify: Code follows TypeScript best practices

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Smart contract security (common vulnerabilities)
- Check: Reentrancy vulnerabilities
- Check: Access control issues
- Check: Integer overflow/underflow protection
- Check: Private key management (backend)
- Check: Input validation and sanitization
- Review: OpenZeppelin contracts usage (secure)
- Document: Security findings and mitigations
- Run: Security linters/tools (Slither, Mythril if available)

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npx hardhat test`
- Verify: All tests pass (100% of required scenarios)
- Run gas benchmarks: `npx hardhat test --gas-reporter`
- Document final test results
- Create test summary report
```

```markdown
# Product Requirements Document: ChainEquity

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Tokenized Security Prototype with Compliance Gating

---

## Executive Summary

ChainEquity is a working prototype demonstrating how tokenized securities function on-chain with compliance gating, corporate actions, and operator workflows. Built as a technical demonstration (NOT regulatory-compliant), the system showcases blockchain primitives for equity management through transparent, automated compliance checks.

**Key Deliverables:**
- ✅ Gated token contract (allowlist-based transfer restrictions)
- ✅ Issuer service for wallet approval and token minting
- ✅ Event indexer producing cap-table snapshots
- ✅ Corporate action system (splits, symbol changes)
- ✅ Operator UI or CLI for demonstrations
- ✅ Test suite and gas benchmarks
- ✅ Technical writeup with decision rationale

**Success Targets:**
- **MVP (Nov 4)**: Core gated token + basic issuer service + event indexer
- **Early Submission (Nov 7)**: + Corporate actions + operator demo
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + gas optimization

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Core gated token contract, basic issuer service, event indexer working |
| **Early Submission** | Friday, November 7, 2025 | + Corporate actions (split + symbol change), operator demo UI/CLI |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive test suite, gas benchmarks, technical writeup |

---

## Technology Stack

### Blockchain Selection
**Recommendation: Ethereum Sepolia Testnet**
- **Rationale**: Most mature ecosystem, ERC-20 standard well-documented, extensive tooling
- **Alternative**: Polygon Mumbai (lower gas costs, EVM-compatible)
- **Local Option**: Hardhat Network for development, then deploy to Sepolia

### Smart Contract Stack
- **Language**: Solidity 0.8.20+
- **Framework**: Hardhat (testing, deployment, gas reporting)
- **Base Contracts**: OpenZeppelin Contracts v5.0+ (ERC-20 base)
- **Testing**: Hardhat + Chai + Ethers.js v6
- **Deployment**: Hardhat scripts with environment variables

### Backend/Indexer Stack
- **Language**: Node.js + TypeScript
- **Blockchain Library**: ethers.js v6
- **Event Listening**: WebSocket subscriptions or polling
- **Storage**: SQLite for cap-table snapshots (simple, no external deps)
- **CLI Framework**: Commander.js for CLI interface
- **Web Framework** (Optional UI): Express.js + React

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Git Hooks**: Husky + lint-staged
- **Documentation**: JSDoc for contracts, README.md for setup

---

## Architecture Overview

### System Components

```
ChainEquity/
├── contracts/
│   ├── GatedToken.sol              # ERC-20 with allowlist gating
│   ├── CorporateActions.sol         # Split and symbol change logic
│   └── interfaces/
│       └── IGatedToken.sol
├── backend/
│   ├── issuer-service/              # Wallet approval + minting
│   │   ├── approval.ts
│   │   ├── mint.ts
│   │   └── allowlist.ts
│   ├── indexer/                     # Event indexing + cap-table
│   │   ├── eventListener.ts
│   │   ├── capTable.ts
│   │   └── snapshot.ts
│   └── cli/                         # Operator CLI
│       └── commands.ts
├── tests/
│   ├── unit/
│   │   ├── GatedToken.test.ts
│   │   └── CorporateActions.test.ts
│   ├── integration/
│   │   ├── fullFlow.test.ts
│   │   └── capTable.test.ts
│   └── gas/
│       └── benchmarks.test.ts
├── scripts/
│   ├── deploy.ts
│   └── seed.ts
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Approval Flow**: User requests → Issuer Service → Contract allowlist update
2. **Minting Flow**: Issuer Service → Contract mint → Event emitted
3. **Transfer Flow**: User initiates → Contract checks allowlist → Transfer succeeds/fails
4. **Indexing Flow**: Contract events → Indexer listens → SQLite cap-table updated
5. **Export Flow**: Indexer queries → CSV/JSON generation

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core functionality working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize Hardhat project with TypeScript
- Duration: 15 minutes
- Create `chainequity/` directory
- Run `npx hardhat init` with TypeScript template
- Install OpenZeppelin Contracts: `npm install @openzeppelin/contracts`
- Install ethers.js v6: `npm install ethers@^6.0.0`
- Verify setup: `npx hardhat compile`
- PowerShell: `New-Item -ItemType Directory -Path chainequity; Set-Location chainequity; npx hardhat init`

**Task 1.1.2**: Configure Hardhat for Sepolia testnet
- Duration: 15 minutes
- Create `.env` file with `SEPOLIA_RPC_URL` and `PRIVATE_KEY`
- Update `hardhat.config.ts` with network config
- Add `.env.example` with placeholders
- Document setup in README.md

**Task 1.1.3**: Setup backend project structure
- Duration: 30 minutes
- Create `backend/` directory with `issuer-service/`, `indexer/`, `cli/`
- Initialize TypeScript: `tsc --init`
- Install dependencies: `npm install ethers commander sqlite3`
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path backend/issuer-service,backend/indexer,backend/cli; npm init -y`

**Task 1.1.4**: Install testing dependencies
- Duration: 15 minutes
- Install: `npm install --save-dev @nomicfoundation/hardhat-chai-matchers @nomicfoundation/hardhat-toolbox chai`
- Create `tests/` directory structure
- Verify test setup: `npx hardhat test`

**Task 1.1.5**: Create .gitignore and initial README
- Duration: 15 minutes
- Add `.env`, `node_modules/`, `artifacts/`, `cache/` to .gitignore
- Write README.md with setup instructions
- Document environment variables needed

#### Phase 1.2: Gated Token Contract (180 minutes)

**Task 1.2.1**: Create base ERC-20 contract extending OpenZeppelin
- Duration: 30 minutes
- Create `contracts/GatedToken.sol`
- Import `@openzeppelin/contracts/token/ERC20/ERC20.sol`
- Add constructor with name, symbol, initial supply
- Add owner/admin role using OpenZeppelin's Ownable
- Compile and verify: `npx hardhat compile`

**Task 1.2.2**: Implement allowlist mapping and modifiers
- Duration: 45 minutes
- Add `mapping(address => bool) public allowlist`
- Create modifier `onlyAllowlisted(address account)`
- Add events: `AllowlistUpdated(address indexed account, bool indexed status)`
- Add view function `isAllowlisted(address account) returns (bool)`
- Test allowlist logic in isolated test file

**Task 1.2.3**: Override transfer functions to check allowlist
- Duration: 60 minutes
- Override `transfer(address to, uint256 amount)` with allowlist check
- Override `transferFrom(address from, address to, uint256 amount)` with allowlist check
- Check BOTH sender AND recipient are on allowlist
- Revert with descriptive error if either not approved
- Emit `Transfer` event only on success
- Test transfer scenarios (approved→approved, approved→non-approved, non-approved→approved)

**Task 1.2.4**: Implement admin functions for allowlist management
- Duration: 45 minutes
- Add `approveWallet(address account)` function (owner only)
- Add `revokeWallet(address account)` function (owner only)
- Add `batchApprove(address[] calldata accounts)` for efficiency
- Add `batchRevoke(address[] calldata accounts)`
- Emit `AllowlistUpdated` events for all changes
- Add access control checks (only owner)

#### Phase 1.3: Issuer Service Backend (150 minutes)

**Task 1.3.1**: Create wallet approval service
- Duration: 45 minutes
- Create `backend/issuer-service/approval.ts`
- Implement `approveWallet(contract: Contract, wallet: string, signer: Signer)`
- Connect to contract using ethers.js
- Call `approveWallet()` function
- Handle errors and transaction confirmation
- Add retry logic for failed transactions

**Task 1.3.2**: Create token minting service
- Duration: 45 minutes
- Create `backend/issuer-service/mint.ts`
- Implement `mintTokens(contract: Contract, to: string, amount: bigint, signer: Signer)`
- Verify recipient is on allowlist before minting
- Call contract's mint function (need to add mint function to contract)
- Wait for transaction confirmation
- Return transaction hash

**Task 1.3.3**: Create allowlist query service
- Duration: 30 minutes
- Create `backend/issuer-service/allowlist.ts`
- Implement `checkAllowlistStatus(contract: Contract, wallet: string)`
- Query contract's `isAllowlisted()` view function
- Return boolean status
- Add batch query support for multiple wallets

**Task 1.3.4**: Add mint function to contract (if not present)
- Duration: 30 minutes
- Add `mint(address to, uint256 amount)` function to GatedToken.sol
- Ensure `to` address is on allowlist before minting
- Check owner/admin permissions (only owner can mint)
- Emit `Transfer(address(0), to, amount)` event
- Test minting in isolation

#### Phase 1.4: Event Indexer (120 minutes)

**Task 1.4.1**: Setup SQLite database schema
- Duration: 30 minutes
- Create `backend/indexer/database.ts`
- Define schema: `wallets(id, address, balance, last_updated)`
- Define schema: `transfers(id, from_address, to_address, amount, block_number, tx_hash)`
- Define schema: `mints(id, to_address, amount, block_number, tx_hash)`
- Initialize database with tables
- Add indexes on addresses and block_numbers

**Task 1.4.2**: Create event listener for Transfer events
- Duration: 45 minutes
- Create `backend/indexer/eventListener.ts`
- Setup ethers.js contract connection
- Listen for `Transfer(address indexed from, address indexed to, uint256 value)` events
- Parse event data and extract from/to/amount
- Store events in SQLite transfers table
- Handle reorgs (query from specific block)

**Task 1.4.3**: Create event listener for Mint events
- Duration: 30 minutes
- Listen for custom `Mint(address indexed to, uint256 amount)` events
- Parse and store in mints table
- Update wallet balances accordingly
- Handle duplicate events (check transaction hash)

**Task 1.4.4**: Implement balance aggregation
- Duration: 15 minutes
- Create function to aggregate current balance per wallet
- Query all transfers and mints from genesis
- Calculate net balance: sum(mints) + sum(received) - sum(sent)
- Update wallets table with current balances

#### Phase 1.5: Basic CLI Interface (90 minutes)

**Task 1.5.1**: Setup Commander.js CLI framework
- Duration: 20 minutes
- Install commander: `npm install commander`
- Create `backend/cli/index.ts` as entry point
- Setup command structure with commander
- Add `--help` and `--version` flags

**Task 1.5.2**: Implement approve-wallet command
- Duration: 20 minutes
- Add `approve-wallet <address>` command
- Connect to contract
- Call approval service
- Show transaction hash and confirmation

**Task 1.5.3**: Implement mint command
- Duration: 25 minutes
- Add `mint <address> <amount>` command
- Validate address format
- Parse amount to BigInt
- Call minting service
- Show confirmation

**Task 1.5.4**: Implement query-allowlist command
- Duration: 15 minutes
- Add `query-allowlist <address>` command
- Query contract for allowlist status
- Display status (approved/not approved)

**Task 1.5.5**: Implement export-cap-table command
- Duration: 10 minutes
- Add `export-cap-table [--format csv|json]` command
- Query database for all wallets and balances
- Calculate percentage ownership (balance / total supply)
- Export to CSV or JSON format
- PowerShell: `node backend/cli/index.ts export-cap-table --format csv > cap-table.csv`

#### Phase 1.6: Deployment & Testing (120 minutes)

**Task 1.6.1**: Deploy contract to Sepolia testnet
- Duration: 30 minutes
- Create `scripts/deploy.ts`
- Load private key from .env
- Deploy GatedToken contract with constructor params
- Verify deployment address
- Save contract address to .env or config file

**Task 1.6.2**: Write basic unit tests for contract
- Duration: 45 minutes
- Create `tests/unit/GatedToken.test.ts`
- Test: approve wallet → verify allowlist status
- Test: mint to approved wallet → verify balance
- Test: transfer between approved wallets → SUCCESS
- Test: transfer to non-approved wallet → FAIL
- Test: transfer from non-approved wallet → FAIL
- Use Hardhat's fixture pattern for setup

**Task 1.6.3**: Write integration test for full flow
- Duration: 30 minutes
- Create `tests/integration/fullFlow.test.ts`
- Test: Approve → Mint → Transfer → Export cap-table
- Verify all events are emitted correctly
- Verify balances are correct at each step

**Task 1.6.4**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Verify all tests pass
- Check coverage (if configured)
- Document any failing tests or known issues

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "This is ChainEquity MVP - a tokenized security prototype"
  - Wallet Approval Demo (30s): Show approve-wallet command, verify status
  - Minting Demo (30s): Mint tokens to approved wallet, show balance
  - Transfer Demo (40s): Transfer between approved wallets (SUCCESS), then try non-approved (FAIL)
  - Cap-Table Export (30s): Export current state to CSV, show ownership percentages
  - Conclusion (10s): "MVP demonstrates core gating, minting, and indexing functionality"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Format:
  ```
  [NARRATION]: "This is ChainEquity MVP..."
  • Highlights: Project introduction, system overview
  [NARRATION]: "First, we'll approve a wallet..."
  • Highlights: CLI approve-wallet command, contract interaction, transaction confirmation
  ```
- Include bullet points for each narration line showing key UI elements/actions
- Keep narration natural, 3 minutes total
- Include visual cues (what to show on screen)

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add corporate actions and operator demo interface

#### Phase 2.1: Corporate Actions - Stock Split (180 minutes)

**Task 2.1.1**: Design split implementation approach
- Duration: 30 minutes
- Evaluate: On-chain iteration vs. virtual split vs. contract migration
- **Decision**: Virtual split (adjust display logic, keep balances unchanged)
- **Rationale**: Gas-efficient, maintains contract state, simpler implementation
- Document tradeoffs in decision-log.md

**Task 2.1.2**: Implement split ratio tracking in contract
- Duration: 45 minutes
- Add `uint256 public splitRatio = 1` (1 = no split, 7 = 7-for-1 split)
- Add `mapping(address => uint256) public adjustedBalances` (optional for display)
- Add `event StockSplit(uint256 oldRatio, uint256 newRatio, uint256 blockNumber)`
- Add `executeSplit(uint256 newRatio)` function (owner only)
- Validate newRatio > 0 and newRatio > splitRatio (only forward splits)

**Task 2.1.3**: Implement split logic in issuer service
- Duration: 45 minutes
- Create `backend/issuer-service/split.ts`
- Implement `executeSplit(contract: Contract, ratio: bigint, signer: Signer)`
- Call contract's `executeSplit()` function
- Wait for event emission
- Update indexer to track split events

**Task 2.1.4**: Update indexer to handle split events
- Duration: 30 minutes
- Listen for `StockSplit` events
- Store split history in database: `splits(id, old_ratio, new_ratio, block_number)`
- Update cap-table export to account for current split ratio
- Calculate adjusted balances: `display_balance = actual_balance * splitRatio`

**Task 2.1.5**: Test 7-for-1 split end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/split.test.ts`
- Test: Initial state (splitRatio = 1, balance = 10)
- Test: Execute 7-for-1 split (splitRatio = 7)
- Test: Verify adjusted balance = 70 (display), actual balance = 10 (contract)
- Test: Verify ownership percentages unchanged
- Test: Export cap-table shows adjusted balances

#### Phase 2.2: Corporate Actions - Symbol Change (120 minutes)

**Task 2.2.1**: Design symbol change implementation
- Duration: 20 minutes
- Evaluate: Mutable contract vs. proxy pattern vs. new deployment
- **Decision**: Add mutable symbol field with owner update function
- **Rationale**: Simplest approach for prototype, sufficient for demonstration
- Document limitations (production would use proxy pattern)

**Task 2.2.2**: Add symbol update function to contract
- Duration: 40 minutes
- Add `string public symbol` variable (separate from constructor)
- Add `event SymbolChanged(string oldSymbol, string newSymbol)`
- Add `changeSymbol(string memory newSymbol)` function (owner only)
- Validate newSymbol length > 0
- Emit event with old and new symbol

**Task 2.2.3**: Implement symbol change in issuer service
- Duration: 30 minutes
- Create `backend/issuer-service/symbol.ts`
- Implement `changeSymbol(contract: Contract, newSymbol: string, signer: Signer)`
- Call contract's `changeSymbol()` function
- Verify event emission
- Return transaction hash

**Task 2.2.4**: Test symbol change end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/symbol.test.ts`
- Test: Initial symbol (e.g., "ACME")
- Test: Change symbol to "ACMEX"
- Test: Verify contract symbol updated
- Test: Verify event emitted correctly
- Test: Verify balances unchanged

#### Phase 2.3: Operator Demo Interface (180 minutes)

**Task 2.3.1**: Enhance CLI with corporate action commands
- Duration: 30 minutes
- Add `split <ratio>` command (e.g., `split 7` for 7-for-1)
- Add `change-symbol <symbol>` command
- Add validation and error handling
- Show confirmation messages

**Task 2.3.2**: Create simple web UI (Optional but recommended)
- Duration: 90 minutes
- Setup Express.js server: `backend/ui/server.ts`
- Create basic HTML page with buttons:
  - "Approve Wallet" (input address)
  - "Mint Tokens" (input address, amount)
  - "Transfer" (input from, to, amount)
  - "Execute Split" (input ratio)
  - "Change Symbol" (input symbol)
  - "Export Cap-Table"
- Add JavaScript to call backend API
- Display results (transaction hashes, confirmations)

**Task 2.3.3**: Create scripted demo (Alternative to UI)
- Duration: 60 minutes
- Create `scripts/demo.ts` that runs automatically
- Sequence:
  1. Approve wallet A
  2. Approve wallet B
  3. Mint 10,000 tokens to A
  4. Mint 7,000 tokens to B
  5. Transfer 3,000 from A to B (SUCCESS)
  6. Try transfer to non-approved C (FAIL)
  7. Approve C
  8. Transfer to C (SUCCESS)
  9. Execute 7-for-1 split
  10. Change symbol
  11. Export cap-table
- Include delays and console output for visibility
- PowerShell: `npx tsx scripts/demo.ts`

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Write tests for corporate actions
- Duration: 45 minutes
- Test split with various ratios (2, 7, 10)
- Test split validation (prevent reverse splits, zero ratios)
- Test symbol change with various lengths
- Test symbol change validation (empty string rejection)
- Test admin-only access for both functions

**Task 2.4.2**: Write integration tests for full operator flow
- Duration: 30 minutes
- Test complete demo scenario in `tests/integration/demoFlow.test.ts`
- Verify all operations succeed in sequence
- Verify cap-table export matches expectations
- Test failure scenarios (unauthorized access)

**Task 2.4.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Run backend tests: `npm test` (if configured)
- Verify all Phase 1 + Phase 2 tests pass
- Document any edge cases or limitations

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity Early Submission - now with corporate actions"
  - Operator Flow Demo (50s): Approve wallets → Mint → Transfer (show success/fail)
  - Split Demo (40s): Execute 7-for-1 split, show adjusted balances, verify percentages unchanged
  - Symbol Change Demo (30s): Change symbol from "ACME" to "ACMEX", verify balances unchanged
  - Cap-Table Export (30s): Export final state, show ownership percentages
  - Conclusion (10s): "Demonstrates compliance gating, corporate actions, and operator workflows"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/CLI outputs
- Visual cues for what to demonstrate
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optimization

#### Phase 3.1: Comprehensive Test Suite (180 minutes)

**Task 3.1.1**: Write all required test scenarios
- Duration: 90 minutes
- Test: Approve wallet → Mint tokens → Verify balance
- Test: Transfer between two approved wallets → SUCCESS
- Test: Transfer from approved to non-approved → FAIL
- Test: Transfer from non-approved to approved → FAIL
- Test: Revoke approval → Previously approved wallet can no longer receive
- Test: Execute 7-for-1 split → All balances multiply by 7, total supply updates
- Test: Change symbol → Metadata updates, balances unchanged
- Test: Export cap-table at block N → Verify accuracy
- Test: Export cap-table at block N+10 → Verify changes reflected
- Test: Unauthorized wallet attempts admin action → FAIL

**Task 3.1.2**: Write edge case tests
- Duration: 45 minutes
- Test: Zero-amount transfers
- Test: Transfer to self (should fail or be blocked)
- Test: Mint to zero address (should fail)
- Test: Maximum uint256 values (overflow protection)
- Test: Empty allowlist operations
- Test: Batch operations with large arrays

**Task 3.1.3**: Write gas optimization tests
- Duration: 45 minutes
- Create `tests/gas/benchmarks.test.ts`
- Measure gas for: mint tokens, approve wallet, transfer (gated), revoke approval, stock split, symbol change
- Target: Mint <100k, Approve <50k, Transfer <100k, Revoke <50k, Symbol <50k
- Document actual costs vs. targets
- Propose optimizations if targets exceeded

#### Phase 3.2: Gas Optimization (120 minutes)

**Task 3.2.1**: Analyze gas usage patterns
- Duration: 30 minutes
- Run gas reports: `npx hardhat test --gas-reporter`
- Identify high-gas operations
- Look for unnecessary storage writes
- Identify loop optimizations

**Task 3.2.2**: Optimize contract code
- Duration: 60 minutes
- Use `immutable` for constants
- Pack structs efficiently
- Reduce storage operations
- Use events instead of storage where possible
- Optimize batch operations

**Task 3.2.3**: Generate final gas report
- Duration: 30 minutes
- Run comprehensive gas benchmarks
- Document all operation costs
- Compare against targets
- Document optimizations applied

#### Phase 3.3: Documentation (150 minutes)

**Task 3.3.1**: Write technical writeup (1-2 pages)
- Duration: 60 minutes
- Sections:
  - Chain selection rationale (why Ethereum Sepolia)
  - Corporate action implementation approach (virtual split, mutable symbol)
  - Key architectural decisions (allowlist gating, event indexing, SQLite storage)
  - Known limitations and risks (not regulatory-compliant, prototype only)
  - Gas costs and optimizations
  - Future improvements

**Task 3.3.2**: Create decision log
- Duration: 30 minutes
- Document all major decisions:
  - Why Ethereum over Polygon/Solana
  - Why virtual split over on-chain iteration
  - Why SQLite over PostgreSQL
  - Why CLI-first over UI-first
  - Tradeoffs and alternatives considered

**Task 3.3.3**: Update README with complete setup instructions
- Duration: 30 minutes
- Prerequisites (Node.js, npm, Hardhat, FFmpeg if needed)
- Environment setup (.env configuration)
- Installation steps
- Deployment instructions (local and Sepolia)
- Testing instructions
- Demo execution instructions
- Troubleshooting section

**Task 3.3.4**: Document AI tools and prompts used
- Duration: 30 minutes
- List all AI tools used (Cursor, GitHub Copilot, etc.)
- Document key prompts that generated code
- Acknowledge AI assistance in README

#### Phase 3.4: Final Integration & Polish (90 minutes)

**Task 3.4.1**: Run complete end-to-end test
- Duration: 30 minutes
- Deploy fresh contract to Sepolia
- Run full demo script
- Verify all operations succeed
- Export cap-table and verify accuracy
- Test failure scenarios

**Task 3.4.2**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove console.logs (or replace with proper logging)
- Add JSDoc comments to all functions

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass
- ✅ Gas costs documented and within targets (or justified)
- ✅ Contract deployed and address documented
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Decision log complete
- ✅ Demo script/UI works end-to-end
- ✅ No secrets in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity - Complete tokenized security prototype"
  - System Overview (20s): Architecture overview, key components
  - Gating Demo (30s): Approve → Mint → Transfer success/fail scenarios
  - Corporate Actions (40s): Split and symbol change, verify balances and metadata
  - Cap-Table Export (30s): Export with historical snapshots, verify accuracy
  - Testing Highlights (30s): Show test suite running, gas benchmarks
  - Conclusion (10s): "Complete system with compliance gating, corporate actions, and operator workflows"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of CLI/UI
- Highlight key technical achievements
- Show test results and gas benchmarks

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npx hardhat test`
- Verify 100% of required scenarios pass
- Run gas benchmarks: `npx hardhat test --gas-reporter`
- Document final test results
- Create test summary report

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Gated token contract deployed and functional
- ✅ Allowlist mechanism working (approve/revoke)
- ✅ Transfer restrictions enforced (both sender and recipient)
- ✅ Issuer service can approve wallets and mint tokens
- ✅ Event indexer listening and storing events
- ✅ Cap-table export working (CSV/JSON)
- ✅ Basic CLI interface functional
- ✅ Unit tests for core contract functions passing

### Early Submission (Phase 2) - November 7
- ✅ Stock split (7-for-1) implemented and tested
- ✅ Symbol change implemented and tested
- ✅ Operator demo interface (CLI or UI) functional
- ✅ Integration tests for full flow passing
- ✅ Demo script/UI demonstrates all operations

### Final Submission (Phase 3) - November 9
- ✅ All required test scenarios passing (10+ tests)
- ✅ Gas benchmarks documented (all operations)
- ✅ Technical writeup complete (1-2 pages)
- ✅ Decision log documenting key choices
- ✅ README with complete setup instructions
- ✅ No false compliance claims (disclaimer included)
- ✅ Reproducible setup (anyone can run demo)
- ✅ All code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Gas costs exceed targets | Medium | Document actual costs, propose optimizations, accept if reasonable for prototype |
| Testnet RPC rate limits | High | Use local Hardhat network for development, Sepolia only for final deployment |
| Event indexing gaps | Medium | Query from genesis block, handle reorgs with block confirmation |
| Contract deployment failures | High | Test deployment locally first, keep backup deployment script |
| Time constraints | High | MVP is safety net, corporate actions are enhancement, prioritize core features first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All contracts in `contracts/` directory
- [ ] All backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] Deployment scripts in `scripts/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation
- [ ] Gas report with benchmarks
- [ ] Test results summary

### Testing
- [ ] Unit tests for all contract functions
- [ ] Integration tests for full flows
- [ ] Gas benchmarks for all operations
- [ ] Test coverage for required scenarios

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployment addresses documented (if testnet)

---

## Notes

- **No Compliance Claims**: This is a technical prototype. Include disclaimer that it is NOT regulatory-compliant.
- **Testnet Only**: Never use real funds. Sepolia testnet or local Hardhat network only.
- **Secrets Management**: Use .env for private keys, never commit secrets.
- **Gas Optimization**: Targets are guidelines. Document actual costs and justify if above targets.

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: ChainEquity

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Tokenized Security Prototype with Compliance Gating

---

## Executive Summary

ChainEquity is a working prototype demonstrating how tokenized securities function on-chain with compliance gating, corporate actions, and operator workflows. Built as a technical demonstration (NOT regulatory-compliant), the system showcases blockchain primitives for equity management through transparent, automated compliance checks.

**Key Deliverables:**
- ✅ Gated token contract (allowlist-based transfer restrictions)
- ✅ Issuer service for wallet approval and token minting
- ✅ Event indexer producing cap-table snapshots
- ✅ Corporate action system (splits, symbol changes)
- ✅ Operator UI or CLI for demonstrations
- ✅ Test suite and gas benchmarks
- ✅ Technical writeup with decision rationale

**Success Targets:**
- **MVP (Nov 4)**: Core gated token + basic issuer service + event indexer
- **Early Submission (Nov 7)**: + Corporate actions + operator demo
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + gas optimization

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Core gated token contract, basic issuer service, event indexer working |
| **Early Submission** | Friday, November 7, 2025 | + Corporate actions (split + symbol change), operator demo UI/CLI |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive test suite, gas benchmarks, technical writeup |

---

## Technology Stack

### Blockchain Selection
**Recommendation: Ethereum Sepolia Testnet**
- **Rationale**: Most mature ecosystem, ERC-20 standard well-documented, extensive tooling
- **Alternative**: Polygon Mumbai (lower gas costs, EVM-compatible)
- **Local Option**: Hardhat Network for development, then deploy to Sepolia

### Smart Contract Stack
- **Language**: Solidity 0.8.20+
- **Framework**: Hardhat (testing, deployment, gas reporting)
- **Base Contracts**: OpenZeppelin Contracts v5.0+ (ERC-20 base)
- **Testing**: Hardhat + Chai + Ethers.js v6
- **Deployment**: Hardhat scripts with environment variables

### Backend/Indexer Stack
- **Language**: Node.js + TypeScript
- **Blockchain Library**: ethers.js v6
- **Event Listening**: WebSocket subscriptions or polling
- **Storage**: SQLite for cap-table snapshots (simple, no external deps)
- **CLI Framework**: Commander.js for CLI interface
- **Web Framework** (Optional UI): Express.js + React

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Git Hooks**: Husky + lint-staged
- **Documentation**: JSDoc for contracts, README.md for setup

---

## Architecture Overview

### System Components

```
ChainEquity/
├── contracts/
│   ├── GatedToken.sol              # ERC-20 with allowlist gating
│   ├── CorporateActions.sol         # Split and symbol change logic
│   └── interfaces/
│       └── IGatedToken.sol
├── backend/
│   ├── issuer-service/              # Wallet approval + minting
│   │   ├── approval.ts
│   │   ├── mint.ts
│   │   └── allowlist.ts
│   ├── indexer/                     # Event indexing + cap-table
│   │   ├── eventListener.ts
│   │   ├── capTable.ts
│   │   └── snapshot.ts
│   └── cli/                         # Operator CLI
│       └── commands.ts
├── tests/
│   ├── unit/
│   │   ├── GatedToken.test.ts
│   │   └── CorporateActions.test.ts
│   ├── integration/
│   │   ├── fullFlow.test.ts
│   │   └── capTable.test.ts
│   └── gas/
│       └── benchmarks.test.ts
├── scripts/
│   ├── deploy.ts
│   └── seed.ts
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Approval Flow**: User requests → Issuer Service → Contract allowlist update
2. **Minting Flow**: Issuer Service → Contract mint → Event emitted
3. **Transfer Flow**: User initiates → Contract checks allowlist → Transfer succeeds/fails
4. **Indexing Flow**: Contract events → Indexer listens → SQLite cap-table updated
5. **Export Flow**: Indexer queries → CSV/JSON generation

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core functionality working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize Hardhat project with TypeScript
- Duration: 15 minutes
- Create `chainequity/` directory
- Run `npx hardhat init` with TypeScript template
- Install OpenZeppelin Contracts: `npm install @openzeppelin/contracts`
- Install ethers.js v6: `npm install ethers@^6.0.0`
- Verify setup: `npx hardhat compile`
- PowerShell: `New-Item -ItemType Directory -Path chainequity; Set-Location chainequity; npx hardhat init`

**Task 1.1.2**: Configure Hardhat for Sepolia testnet
- Duration: 15 minutes
- Create `.env` file with `SEPOLIA_RPC_URL` and `PRIVATE_KEY`
- Update `hardhat.config.ts` with network config
- Add `.env.example` with placeholders
- Document setup in README.md

**Task 1.1.3**: Setup backend project structure
- Duration: 30 minutes
- Create `backend/` directory with `issuer-service/`, `indexer/`, `cli/`
- Initialize TypeScript: `tsc --init`
- Install dependencies: `npm install ethers commander sqlite3`
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path backend/issuer-service,backend/indexer,backend/cli; npm init -y`

**Task 1.1.4**: Install testing dependencies
- Duration: 15 minutes
- Install: `npm install --save-dev @nomicfoundation/hardhat-chai-matchers @nomicfoundation/hardhat-toolbox chai`
- Create `tests/` directory structure
- Verify test setup: `npx hardhat test`

**Task 1.1.5**: Create .gitignore and initial README
- Duration: 15 minutes
- Add `.env`, `node_modules/`, `artifacts/`, `cache/` to .gitignore
- Write README.md with setup instructions
- Document environment variables needed

#### Phase 1.2: Gated Token Contract (180 minutes)

**Task 1.2.1**: Create base ERC-20 contract extending OpenZeppelin
- Duration: 30 minutes
- Create `contracts/GatedToken.sol`
- Import `@openzeppelin/contracts/token/ERC20/ERC20.sol`
- Add constructor with name, symbol, initial supply
- Add owner/admin role using OpenZeppelin's Ownable
- Compile and verify: `npx hardhat compile`

**Task 1.2.2**: Implement allowlist mapping and modifiers
- Duration: 45 minutes
- Add `mapping(address => bool) public allowlist`
- Create modifier `onlyAllowlisted(address account)`
- Add events: `AllowlistUpdated(address indexed account, bool indexed status)`
- Add view function `isAllowlisted(address account) returns (bool)`
- Test allowlist logic in isolated test file

**Task 1.2.3**: Override transfer functions to check allowlist
- Duration: 60 minutes
- Override `transfer(address to, uint256 amount)` with allowlist check
- Override `transferFrom(address from, address to, uint256 amount)` with allowlist check
- Check BOTH sender AND recipient are on allowlist
- Revert with descriptive error if either not approved
- Emit `Transfer` event only on success
- Test transfer scenarios (approved→approved, approved→non-approved, non-approved→approved)

**Task 1.2.4**: Implement admin functions for allowlist management
- Duration: 45 minutes
- Add `approveWallet(address account)` function (owner only)
- Add `revokeWallet(address account)` function (owner only)
- Add `batchApprove(address[] calldata accounts)` for efficiency
- Add `batchRevoke(address[] calldata accounts)`
- Emit `AllowlistUpdated` events for all changes
- Add access control checks (only owner)

#### Phase 1.3: Issuer Service Backend (150 minutes)

**Task 1.3.1**: Create wallet approval service
- Duration: 45 minutes
- Create `backend/issuer-service/approval.ts`
- Implement `approveWallet(contract: Contract, wallet: string, signer: Signer)`
- Connect to contract using ethers.js
- Call `approveWallet()` function
- Handle errors and transaction confirmation
- Add retry logic for failed transactions

**Task 1.3.2**: Create token minting service
- Duration: 45 minutes
- Create `backend/issuer-service/mint.ts`
- Implement `mintTokens(contract: Contract, to: string, amount: bigint, signer: Signer)`
- Verify recipient is on allowlist before minting
- Call contract's mint function (need to add mint function to contract)
- Wait for transaction confirmation
- Return transaction hash

**Task 1.3.3**: Create allowlist query service
- Duration: 30 minutes
- Create `backend/issuer-service/allowlist.ts`
- Implement `checkAllowlistStatus(contract: Contract, wallet: string)`
- Query contract's `isAllowlisted()` view function
- Return boolean status
- Add batch query support for multiple wallets

**Task 1.3.4**: Add mint function to contract (if not present)
- Duration: 30 minutes
- Add `mint(address to, uint256 amount)` function to GatedToken.sol
- Ensure `to` address is on allowlist before minting
- Check owner/admin permissions (only owner can mint)
- Emit `Transfer(address(0), to, amount)` event
- Test minting in isolation

#### Phase 1.4: Event Indexer (120 minutes)

**Task 1.4.1**: Setup SQLite database schema
- Duration: 30 minutes
- Create `backend/indexer/database.ts`
- Define schema: `wallets(id, address, balance, last_updated)`
- Define schema: `transfers(id, from_address, to_address, amount, block_number, tx_hash)`
- Define schema: `mints(id, to_address, amount, block_number, tx_hash)`
- Initialize database with tables
- Add indexes on addresses and block_numbers

**Task 1.4.2**: Create event listener for Transfer events
- Duration: 45 minutes
- Create `backend/indexer/eventListener.ts`
- Setup ethers.js contract connection
- Listen for `Transfer(address indexed from, address indexed to, uint256 value)` events
- Parse event data and extract from/to/amount
- Store events in SQLite transfers table
- Handle reorgs (query from specific block)

**Task 1.4.3**: Create event listener for Mint events
- Duration: 30 minutes
- Listen for custom `Mint(address indexed to, uint256 amount)` events
- Parse and store in mints table
- Update wallet balances accordingly
- Handle duplicate events (check transaction hash)

**Task 1.4.4**: Implement balance aggregation
- Duration: 15 minutes
- Create function to aggregate current balance per wallet
- Query all transfers and mints from genesis
- Calculate net balance: sum(mints) + sum(received) - sum(sent)
- Update wallets table with current balances

#### Phase 1.5: Basic CLI Interface (90 minutes)

**Task 1.5.1**: Setup Commander.js CLI framework
- Duration: 20 minutes
- Install commander: `npm install commander`
- Create `backend/cli/index.ts` as entry point
- Setup command structure with commander
- Add `--help` and `--version` flags

**Task 1.5.2**: Implement approve-wallet command
- Duration: 20 minutes
- Add `approve-wallet <address>` command
- Connect to contract
- Call approval service
- Show transaction hash and confirmation

**Task 1.5.3**: Implement mint command
- Duration: 25 minutes
- Add `mint <address> <amount>` command
- Validate address format
- Parse amount to BigInt
- Call minting service
- Show confirmation

**Task 1.5.4**: Implement query-allowlist command
- Duration: 15 minutes
- Add `query-allowlist <address>` command
- Query contract for allowlist status
- Display status (approved/not approved)

**Task 1.5.5**: Implement export-cap-table command
- Duration: 10 minutes
- Add `export-cap-table [--format csv|json]` command
- Query database for all wallets and balances
- Calculate percentage ownership (balance / total supply)
- Export to CSV or JSON format
- PowerShell: `node backend/cli/index.ts export-cap-table --format csv > cap-table.csv`

#### Phase 1.6: Deployment & Testing (120 minutes)

**Task 1.6.1**: Deploy contract to Sepolia testnet
- Duration: 30 minutes
- Create `scripts/deploy.ts`
- Load private key from .env
- Deploy GatedToken contract with constructor params
- Verify deployment address
- Save contract address to .env or config file

**Task 1.6.2**: Write basic unit tests for contract
- Duration: 45 minutes
- Create `tests/unit/GatedToken.test.ts`
- Test: approve wallet → verify allowlist status
- Test: mint to approved wallet → verify balance
- Test: transfer between approved wallets → SUCCESS
- Test: transfer to non-approved wallet → FAIL
- Test: transfer from non-approved wallet → FAIL
- Use Hardhat's fixture pattern for setup

**Task 1.6.3**: Write integration test for full flow
- Duration: 30 minutes
- Create `tests/integration/fullFlow.test.ts`
- Test: Approve → Mint → Transfer → Export cap-table
- Verify all events are emitted correctly
- Verify balances are correct at each step

**Task 1.6.4**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Verify all tests pass
- Check coverage (if configured)
- Document any failing tests or known issues

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "This is ChainEquity MVP - a tokenized security prototype"
  - Wallet Approval Demo (30s): Show approve-wallet command, verify status
  - Minting Demo (30s): Mint tokens to approved wallet, show balance
  - Transfer Demo (40s): Transfer between approved wallets (SUCCESS), then try non-approved (FAIL)
  - Cap-Table Export (30s): Export current state to CSV, show ownership percentages
  - Conclusion (10s): "MVP demonstrates core gating, minting, and indexing functionality"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Format:
  ```
  [NARRATION]: "This is ChainEquity MVP..."
  • Highlights: Project introduction, system overview
  [NARRATION]: "First, we'll approve a wallet..."
  • Highlights: CLI approve-wallet command, contract interaction, transaction confirmation
  ```
- Include bullet points for each narration line showing key UI elements/actions
- Keep narration natural, 3 minutes total
- Include visual cues (what to show on screen)

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add corporate actions and operator demo interface

#### Phase 2.1: Corporate Actions - Stock Split (180 minutes)

**Task 2.1.1**: Design split implementation approach
- Duration: 30 minutes
- Evaluate: On-chain iteration vs. virtual split vs. contract migration
- **Decision**: Virtual split (adjust display logic, keep balances unchanged)
- **Rationale**: Gas-efficient, maintains contract state, simpler implementation
- Document tradeoffs in decision-log.md

**Task 2.1.2**: Implement split ratio tracking in contract
- Duration: 45 minutes
- Add `uint256 public splitRatio = 1` (1 = no split, 7 = 7-for-1 split)
- Add `mapping(address => uint256) public adjustedBalances` (optional for display)
- Add `event StockSplit(uint256 oldRatio, uint256 newRatio, uint256 blockNumber)`
- Add `executeSplit(uint256 newRatio)` function (owner only)
- Validate newRatio > 0 and newRatio > splitRatio (only forward splits)

**Task 2.1.3**: Implement split logic in issuer service
- Duration: 45 minutes
- Create `backend/issuer-service/split.ts`
- Implement `executeSplit(contract: Contract, ratio: bigint, signer: Signer)`
- Call contract's `executeSplit()` function
- Wait for event emission
- Update indexer to track split events

**Task 2.1.4**: Update indexer to handle split events
- Duration: 30 minutes
- Listen for `StockSplit` events
- Store split history in database: `splits(id, old_ratio, new_ratio, block_number)`
- Update cap-table export to account for current split ratio
- Calculate adjusted balances: `display_balance = actual_balance * splitRatio`

**Task 2.1.5**: Test 7-for-1 split end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/split.test.ts`
- Test: Initial state (splitRatio = 1, balance = 10)
- Test: Execute 7-for-1 split (splitRatio = 7)
- Test: Verify adjusted balance = 70 (display), actual balance = 10 (contract)
- Test: Verify ownership percentages unchanged
- Test: Export cap-table shows adjusted balances

#### Phase 2.2: Corporate Actions - Symbol Change (120 minutes)

**Task 2.2.1**: Design symbol change implementation
- Duration: 20 minutes
- Evaluate: Mutable contract vs. proxy pattern vs. new deployment
- **Decision**: Add mutable symbol field with owner update function
- **Rationale**: Simplest approach for prototype, sufficient for demonstration
- Document limitations (production would use proxy pattern)

**Task 2.2.2**: Add symbol update function to contract
- Duration: 40 minutes
- Add `string public symbol` variable (separate from constructor)
- Add `event SymbolChanged(string oldSymbol, string newSymbol)`
- Add `changeSymbol(string memory newSymbol)` function (owner only)
- Validate newSymbol length > 0
- Emit event with old and new symbol

**Task 2.2.3**: Implement symbol change in issuer service
- Duration: 30 minutes
- Create `backend/issuer-service/symbol.ts`
- Implement `changeSymbol(contract: Contract, newSymbol: string, signer: Signer)`
- Call contract's `changeSymbol()` function
- Verify event emission
- Return transaction hash

**Task 2.2.4**: Test symbol change end-to-end
- Duration: 30 minutes
- Write test: `tests/integration/symbol.test.ts`
- Test: Initial symbol (e.g., "ACME")
- Test: Change symbol to "ACMEX"
- Test: Verify contract symbol updated
- Test: Verify event emitted correctly
- Test: Verify balances unchanged

#### Phase 2.3: Operator Demo Interface (180 minutes)

**Task 2.3.1**: Enhance CLI with corporate action commands
- Duration: 30 minutes
- Add `split <ratio>` command (e.g., `split 7` for 7-for-1)
- Add `change-symbol <symbol>` command
- Add validation and error handling
- Show confirmation messages

**Task 2.3.2**: Create simple web UI (Optional but recommended)
- Duration: 90 minutes
- Setup Express.js server: `backend/ui/server.ts`
- Create basic HTML page with buttons:
  - "Approve Wallet" (input address)
  - "Mint Tokens" (input address, amount)
  - "Transfer" (input from, to, amount)
  - "Execute Split" (input ratio)
  - "Change Symbol" (input symbol)
  - "Export Cap-Table"
- Add JavaScript to call backend API
- Display results (transaction hashes, confirmations)

**Task 2.3.3**: Create scripted demo (Alternative to UI)
- Duration: 60 minutes
- Create `scripts/demo.ts` that runs automatically
- Sequence:
  1. Approve wallet A
  2. Approve wallet B
  3. Mint 10,000 tokens to A
  4. Mint 7,000 tokens to B
  5. Transfer 3,000 from A to B (SUCCESS)
  6. Try transfer to non-approved C (FAIL)
  7. Approve C
  8. Transfer to C (SUCCESS)
  9. Execute 7-for-1 split
  10. Change symbol
  11. Export cap-table
- Include delays and console output for visibility
- PowerShell: `npx tsx scripts/demo.ts`

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Write tests for corporate actions
- Duration: 45 minutes
- Test split with various ratios (2, 7, 10)
- Test split validation (prevent reverse splits, zero ratios)
- Test symbol change with various lengths
- Test symbol change validation (empty string rejection)
- Test admin-only access for both functions

**Task 2.4.2**: Write integration tests for full operator flow
- Duration: 30 minutes
- Test complete demo scenario in `tests/integration/demoFlow.test.ts`
- Verify all operations succeed in sequence
- Verify cap-table export matches expectations
- Test failure scenarios (unauthorized access)

**Task 2.4.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npx hardhat test`
- Run backend tests: `npm test` (if configured)
- Verify all Phase 1 + Phase 2 tests pass
- Document any edge cases or limitations

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity Early Submission - now with corporate actions"
  - Operator Flow Demo (50s): Approve wallets → Mint → Transfer (show success/fail)
  - Split Demo (40s): Execute 7-for-1 split, show adjusted balances, verify percentages unchanged
  - Symbol Change Demo (30s): Change symbol from "ACME" to "ACMEX", verify balances unchanged
  - Cap-Table Export (30s): Export final state, show ownership percentages
  - Conclusion (10s): "Demonstrates compliance gating, corporate actions, and operator workflows"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/CLI outputs
- Visual cues for what to demonstrate
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optimization

#### Phase 3.1: Comprehensive Test Suite (180 minutes)

**Task 3.1.1**: Write all required test scenarios
- Duration: 90 minutes
- Test: Approve wallet → Mint tokens → Verify balance
- Test: Transfer between two approved wallets → SUCCESS
- Test: Transfer from approved to non-approved → FAIL
- Test: Transfer from non-approved to approved → FAIL
- Test: Revoke approval → Previously approved wallet can no longer receive
- Test: Execute 7-for-1 split → All balances multiply by 7, total supply updates
- Test: Change symbol → Metadata updates, balances unchanged
- Test: Export cap-table at block N → Verify accuracy
- Test: Export cap-table at block N+10 → Verify changes reflected
- Test: Unauthorized wallet attempts admin action → FAIL

**Task 3.1.2**: Write edge case tests
- Duration: 45 minutes
- Test: Zero-amount transfers
- Test: Transfer to self (should fail or be blocked)
- Test: Mint to zero address (should fail)
- Test: Maximum uint256 values (overflow protection)
- Test: Empty allowlist operations
- Test: Batch operations with large arrays

**Task 3.1.3**: Write gas optimization tests
- Duration: 45 minutes
- Create `tests/gas/benchmarks.test.ts`
- Measure gas for: mint tokens, approve wallet, transfer (gated), revoke approval, stock split, symbol change
- Target: Mint <100k, Approve <50k, Transfer <100k, Revoke <50k, Symbol <50k
- Document actual costs vs. targets
- Propose optimizations if targets exceeded

#### Phase 3.2: Gas Optimization (120 minutes)

**Task 3.2.1**: Analyze gas usage patterns
- Duration: 30 minutes
- Run gas reports: `npx hardhat test --gas-reporter`
- Identify high-gas operations
- Look for unnecessary storage writes
- Identify loop optimizations

**Task 3.2.2**: Optimize contract code
- Duration: 60 minutes
- Use `immutable` for constants
- Pack structs efficiently
- Reduce storage operations
- Use events instead of storage where possible
- Optimize batch operations

**Task 3.2.3**: Generate final gas report
- Duration: 30 minutes
- Run comprehensive gas benchmarks
- Document all operation costs
- Compare against targets
- Document optimizations applied

#### Phase 3.3: Documentation (150 minutes)

**Task 3.3.1**: Write technical writeup (1-2 pages)
- Duration: 60 minutes
- Sections:
  - Chain selection rationale (why Ethereum Sepolia)
  - Corporate action implementation approach (virtual split, mutable symbol)
  - Key architectural decisions (allowlist gating, event indexing, SQLite storage)
  - Known limitations and risks (not regulatory-compliant, prototype only)
  - Gas costs and optimizations
  - Future improvements

**Task 3.3.2**: Create decision log
- Duration: 30 minutes
- Document all major decisions:
  - Why Ethereum over Polygon/Solana
  - Why virtual split over on-chain iteration
  - Why SQLite over PostgreSQL
  - Why CLI-first over UI-first
  - Tradeoffs and alternatives considered

**Task 3.3.3**: Update README with complete setup instructions
- Duration: 30 minutes
- Prerequisites (Node.js, npm, Hardhat, FFmpeg if needed)
- Environment setup (.env configuration)
- Installation steps
- Deployment instructions (local and Sepolia)
- Testing instructions
- Demo execution instructions
- Troubleshooting section

**Task 3.3.4**: Document AI tools and prompts used
- Duration: 30 minutes
- List all AI tools used (Cursor, GitHub Copilot, etc.)
- Document key prompts that generated code
- Acknowledge AI assistance in README

#### Phase 3.4: Final Integration & Polish (90 minutes)

**Task 3.4.1**: Run complete end-to-end test
- Duration: 30 minutes
- Deploy fresh contract to Sepolia
- Run full demo script
- Verify all operations succeed
- Export cap-table and verify accuracy
- Test failure scenarios

**Task 3.4.2**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove console.logs (or replace with proper logging)
- Add JSDoc comments to all functions

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass
- ✅ Gas costs documented and within targets (or justified)
- ✅ Contract deployed and address documented
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Decision log complete
- ✅ Demo script/UI works end-to-end
- ✅ No secrets in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "ChainEquity - Complete tokenized security prototype"
  - System Overview (20s): Architecture overview, key components
  - Gating Demo (30s): Approve → Mint → Transfer success/fail scenarios
  - Corporate Actions (40s): Split and symbol change, verify balances and metadata
  - Cap-Table Export (30s): Export with historical snapshots, verify accuracy
  - Testing Highlights (30s): Show test suite running, gas benchmarks
  - Conclusion (10s): "Complete system with compliance gating, corporate actions, and operator workflows"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of CLI/UI
- Highlight key technical achievements
- Show test results and gas benchmarks

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All contract code for optimizations
- Review: All backend code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Solidity best practices
- Verify: Code follows TypeScript best practices

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Smart contract security (common vulnerabilities)
- Check: Reentrancy vulnerabilities
- Check: Access control issues
- Check: Integer overflow/underflow protection
- Check: Private key management (backend)
- Check: Input validation and sanitization
- Review: OpenZeppelin contracts usage (secure)
- Document: Security findings and mitigations
- Run: Security linters/tools (Slither, Mythril if available)

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npx hardhat test`
- Verify 100% of required scenarios pass
- Run gas benchmarks: `npx hardhat test --gas-reporter`
- Document final test results
- Create test summary report

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Gated token contract deployed and functional
- ✅ Allowlist mechanism working (approve/revoke)
- ✅ Transfer restrictions enforced (both sender and recipient)
- ✅ Issuer service can approve wallets and mint tokens
- ✅ Event indexer listening and storing events
- ✅ Cap-table export working (CSV/JSON)
- ✅ Basic CLI interface functional
- ✅ Unit tests for core contract functions passing

### Early Submission (Phase 2) - November 7
- ✅ Stock split (7-for-1) implemented and tested
- ✅ Symbol change implemented and tested
- ✅ Operator demo interface (CLI or UI) functional
- ✅ Integration tests for full flow passing
- ✅ Demo script/UI demonstrates all operations

### Final Submission (Phase 3) - November 9
- ✅ All required test scenarios passing (10+ tests)
- ✅ Gas benchmarks documented (all operations)
- ✅ Technical writeup complete (1-2 pages)
- ✅ Decision log documenting key choices
- ✅ README with complete setup instructions
- ✅ No false compliance claims (disclaimer included)
- ✅ Reproducible setup (anyone can run demo)
- ✅ All code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Gas costs exceed targets | Medium | Document actual costs, propose optimizations, accept if reasonable for prototype |
| Testnet RPC rate limits | High | Use local Hardhat network for development, Sepolia only for final deployment |
| Event indexing gaps | Medium | Query from genesis block, handle reorgs with block confirmation |
| Contract deployment failures | High | Test deployment locally first, keep backup deployment script |
| Time constraints | High | MVP is safety net, corporate actions are enhancement, prioritize core features first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All contracts in `contracts/` directory
- [ ] All backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] Deployment scripts in `scripts/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation
- [ ] Gas report with benchmarks
- [ ] Test results summary

### Testing
- [ ] Unit tests for all contract functions
- [ ] Integration tests for full flows
- [ ] Gas benchmarks for all operations
- [ ] Test coverage for required scenarios

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployment addresses documented (if testnet)

---

## Notes

- **No Compliance Claims**: This is a technical prototype. Include disclaimer that it is NOT regulatory-compliant.
- **Testnet Only**: Never use real funds. Sepolia testnet or local Hardhat network only.
- **Secrets Management**: Use .env for private keys, never commit secrets.
- **Gas Optimization**: Targets are guidelines. Document actual costs and justify if above targets.

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

Updating the remaining PRDs and creating risk assessment documents:


Updating the remaining PRDs in batches:


```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Final Polish (60 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.6.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: AI Math Tutor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Socratic Learning Assistant with OCR/Vision LLM Parsing

---

## Executive Summary

AI Math Tutor is an AI-powered educational assistant that guides students through math problems using Socratic questioning. The system accepts problems via screenshot or text input, parses them using OCR/Vision LLM, and maintains multi-turn conversations that guide students to discover solutions without providing direct answers.

**Key Deliverables:**
- ✅ Problem Input: Text entry + image upload with OCR/Vision LLM parsing
- ✅ Socratic Dialogue: Multi-turn conversation with guiding questions
- ✅ Math Rendering: Display equations properly (LaTeX/KaTeX)
- ✅ Web Interface: Clean chat UI with image upload and conversation history
- ✅ Context Management: Maintains conversation context across turns
- ✅ Adaptive Scaffolding: Adjusts to student understanding level

**Success Targets:**
- **MVP (Nov 4)**: Basic chat + LLM integration + Socratic prompting + image parsing
- **Early Submission (Nov 7)**: + Math rendering + conversation history + UI polish
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + optional stretch features

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Basic chat UI, LLM integration, Socratic prompting, image parsing working |
| **Early Submission** | Friday, November 7, 2025 | + Math rendering (LaTeX), conversation history, UI polish |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive testing, documentation, optional stretch features |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Math Rendering**: KaTeX or MathJax
- **Image Upload**: React Dropzone or similar
- **Chat UI**: Custom chat component or react-chat-elements
- **Build Tool**: Vite or Next.js (if SSR needed)

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API routes
- **Language**: TypeScript
- **AI Integration**: OpenAI GPT-4 with Vision API or Anthropic Claude with vision
- **Image Processing**: Sharp (for image optimization)
- **Storage**: SQLite (local) or PostgreSQL (if deployed)
- **Session Management**: In-memory or Redis (if deployed)

### AI Integration
- **LLM**: OpenAI GPT-4 Turbo with Vision or Anthropic Claude 3.5 Sonnet with vision
- **SDK**: OpenAI Node.js SDK or Anthropic SDK
- **Image Processing**: OpenAI Vision API for OCR or Tesseract.js (local OCR)
- **Prompt Engineering**: Structured prompts for Socratic method

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
AI-Math-Tutor/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── Chat.tsx              # Main chat interface
│   │   │   ├── MessageBubble.tsx     # Message display
│   │   │   ├── ImageUpload.tsx       # Image upload component
│   │   │   ├── MathRenderer.tsx      # LaTeX math rendering
│   │   │   └── ConversationHistory.tsx # History sidebar
│   │   ├── hooks/
│   │   │   ├── useChat.ts            # Chat state management
│   │   │   └── useImageUpload.ts     # Image upload logic
│   │   ├── services/
│   │   │   ├── api.ts                # API client
│   │   │   └── mathParser.ts         # Math parsing utilities
│   │   └── App.tsx
│   └── public/
├── backend/
│   ├── api/
│   │   ├── chat.ts                   # Chat endpoint
│   │   ├── image-upload.ts            # Image upload endpoint
│   │   └── health.ts                  # Health check
│   ├── services/
│   │   ├── llm.ts                     # LLM service
│   │   ├── imageProcessor.ts          # Image processing
│   │   ├── socratic.ts                # Socratic method logic
│   │   └── contextManager.ts         # Conversation context
│   ├── prompts/
│   │   └── socratic.ts                # Socratic prompt templates
│   └── server.ts                      # Express server
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── prompt-engineering-notes.md
```

### Data Flow

1. **Problem Input**: User uploads image or types text → Image processed via Vision API → Problem text extracted
2. **Socratic Dialogue**: Problem text → LLM with Socratic prompt → Guiding question → User response → Context updated
3. **Math Rendering**: Mathematical expressions → LaTeX parser → KaTeX renderer → Displayed in UI
4. **Context Management**: Each turn → Context stored → Previous turns retrieved → Full conversation context maintained

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic chat with LLM integration and Socratic prompting working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 20 minutes
- Create `ai-math-tutor/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install dependencies: React, TypeScript, Tailwind CSS
- Verify setup: `npm run dev`
- PowerShell: `New-Item -ItemType Directory -Path ai-math-tutor; Set-Location ai-math-tutor; npm create vite@latest frontend -- --template react-ts`

**Task 1.1.2**: Setup backend with Express
- Duration: 20 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, tsx, OpenAI SDK
- Create `server.ts` with basic Express setup
- Add TypeScript config: `tsconfig.json`
- PowerShell: `New-Item -ItemType Directory -Path backend; Set-Location backend; npm init -y; npm install express typescript tsx @types/express @types/node`

**Task 1.1.3**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files (frontend and backend)
- Add: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries
- Document setup in README

**Task 1.1.4**: Setup testing framework
- Duration: 20 minutes
- Install Vitest: `npm install -D vitest @testing-library/react`
- Create `tests/` directory structure
- Create sample test to verify setup
- Run: `npm test` to verify working

**Task 1.1.5**: Create project structure
- Duration: 15 minutes
- Create directory structure (components, services, hooks)
- Create placeholder files for main components
- Add basic README with setup instructions

#### Phase 1.2: Basic Chat UI (120 minutes)

**Task 1.2.1**: Create chat interface component
- Duration: 45 minutes
- Create `frontend/src/components/Chat.tsx`
- Basic chat UI with:
  - Message input area
  - Send button
  - Message list area
  - Simple styling with Tailwind
- Add state management for messages

**Task 1.2.2**: Create message bubble component
- Duration: 30 minutes
- Create `frontend/src/components/MessageBubble.tsx`
- Display: User messages (right-aligned), Tutor messages (left-aligned)
- Add basic styling and animations
- Show timestamps

**Task 1.2.3**: Create chat hook for state management
- Duration: 30 minutes
- Create `frontend/src/hooks/useChat.ts`
- Manage: Messages array, current input, loading state
- Functions: `sendMessage()`, `addMessage()`, `clearChat()`
- Return: Messages, loading, sendMessage function

**Task 1.2.4**: Create API client
- Duration: 15 minutes
- Create `frontend/src/services/api.ts`
- Function: `sendMessage(message: string) => Promise<string>`
- POST to `/api/chat` endpoint
- Handle errors and loading states

#### Phase 1.3: Backend API - Chat Endpoint (120 minutes)

**Task 1.3.1**: Create Express server with chat endpoint
- Duration: 30 minutes
- Create `backend/server.ts`
- Setup Express with CORS middleware
- Create `/api/chat` POST endpoint
- Receive: `{ message: string, conversationHistory?: Message[] }`
- Return: `{ response: string }`

**Task 1.3.2**: Integrate OpenAI SDK
- Duration: 30 minutes
- Create `backend/services/llm.ts`
- Setup OpenAI client with API key
- Function: `generateResponse(messages: Message[]) => Promise<string>`
- Call `openai.chat.completions.create()` with messages
- Return assistant message

**Task 1.3.3**: Create Socratic prompt template
- Duration: 30 minutes
- Create `backend/prompts/socratic.ts`
- System prompt: "You are a patient math tutor. NEVER give direct answers. Guide through questions: 'What information do we have?' 'What method might help?' If stuck >2 turns, provide concrete hint. Use encouraging language."
- Format: System message + conversation history
- Add prompt validation

**Task 1.3.4**: Implement Socratic logic
- Duration: 30 minutes
- Create `backend/services/socratic.ts`
- Function: `buildPrompt(conversationHistory: Message[]) => Message[]`
- Logic:
  - If first message: Parse problem → Inventory knowns → Identify goal
  - If subsequent: Guide method selection → Step through solution → Validate answer
- Add validation for Socratic method (no direct answers)

#### Phase 1.4: Image Upload & OCR/Vision LLM (150 minutes)

**Task 1.4.1**: Create image upload component
- Duration: 45 minutes
- Create `frontend/src/components/ImageUpload.tsx`
- Use React Dropzone or similar
- Features: Drag & drop, click to upload, preview image
- Accept: PNG, JPG, JPEG formats
- Display upload progress

**Task 1.4.2**: Create image upload API endpoint
- Duration: 30 minutes
- Create `/api/image-upload` POST endpoint
- Use Multer or similar for file handling
- Save uploaded image temporarily
- Return: Image URL or base64 data

**Task 1.4.3**: Integrate Vision API for OCR
- Duration: 45 minutes
- Create `backend/services/imageProcessor.ts`
- Function: `extractTextFromImage(imagePath: string) => Promise<string>`
- Option A: OpenAI Vision API:
  - Use `openai.chat.completions.create()` with vision model
  - Prompt: "Extract all mathematical expressions and text from this image. Return the problem statement."
  - Parse response for problem text
- Option B: Tesseract.js (local OCR):
  - Install Tesseract.js
  - Process image locally
  - Extract text (less accurate for math)

**Task 1.4.4**: Integrate image upload into chat flow
- Duration: 30 minutes
- Modify chat UI to include image upload
- When image uploaded: Extract text → Send to LLM as first message
- Handle: Image processing → Text extraction → Problem parsing → Chat start
- Add loading state for image processing

#### Phase 1.5: Conversation Context Management (90 minutes)

**Task 1.5.1**: Implement context manager
- Duration: 45 minutes
- Create `backend/services/contextManager.ts`
- Function: `buildContext(userId: string, conversationId: string) => Message[]`
- Store: Messages in-memory (Map<conversationId, Message[]>)
- Or: Use SQLite for persistence
- Retrieve: Full conversation history for context

**Task 1.5.2**: Add context to chat endpoint
- Duration: 30 minutes
- Modify `/api/chat` endpoint to accept `conversationId`
- Retrieve conversation history from context manager
- Pass full history to LLM (last 10-20 messages for token efficiency)
- Update context after each message

**Task 1.5.3**: Test context management
- Duration: 15 minutes
- Test: Multi-turn conversation maintains context
- Test: LLM references previous messages
- Test: Context persists across API calls
- Verify: Conversation history is accurate

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Write unit tests for chat logic
- Duration: 45 minutes
- Test: Message sending and receiving
- Test: Context management (add/retrieve messages)
- Test: Socratic prompt building
- Test: Image text extraction (mock)
- Run: `npm test` to verify

**Task 1.6.2**: Test with hardcoded problems
- Duration: 30 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problem: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
- Verify: Socratic method works (no direct answers)
- Verify: Context maintained across turns

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All unit tests pass
- Verify: Chat flow works end-to-end
- Document any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor MVP - Socratic learning assistant"
  - Problem Input (30s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (60s): Show multi-turn conversation, demonstrate guiding questions (no direct answers)
  - Context Management (30s): Show how tutor references previous messages
  - Conclusion (10s): "MVP demonstrates core Socratic method with image parsing and context management"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for chat interface
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add math rendering, conversation history, UI polish

#### Phase 2.1: Math Rendering with LaTeX/KaTeX (120 minutes)

**Task 2.1.1**: Install and configure KaTeX
- Duration: 20 minutes
- Install: `npm install katex react-katex`
- Add KaTeX CSS to main CSS file
- Create `frontend/src/components/MathRenderer.tsx`
- Function: `renderMath(text: string) => JSX.Element`
- Support: Inline math `$...$` and display math `$$...$$`

**Task 2.1.2**: Parse mathematical expressions from messages
- Duration: 45 minutes
- Create `frontend/src/services/mathParser.ts`
- Function: `parseMath(text: string) => string`
- Extract: LaTeX expressions from text
- Handle: Inline math and display math
- Preserve: Non-math text

**Task 2.1.3**: Integrate math rendering into message bubbles
- Duration: 35 minutes
- Modify `MessageBubble.tsx` to render math
- Parse message text for math expressions
- Render math with KaTeX component
- Handle: Error cases (invalid LaTeX)
- Style: Math expressions appropriately

**Task 2.1.4**: Test math rendering
- Duration: 20 minutes
- Test: Simple equations (`x + 5 = 13`)
- Test: Complex equations (`x² + 5x + 6 = 0`)
- Test: Fractions, exponents, integrals
- Test: Mixed text and math
- Verify: All expressions render correctly

#### Phase 2.2: Conversation History (90 minutes)

**Task 2.2.1**: Create conversation history UI
- Duration: 45 minutes
- Create `frontend/src/components/ConversationHistory.tsx`
- Display: List of past conversations
- Features:
  - Conversation titles (first problem statement)
  - Timestamp
  - Click to load conversation
  - Delete conversation
- Sidebar or dropdown menu

**Task 2.2.2**: Persist conversations in backend
- Duration: 30 minutes
- Create SQLite database for conversations
- Schema: `conversations(id, userId, title, createdAt)`, `messages(conversationId, role, content, timestamp)`
- Store: Full conversation history
- Retrieve: Conversations by user

**Task 2.2.3**: Load conversation from history
- Duration: 15 minutes
- Add API endpoint: `GET /api/conversations/:id`
- Load conversation messages into chat
- Update context manager with loaded conversation
- Test: Loading conversation works correctly

#### Phase 2.3: UI Polish & Enhancements (120 minutes)

**Task 2.3.1**: Improve chat UI design
- Duration: 45 minutes
- Enhance: Message bubble styling
- Add: Typing indicator
- Add: Loading states for LLM responses
- Add: Smooth scroll to latest message
- Add: Dark mode (optional)

**Task 2.3.2**: Add error handling and user feedback
- Duration: 30 minutes
- Display: Error messages for failed API calls
- Add: Retry button for failed requests
- Show: Network errors clearly
- Add: Input validation (empty messages, file size limits)

**Task 2.3.3**: Optimize image upload UX
- Duration: 30 minutes
- Add: Image preview before upload
- Add: Image compression (if needed)
- Add: Progress bar for upload
- Add: Error handling for unsupported formats
- Add: Maximum file size validation

**Task 2.3.4**: Add keyboard shortcuts
- Duration: 15 minutes
- Add: Enter to send message
- Add: Shift+Enter for new line
- Add: Escape to clear input
- Test: Keyboard shortcuts work correctly

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Test math rendering with various problems
- Duration: 30 minutes
- Test: 5+ problem types (arithmetic, algebra, geometry, word problems, multi-step)
- Verify: All math expressions render correctly
- Verify: LaTeX syntax is handled properly
- Document: Any rendering issues

**Task 2.4.2**: Test conversation history
- Duration: 30 minutes
- Test: Saving conversations
- Test: Loading conversations
- Test: Deleting conversations
- Test: Multiple conversations per user
- Verify: Context maintained when loading

**Task 2.4.3**: Integration tests for full flow
- Duration: 30 minutes
- Test: Upload image → Extract text → Start chat → Socratic dialogue → Math rendering
- Test: Load conversation → Continue chat → Context maintained
- Test: Error scenarios (API failures, invalid images)
- Verify: All features work together

**Task 2.4.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor Early Submission - now with math rendering and history"
  - Problem Input (25s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (50s): Show multi-turn conversation, demonstrate guiding questions, show math rendering
  - Conversation History (30s): Show past conversations, load conversation, continue dialogue
  - UI Polish (25s): Show improved UI, error handling, keyboard shortcuts
  - Conclusion (10s): "Demonstrates complete Socratic learning experience with math rendering"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for improved interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optional stretch features

#### Phase 3.1: Comprehensive Testing (120 minutes)

**Task 3.1.1**: Test all 5+ problem types
- Duration: 60 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problems: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
  6. Fractions: "1/2 + 1/3 = ?"
  7. Exponents: "2³ × 2² = ?"
- Verify: Socratic method works for all types
- Verify: No direct answers given
- Verify: Context maintained across turns

**Task 3.1.2**: Test edge cases and error scenarios
- Duration: 45 minutes
- Test: Invalid image formats
- Test: Very large images
- Test: Blurry images (OCR failure)
- Test: No text in image
- Test: Network errors
- Test: LLM API errors
- Test: Empty messages
- Test: Very long conversations (>20 turns)

**Task 3.1.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.2: Optional Stretch Features (180 minutes)

**Task 3.2.1**: Interactive Whiteboard (Optional - High Value)
- Duration: 90 minutes
- Install: react-sketch-canvas or similar
- Create: `Whiteboard.tsx` component
- Features:
  - Draw diagrams and equations
  - Share with tutor
  - Tutor can draw explanations
- Integrate: Into chat flow
- Test: Whiteboard functionality

**Task 3.2.2**: Step Visualization (Optional - High Value)
- Duration: 60 minutes
- Create: `StepVisualization.tsx` component
- Display: Solution steps animated or sequentially
- Parse: LLM response for steps
- Render: Each step with math rendering
- Test: Step visualization works

**Task 3.2.3**: Voice Interface (Optional - High Value)
- Duration: 30 minutes
- Add: Text-to-speech for tutor responses
- Add: Speech-to-text for student input
- Use: Web Speech API
- Test: Voice interface works

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - LLM Integration (OpenAI/Anthropic)
  - Socratic Method Implementation
  - Image Processing & OCR
  - Math Rendering (KaTeX)
  - Context Management
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Write prompt engineering notes
- Duration: 30 minutes
- Document: Socratic prompt design
- Document: Prompt iterations and improvements
- Document: Techniques for preventing direct answers
- Document: Context management strategies
- Include: Example prompts and responses

**Task 3.3.3**: Update README with complete setup
- Duration: 30 minutes
- Prerequisites (Node.js, npm, API keys)
- Installation steps
- Environment setup
- Running locally
- Testing instructions
- Deployment instructions (if applicable)
- Troubleshooting section

**Task 3.3.4**: Document 5+ example problem walkthroughs
- Duration: 15 minutes
- Document: 5+ complete problem walkthroughs
- Include: Problem statement, conversation flow, final answer
- Show: Socratic method in action
- Show: Context maintained across turns

**Task 3.3.5**: Document AI tools and prompts used
- Duration: 15 minutes
- List: AI tools used (Cursor, Copilot, etc.)
- Document: Key prompts that generated code
- Acknowledge: AI assistance in README

#### Phase 3.4: Final Polish (90 minutes)

**Task 3.4.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove: Debug code and console.logs
- Add: JSDoc comments to all functions

**Task 3.4.2**: Performance optimization
- Duration: 30 minutes
- Optimize: Image upload (compression if needed)
- Optimize: Math rendering (cache if possible)
- Optimize: Context management (limit history length)
- Profile: API response times
- Optimize: Re-renders in React

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All 5+ problem types tested
- ✅ Socratic method works (no direct answers)
- ✅ Context maintained across turns
- ✅ Math rendering works correctly
- ✅ Image upload and OCR working
- ✅ Conversation history working
- ✅ README complete and accurate
- ✅ Documentation complete
- ✅ Tests passing
- ✅ No API keys in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor - Complete Socratic learning assistant"
  - System Overview (15s): Architecture, key features
  - Problem Input (25s): Text input and image upload, OCR demonstration
  - Socratic Dialogue (60s): Multi-turn conversation, guiding questions, math rendering, no direct answers
  - Conversation History (20s): Past conversations, loading and continuing
  - Stretch Features (20s): Whiteboard/step visualization/voice (if built)
  - Conclusion (10s): "Complete Socratic learning experience with comprehensive features"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all features
- Highlight key technical achievements
- Show Socratic method in action

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Check: Test coverage (aim for >80%)
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Problem input working (text + image upload)
- ✅ OCR/Vision LLM parsing working
- ✅ Basic chat UI functional
- ✅ LLM integration with Socratic prompting
- ✅ Socratic method working (no direct answers)
- ✅ Context management working
- ✅ Multi-turn conversations functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Math rendering with LaTeX/KaTeX working
- ✅ Conversation history implemented
- ✅ UI polish and enhancements complete
- ✅ Integration tests passing
- ✅ Tested on 5+ problem types

### Final Submission (Phase 3) - November 9
- ✅ Comprehensive testing (5+ problem types)
- ✅ Edge cases handled gracefully
- ✅ Documentation complete (technical writeup, prompt notes, README, walkthroughs)
- ✅ Optional stretch features implemented (if time permits)
- ✅ Code clean, linted, and documented
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM gives direct answers | High | Strict Socratic prompt, validation in code, test thoroughly |
| OCR fails on images | Medium | Fallback to text input, clear error messages, test with various images |
| Context too long for LLM | Medium | Limit conversation history (last 10-20 messages), summarize if needed |
| API costs too high | Medium | Cache responses if possible, use cheaper models for iteration |
| Math rendering errors | Low | Validate LaTeX syntax, handle errors gracefully |
| Time constraints | High | MVP is safety net, stretch features are optional, prioritize core Socratic method |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Prompt engineering notes
- [ ] README with complete setup and usage
- [ ] 5+ example problem walkthroughs
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for chat logic
- [ ] Integration tests for full flow
- [ ] Tests for 5+ problem types
- [ ] Edge case tests

### Demo
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployed app or local setup instructions

---

## Notes

- **Socratic Method**: NEVER give direct answers. Guide through questions only.
- **Context Management**: Maintain full conversation history for proper context.
- **Math Rendering**: Use KaTeX for accurate math display.
- **Image Processing**: Use Vision API for best OCR results (or Tesseract.js for local).
- **Testing**: Test with diverse problem types to ensure Socratic method works.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Final Polish (90 minutes)

**Task 3.5.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.5.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.5.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.6: Final Submission Demo Video Preparation (45 minutes)

**Task 3.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

Adding the code evaluation/security assessment task to each PRD before the final testing phase:


Adding the code evaluation/security assessment task to each PRD:


```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.6.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.7.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.7.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.7.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.9: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.7.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.9: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

Fixing the task numbering and continuing with the other PRDs:


```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.7.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.9: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: SpendSense

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Explainable, Consent-Aware Financial Education System

---

## Executive Summary

SpendSense is an explainable, consent-aware system that detects behavioral patterns from Plaid-style transaction data, assigns personas, and delivers personalized financial education with clear guardrails around eligibility and tone. The system transforms transaction data into actionable insights without crossing into regulated financial advice.

**Key Deliverables:**
- ✅ Synthetic Plaid-style data generator (50-100 users)
- ✅ Feature pipeline detecting subscriptions, savings, credit, income patterns
- ✅ Persona assignment system (5 personas)
- ✅ Recommendation engine with plain-language rationales
- ✅ Consent and eligibility guardrails
- ✅ Operator view for oversight
- ✅ Evaluation harness with metrics

**Success Targets:**
- **MVP (Nov 4)**: Data ingestion + behavioral signal detection + basic persona assignment
- **Early Submission (Nov 7)**: + Recommendation engine + consent guardrails + operator view
- **Final Submission (Nov 9)**: + Evaluation harness + comprehensive testing + documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Synthetic data generator, signal detection, basic persona assignment working |
| **Early Submission** | Friday, November 7, 2025 | + Recommendation engine, consent guardrails, operator view |
| **Final Submission** | Sunday, November 9, 2025 | + Evaluation harness, comprehensive testing, documentation |

---

## Technology Stack

### Backend Stack
- **Language**: Python 3.11+ (or Node.js + TypeScript)
- **Data Processing**: Pandas (Python) or similar libraries
- **Storage**: SQLite for relational data, Parquet for analytics
- **API Framework**: FastAPI (Python) or Express.js (Node.js)
- **Testing**: pytest (Python) or Jest (Node.js)

### Frontend Stack (Optional Operator View)
- **Framework**: React + TypeScript (if web UI needed)
- **UI Library**: Tailwind CSS or Material-UI
- **Data Visualization**: Recharts or Chart.js

### AI Integration (Optional)
- **LLM**: OpenAI GPT-4 or Anthropic Claude (for educational content generation)
- **Framework**: OpenAI SDK or Anthropic SDK

### Development Tools
- **Package Manager**: pip (Python) or npm (Node.js)
- **Linting**: pylint/flake8 (Python) or ESLint (Node.js)
- **Type Checking**: mypy (Python) or TypeScript
- **Documentation**: Sphinx (Python) or JSDoc (Node.js)

---

## Architecture Overview

### System Components

```
SpendSense/
├── ingest/
│   ├── data_generator.py          # Synthetic Plaid data generator
│   ├── data_loader.py              # CSV/JSON ingestion
│   └── schema_validator.py         # Data validation
├── features/
│   ├── subscriptions.py            # Subscription signal detection
│   ├── savings.py                  # Savings pattern detection
│   ├── credit.py                   # Credit utilization detection
│   ├── income.py                   # Income stability detection
│   └── aggregator.py               # Feature aggregation
├── personas/
│   ├── assignment.py              # Persona assignment logic
│   ├── prioritization.py          # Multi-persona prioritization
│   └── personas.py                 # Persona definitions
├── recommend/
│   ├── engine.py                   # Recommendation engine
│   ├── content_catalog.py          # Education content catalog
│   ├── partner_offers.py           # Partner offer catalog
│   └── rationales.py               # Rationale generation
├── guardrails/
│   ├── consent.py                  # Consent management
│   ├── eligibility.py              # Eligibility checking
│   └── tone_checker.py             # Tone validation
├── ui/
│   ├── operator_view.py            # Operator interface
│   ├── api.py                       # REST API endpoints
│   └── frontend/                    # React UI (optional)
├── eval/
│   ├── metrics.py                   # Evaluation metrics
│   ├── harness.py                   # Evaluation harness
│   └── reports.py                  # Report generation
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Data Flow

1. **Ingestion**: Synthetic data → Validation → Storage (SQLite/Parquet)
2. **Feature Extraction**: Transaction data → Signal detection → Feature vectors
3. **Persona Assignment**: Features → Persona matching → Priority selection
4. **Recommendation**: Persona + Features → Content matching → Rationale generation
5. **Guardrails**: Recommendations → Consent check → Eligibility check → Tone check
6. **Operator Review**: Recommendations → Operator approval → Final output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Data ingestion and signal detection working end-to-end

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize project structure
- Duration: 15 minutes
- Create `spendsense/` directory
- Setup Python virtual environment: `python -m venv venv`
- Activate venv: PowerShell: `.\venv\Scripts\Activate.ps1`
- Create directory structure (ingest/, features/, personas/, etc.)
- Initialize git repository: `git init`

**Task 1.1.2**: Install dependencies
- Duration: 15 minutes
- Create `requirements.txt` with dependencies:
  - pandas, numpy, sqlalchemy, pytest
- Install: `pip install -r requirements.txt`
- Create `.gitignore` with venv/, `__pycache__/`, `.env`, `*.db`

**Task 1.1.3**: Setup SQLite database
- Duration: 20 minutes
- Create `database.py` with SQLite connection
- Define schema: users, accounts, transactions, liabilities
- Create tables with proper indexes
- Add seed data helper functions

**Task 1.1.4**: Create configuration system
- Duration: 20 minutes
- Create `config.py` with settings:
  - Database path
  - Feature window sizes (30d, 180d)
  - Signal thresholds
- Create `.env.example` with configuration placeholders
- Document configuration in README

**Task 1.1.5**: Setup testing framework
- Duration: 20 minutes
- Create `tests/` directory structure
- Create `pytest.ini` configuration
- Write sample test to verify setup
- Run: `pytest` to verify working

#### Phase 1.2: Synthetic Data Generator (180 minutes)

**Task 1.2.1**: Define Plaid-style data schemas
- Duration: 30 minutes
- Create `ingest/schemas.py` with data models:
  - Account: account_id, type, subtype, balances, iso_currency_code
  - Transaction: account_id, date, amount, merchant_name, category, pending
  - Liability: account_id, aprs, minimum_payment, last_payment, is_overdue
- Use dataclasses or Pydantic models
- Add validation logic

**Task 1.2.2**: Implement account generator
- Duration: 45 minutes
- Create `ingest/data_generator.py`
- Function: `generate_accounts(user_id: int, count: int)` 
- Generate: checking, savings, credit cards, money market, HSA
- Randomize balances, limits, APRs
- Ensure diversity (various income levels, credit behaviors)

**Task 1.2.3**: Implement transaction generator
- Duration: 60 minutes
- Function: `generate_transactions(account_id: str, days: int)`
- Generate realistic transaction patterns:
  - Payroll deposits (bi-weekly/monthly)
  - Recurring subscriptions (Netflix, Spotify, etc.)
  - Variable expenses (groceries, gas, restaurants)
  - Random one-time purchases
- Include merchant names, categories, amounts
- Ensure variety in transaction patterns

**Task 1.2.4**: Implement liability generator
- Duration: 30 minutes
- Function: `generate_liabilities(user_id: int)`
- Generate credit cards with:
  - APRs (type, percentage)
  - Minimum payment amounts
  - Last payment amounts
  - Overdue status (occasional)
- Generate mortgages/student loans (optional)

**Task 1.2.5**: Generate 50-100 synthetic users
- Duration: 15 minutes
- Function: `generate_users(count: int = 75)`
- Generate diverse financial situations:
  - High utilization users (credit cards)
  - Variable income users (irregular pay)
  - Subscription-heavy users (many recurring)
  - Savings builders (growing savings)
  - Mixed scenarios
- Use fake names, masked account numbers (no PII)
- Save to SQLite database

**Task 1.2.6**: Create data validation
- Duration: 30 minutes
- Create `ingest/schema_validator.py`
- Validate: account structure, transaction structure, liability structure
- Check: date ranges, amount ranges, required fields
- Generate validation report
- Handle invalid data gracefully

#### Phase 1.3: Behavioral Signal Detection - Subscriptions (120 minutes)

**Task 1.3.1**: Implement recurring merchant detection
- Duration: 45 minutes
- Create `features/subscriptions.py`
- Function: `detect_recurring_merchants(transactions: DataFrame, days: int = 90)`
- Logic: Find merchants with ≥3 transactions in 90 days
- Calculate cadence (monthly/weekly)
- Identify subscription merchants (Netflix, Spotify, etc.)
- Return: list of recurring merchants with cadence

**Task 1.3.2**: Calculate subscription metrics
- Duration: 45 minutes
- Function: `calculate_subscription_metrics(transactions: DataFrame, window: int)`
- Calculate: Monthly recurring spend (sum of recurring transactions)
- Calculate: Subscription share of total spend (recurring / total)
- Support: 30-day and 180-day windows
- Return: metrics dictionary

**Task 1.3.3**: Test subscription detection
- Duration: 30 minutes
- Create test data with known subscriptions
- Test: Detection of monthly subscriptions
- Test: Detection of weekly subscriptions
- Test: False positives (non-recurring merchants)
- Verify metrics calculation accuracy

#### Phase 1.4: Behavioral Signal Detection - Savings (90 minutes)

**Task 1.4.1**: Detect savings account activity
- Duration: 30 minutes
- Create `features/savings.py`
- Function: `detect_savings_activity(transactions: DataFrame, accounts: DataFrame)`
- Filter: savings-like accounts (savings, money market, cash management, HSA)
- Calculate: Net inflow (inflows - outflows)
- Calculate: Growth rate over window
- Return: net inflow, growth rate

**Task 1.4.2**: Calculate emergency fund coverage
- Duration: 30 minutes
- Function: `calculate_emergency_fund_coverage(accounts: DataFrame, transactions: DataFrame)`
- Logic: savings_balance / average_monthly_expenses
- Calculate average monthly expenses from transaction history
- Return: coverage ratio (months of expenses covered)
- Handle edge cases (zero expenses, negative savings)

**Task 1.4.3**: Test savings detection
- Duration: 30 minutes
- Create test data with known savings patterns
- Test: Growing savings detection
- Test: Declining savings detection
- Test: Emergency fund calculation accuracy
- Verify metrics for 30d and 180d windows

#### Phase 1.5: Behavioral Signal Detection - Credit (120 minutes)

**Task 1.5.1**: Calculate credit utilization
- Duration: 30 minutes
- Create `features/credit.py`
- Function: `calculate_utilization(liabilities: DataFrame)`
- Logic: utilization = balance / limit (per card)
- Calculate overall utilization (total balance / total limit)
- Return: per-card and overall utilization
- Handle missing limits gracefully

**Task 1.5.2**: Detect credit flags
- Duration: 45 minutes
- Function: `detect_credit_flags(liabilities: DataFrame)`
- Flags:
  - Utilization ≥30%
  - Utilization ≥50%
  - Utilization ≥80%
  - Minimum-payment-only (min_payment = statement_balance)
  - Interest charges present
  - Overdue status
- Return: list of flags per card and overall

**Task 1.5.3**: Test credit detection
- Duration: 45 minutes
- Create test data with various utilization levels
- Test: Low utilization (<30%)
- Test: Medium utilization (30-50%)
- Test: High utilization (>80%)
- Test: Minimum-payment-only detection
- Test: Overdue detection
- Verify all flags triggered correctly

#### Phase 1.6: Behavioral Signal Detection - Income Stability (90 minutes)

**Task 1.6.1**: Detect payroll ACH deposits
- Duration: 30 minutes
- Create `features/income.py`
- Function: `detect_payroll_deposits(transactions: DataFrame)`
- Logic: Find ACH deposits with consistent patterns
- Identify: Payment frequency (weekly/bi-weekly/monthly)
- Calculate: Payment amount and variability
- Return: payroll transactions with frequency

**Task 1.6.2**: Calculate income stability metrics
- Duration: 30 minutes
- Function: `calculate_income_stability(transactions: DataFrame, window: int)`
- Calculate: Median pay gap (days between payments)
- Calculate: Payment variability (standard deviation)
- Calculate: Cash-flow buffer (months of expenses vs. income)
- Return: stability metrics

**Task 1.6.3**: Test income detection
- Duration: 30 minutes
- Create test data with regular and irregular income
- Test: Regular bi-weekly income detection
- Test: Irregular income detection
- Test: Cash-flow buffer calculation
- Verify metrics for variable income users

#### Phase 1.7: Feature Aggregation (60 minutes)

**Task 1.7.1**: Create feature aggregation pipeline
- Duration: 30 minutes
- Create `features/aggregator.py`
- Function: `aggregate_features(user_id: int, window_30d: bool, window_180d: bool)`
- Aggregate: Subscriptions, Savings, Credit, Income features
- Return: Comprehensive feature dictionary for user
- Support: 30-day and 180-day windows separately

**Task 1.7.2**: Store aggregated features
- Duration: 20 minutes
- Create database table: `user_features`
- Store: user_id, window_size, feature_vector (JSON)
- Index on user_id and window_size
- Add function to retrieve features

**Task 1.7.3**: Test feature aggregation
- Duration: 10 minutes
- Test: Aggregation for multiple users
- Test: Both window sizes (30d, 180d)
- Verify: All signals included correctly

#### Phase 1.8: Basic Persona Assignment (90 minutes)

**Task 1.8.1**: Define persona criteria
- Duration: 30 minutes
- Create `personas/personas.py`
- Define 4 required personas:
  1. High Utilization (utilization ≥50% OR interest > 0 OR min-payment-only OR overdue)
  2. Variable Income Budgeter (median pay gap > 45d AND cash-flow < 1 month)
  3. Subscription-Heavy (recurring ≥3 AND (monthly spend ≥$50 OR share ≥10%))
  4. Savings Builder (growth rate ≥2% OR inflow ≥$200/month AND all utilizations < 30%)
- Document criteria clearly

**Task 1.8.2**: Implement persona assignment logic
- Duration: 45 minutes
- Create `personas/assignment.py`
- Function: `assign_persona(user_id: int, features: dict) -> str`
- Check each persona criteria in order
- Return: First matching persona (or "None" if no match)
- Log assignment rationale

**Task 1.8.3**: Test persona assignment
- Duration: 15 minutes
- Create test users matching each persona
- Test: High Utilization user assignment
- Test: Variable Income user assignment
- Test: Subscription-Heavy user assignment
- Test: Savings Builder user assignment
- Verify: Correct persona assigned

#### Phase 1.9: Basic CLI & Testing (90 minutes)

**Task 1.9.1**: Create simple CLI for data generation
- Duration: 30 minutes
- Create `cli.py` with commands:
  - `generate-data --count 75` (generate 75 users)
  - `detect-features --user-id <id>` (detect features for user)
  - `assign-persona --user-id <id>` (assign persona)
- Use argparse or click for CLI
- PowerShell: `python cli.py generate-data --count 75`

**Task 1.9.2**: Write basic unit tests
- Duration: 30 minutes
- Test: Subscription detection logic
- Test: Savings calculation logic
- Test: Credit utilization calculation
- Test: Income stability detection
- Test: Persona assignment logic
- Run: `pytest tests/unit/`

**Task 1.9.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Run full test suite: `pytest`
- Verify: Data generation works
- Verify: All signal detection works
- Verify: Persona assignment works
- Document any failing tests

#### Phase 1.10: MVP Demo Video Preparation (45 minutes)

**Task 1.10.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense MVP - financial behavior detection system"
  - Data Generation (30s): Generate 75 synthetic users, show data structure
  - Signal Detection (50s): Show subscription detection, savings patterns, credit utilization, income stability
  - Persona Assignment (40s): Assign personas to users, show criteria matching
  - Feature Aggregation (20s): Show aggregated features for a user
  - Conclusion (10s): "MVP demonstrates data ingestion, signal detection, and persona assignment"

**Task 1.10.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key outputs/CLI commands
- Visual cues for data visualization (if applicable)
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add recommendation engine, consent guardrails, and operator view

#### Phase 2.1: Recommendation Engine - Content Catalog (120 minutes)

**Task 2.1.1**: Create education content catalog
- Duration: 45 minutes
- Create `recommend/content_catalog.py`
- Define content items:
  - Articles on debt paydown strategies
  - Budget templates for variable income
  - Subscription audit checklists
  - Emergency fund calculators
  - Credit utilization explainers
- Each item: id, title, description, persona_tags, category

**Task 2.1.2**: Create partner offer catalog
- Duration: 45 minutes
- Create `recommend/partner_offers.py`
- Define offers:
  - Balance transfer credit cards (for high utilization)
  - High-yield savings accounts (for emergency fund builders)
  - Budgeting apps (for variable income)
  - Subscription management tools (for subscription-heavy)
- Each offer: id, title, description, eligibility_criteria, persona_tags

**Task 2.1.3**: Implement content matching
- Duration: 30 minutes
- Function: `match_content(persona: str, features: dict) -> List[ContentItem]`
- Match: Content items tagged with persona
- Rank: By relevance to user's specific signals
- Return: 3-5 education items, 1-3 partner offers

#### Phase 2.2: Recommendation Engine - Rationale Generation (150 minutes)

**Task 2.2.1**: Implement rationale generation
- Duration: 60 minutes
- Create `recommend/rationales.py`
- Function: `generate_rationale(recommendation: dict, user_features: dict) -> str`
- Format: "We noticed [specific data point]. [Recommendation] because [reason]."
- Example: "We noticed your Visa ending in 4523 is at 68% utilization ($3,400 of $5,000 limit). Bringing this below 30% could improve your credit score and reduce interest charges of $87/month."
- Use plain language, cite concrete data

**Task 2.2.2**: Generate recommendation output
- Duration: 45 minutes
- Function: `generate_recommendations(user_id: int, window: str) -> dict`
- Output per user per window:
  - 3-5 education items with rationales
  - 1-3 partner offers with eligibility checks
  - Every item includes "because" rationale
- Return: Structured recommendation dictionary

**Task 2.2.3**: Test recommendation generation
- Duration: 45 minutes
- Test: Recommendations for each persona
- Test: Rationales cite specific data points
- Test: Plain language (no jargon)
- Verify: All recommendations have rationales

#### Phase 2.3: Consent Management (90 minutes)

**Task 2.3.1**: Implement consent tracking
- Duration: 30 minutes
- Create `guardrails/consent.py`
- Database table: `user_consent(user_id, consent_given, consent_date, revoked_date)`
- Function: `check_consent(user_id: int) -> bool`
- Function: `record_consent(user_id: int, given: bool)`
- Function: `revoke_consent(user_id: int)`

**Task 2.3.2**: Enforce consent in recommendation flow
- Duration: 30 minutes
- Modify recommendation generation to check consent first
- If no consent: return empty recommendations
- Log consent status for audit
- Add consent check to all data processing functions

**Task 2.3.3**: Test consent enforcement
- Duration: 30 minutes
- Test: Recommendations blocked without consent
- Test: Consent can be revoked
- Test: Recommendations return after consent given
- Verify: No recommendations without consent

#### Phase 2.4: Eligibility Guardrails (90 minutes)

**Task 2.4.1**: Implement eligibility checking
- Duration: 45 minutes
- Create `guardrails/eligibility.py`
- Function: `check_eligibility(offer: dict, user_features: dict) -> bool`
- Check:
  - Minimum income requirements
  - Credit requirements
  - Existing account filter (don't offer savings if they have one)
  - Avoid harmful suggestions (no payday loans, predatory products)
- Return: Eligible (True/False) with reason

**Task 2.4.2**: Filter ineligible offers
- Duration: 30 minutes
- Modify recommendation generation to filter offers
- Only include eligible offers in output
- Log ineligible offers for operator review
- Return eligibility status with recommendations

**Task 2.4.3**: Test eligibility checking
- Duration: 15 minutes
- Test: High-yield savings offer (check existing accounts)
- Test: Balance transfer card (check credit requirements)
- Test: Harmful product filtering (payday loans blocked)
- Verify: Only eligible offers included

#### Phase 2.5: Tone Guardrails (60 minutes)

**Task 2.5.1**: Implement tone checker
- Duration: 30 minutes
- Create `guardrails/tone_checker.py`
- Function: `check_tone(text: str) -> dict`
- Check for:
  - Shaming language ("you're overspending")
  - Judgmental phrases
  - Negative tone indicators
- Return: Tone score and flags

**Task 2.5.2**: Validate recommendation tone
- Duration: 20 minutes
- Run tone check on all rationales
- Filter or rewrite shaming language
- Ensure empowering, educational tone
- Use neutral, supportive language

**Task 2.5.3**: Test tone checking
- Duration: 10 minutes
- Test: Shaming language detection
- Test: Judgmental phrase detection
- Test: Neutral tone preservation
- Verify: All recommendations pass tone check

#### Phase 2.6: Operator View (150 minutes)

**Task 2.6.1**: Create REST API endpoints
- Duration: 60 minutes
- Create `ui/api.py` with FastAPI
- Endpoints:
  - `GET /users/{user_id}/signals` (detected signals)
  - `GET /users/{user_id}/persona` (assigned persona)
  - `GET /users/{user_id}/recommendations` (recommendations with rationales)
  - `POST /operator/approve/{recommendation_id}` (approve recommendation)
  - `POST /operator/override/{user_id}` (override recommendations)
  - `GET /operator/review` (approval queue)
- Add request/response models

**Task 2.6.2**: Create operator interface (CLI or Web)
- Duration: 60 minutes
- Option A: CLI interface:
  - `python cli.py view-signals --user-id <id>`
  - `python cli.py view-persona --user-id <id>`
  - `python cli.py view-recommendations --user-id <id>`
  - `python cli.py approve-recommendation --id <id>`
- Option B: Simple web UI (React + Tailwind):
  - Dashboard showing users
  - User detail page with signals, persona, recommendations
  - Approve/override buttons
  - Decision trace view

**Task 2.6.3**: Implement decision trace
- Duration: 30 minutes
- Function: `get_decision_trace(user_id: int) -> dict`
- Include: Features detected, persona assignment rationale, recommendation matching logic
- Store trace in database for audit
- Display in operator view

#### Phase 2.7: Enhanced Testing (90 minutes)

**Task 2.7.1**: Write integration tests
- Duration: 45 minutes
- Test: Full flow from data → features → persona → recommendations
- Test: Consent enforcement in flow
- Test: Eligibility filtering in flow
- Test: Tone checking in flow
- Test: Operator approval flow

**Task 2.7.2**: Write operator view tests
- Duration: 30 minutes
- Test: API endpoints return correct data
- Test: Operator can approve recommendations
- Test: Operator can override recommendations
- Test: Decision trace is accurate

**Task 2.7.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `pytest`
- Verify: All Phase 1 + Phase 2 tests pass
- Document any edge cases

#### Phase 2.8: Early Submission Demo Video Preparation (45 minutes)

**Task 2.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense Early Submission - now with recommendations and guardrails"
  - Signal Detection (30s): Show subscription, savings, credit, income signals for a user
  - Persona Assignment (20s): Show persona assignment with criteria matching
  - Recommendation Generation (50s): Show recommendations with rationales, explain plain language
  - Consent & Eligibility (30s): Show consent enforcement, eligibility filtering
  - Operator View (30s): Show operator interface reviewing recommendations
  - Conclusion (10s): "Demonstrates explainable, consent-aware financial education system"

**Task 2.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/outputs
- Visual cues for operator interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Evaluation harness, comprehensive testing, documentation

#### Phase 3.1: Evaluation Harness (180 minutes)

**Task 3.1.1**: Implement coverage metric
- Duration: 30 minutes
- Create `eval/metrics.py`
- Function: `calculate_coverage(users: List[int]) -> dict`
- Metric: % of users with assigned persona AND ≥3 detected behaviors
- Target: 100% coverage
- Calculate and return coverage percentage

**Task 3.1.2**: Implement explainability metric
- Duration: 30 minutes
- Function: `calculate_explainability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with plain-language rationales
- Target: 100% explainability
- Check: Rationales exist, cite data, plain language

**Task 3.1.3**: Implement latency metric
- Duration: 30 minutes
- Function: `measure_latency(user_id: int) -> dict`
- Metric: Time to generate recommendations per user
- Target: <5 seconds per user
- Measure: Feature detection + persona assignment + recommendation generation
- Return: Latency in seconds

**Task 3.1.4**: Implement auditability metric
- Duration: 30 minutes
- Function: `calculate_auditability(recommendations: List[dict]) -> dict`
- Metric: % of recommendations with decision traces
- Target: 100% auditability
- Check: Decision traces exist for all recommendations

**Task 3.1.5**: Implement fairness metric (basic)
- Duration: 30 minutes
- Function: `calculate_fairness(users: List[int]) -> dict`
- Basic check: Demographic parity if synthetic data includes demographics
- Calculate: Persona distribution across groups
- Flag: Significant disparities (>20% difference)
- Return: Fairness report

**Task 3.1.6**: Create evaluation harness
- Duration: 30 minutes
- Create `eval/harness.py`
- Function: `run_evaluation(users: List[int]) -> dict`
- Run all metrics: Coverage, Explainability, Latency, Auditability, Fairness
- Generate: JSON/CSV metrics file
- Generate: Summary report (1-2 pages)

#### Phase 3.2: Evaluation Reports (90 minutes)

**Task 3.2.1**: Generate metrics file
- Duration: 30 minutes
- Function: `generate_metrics_file(results: dict, format: str = 'json')`
- Export: All metrics to JSON or CSV
- Include: Per-user metrics, aggregate metrics
- Save: `eval/output/metrics.json` or `metrics.csv`

**Task 3.2.2**: Generate summary report
- Duration: 45 minutes
- Function: `generate_summary_report(results: dict) -> str`
- Sections:
  - Executive Summary
  - Coverage Results
  - Explainability Results
  - Latency Results
  - Auditability Results
  - Fairness Analysis
  - Limitations and Recommendations
- Save: `eval/output/summary_report.md`

**Task 3.2.3**: Generate per-user decision traces
- Duration: 15 minutes
- Function: `generate_decision_traces(users: List[int])`
- Export: Decision trace for each user
- Include: Features, persona assignment, recommendation logic
- Save: `eval/output/decision_traces.json`

#### Phase 3.3: Custom Persona (90 minutes)

**Task 3.3.1**: Design custom persona (Persona 5)
- Duration: 30 minutes
- Create custom persona based on behavioral signals
- Example: "Credit Score Builder" (low utilization, on-time payments, growing savings)
- Document: Clear criteria, rationale, primary educational focus
- Add to `personas/personas.py`

**Task 3.3.2**: Implement custom persona assignment
- Duration: 30 minutes
- Add custom persona to assignment logic
- Update prioritization if multiple personas match
- Document prioritization logic clearly
- Test: Assignment for users matching custom persona

**Task 3.3.3**: Test custom persona
- Duration: 30 minutes
- Create test users matching custom persona
- Test: Persona assignment works
- Test: Recommendations match persona
- Verify: All criteria documented and working

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Write comprehensive unit tests
- Duration: 60 minutes
- Test: All signal detection functions
- Test: All persona assignment logic
- Test: All recommendation generation
- Test: All guardrails (consent, eligibility, tone)
- Test: All evaluation metrics
- Target: ≥10 unit tests

**Task 3.4.2**: Write integration tests
- Duration: 45 minutes
- Test: End-to-end flow (data → features → persona → recommendations)
- Test: Consent enforcement in full flow
- Test: Eligibility filtering in full flow
- Test: Operator approval flow
- Test: Evaluation harness execution

**Task 3.4.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup (1-2 pages)
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Key Technical Choices
  - Signal Detection Approach
  - Persona Assignment Logic
  - Recommendation Engine Design
  - Guardrails Implementation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why SQLite over PostgreSQL
  - Why Python over Node.js
  - Why rule-based over ML-based
  - Why virtual split (if applicable)
  - Tradeoffs and alternatives considered

**Task 3.5.3**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation (if applicable)
- Testing instructions
- Evaluation harness usage
- Troubleshooting section

**Task 3.5.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used (Cursor, Copilot, etc.)
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All Python/Node.js code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows Python/TypeScript best practices
- Review: Data processing logic for efficiency
- Optimize: Database queries and data aggregation

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: Data privacy and PII handling
- Check: Input validation and sanitization (synthetic data, user inputs)
- Check: SQL injection vulnerabilities (if using SQL)
- Check: API security (authentication, authorization)
- Review: Consent management implementation (secure storage)
- Review: Financial data handling (disclaimer compliance)
- Document: Security findings and mitigations
- Verify: No real PII in synthetic data

#### Phase 3.7: Final Polish (60 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `pylint` or `flake8`
- Fix all linting errors
- Format code: `black` or `autopep8`
- Remove debug code
- Add docstrings to all functions

**Task 3.7.2**: Final verification checklist
- Duration: 30 minutes
- ✅ All test scenarios pass (≥10 tests)
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Evaluation report generated
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "SpendSense - Complete financial education system"
  - System Overview (20s): Architecture, components, data flow
  - Signal Detection (30s): Show subscription, savings, credit, income detection
  - Persona Assignment (20s): Show 5 personas, assignment logic
  - Recommendation Engine (40s): Show recommendations with rationales, plain language
  - Guardrails (30s): Show consent enforcement, eligibility filtering, tone checking
  - Operator View (20s): Show operator reviewing and approving recommendations
  - Evaluation Results (20s): Show metrics (coverage, explainability, latency)
  - Conclusion (10s): "Complete explainable, consent-aware financial education system"

**Task 3.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of UI/CLI
- Highlight key metrics and achievements
- Show evaluation results

#### Phase 3.9: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `pytest`
- Run evaluation harness: `python eval/harness.py`
- Verify: All metrics calculated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Synthetic data generator creates 50-100 users
- ✅ Subscription signal detection working
- ✅ Savings signal detection working
- ✅ Credit utilization detection working
- ✅ Income stability detection working
- ✅ Feature aggregation working (30d and 180d windows)
- ✅ Basic persona assignment working (4 personas)
- ✅ Unit tests for signal detection passing

### Early Submission (Phase 2) - November 7
- ✅ Recommendation engine generating recommendations with rationales
- ✅ Consent management enforced
- ✅ Eligibility guardrails filtering ineligible offers
- ✅ Tone guardrails ensuring neutral language
- ✅ Operator view functional (CLI or web)
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Evaluation harness implemented and run
- ✅ Coverage: 100% users with persona + ≥3 behaviors
- ✅ Explainability: 100% recommendations with rationales
- ✅ Latency: <5 seconds per user
- ✅ Auditability: 100% with decision traces
- ✅ Custom persona (5th persona) implemented
- ✅ Comprehensive test suite (≥10 tests) passing
- ✅ Technical writeup complete
- ✅ Evaluation report generated

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Synthetic data unrealistic | Medium | Use real-world transaction patterns, validate with domain knowledge |
| Signal detection false positives | Medium | Tune thresholds based on test data, validate with manual review |
| Persona assignment errors | Medium | Document criteria clearly, test with known users, log assignment rationale |
| Recommendation quality | High | Use plain language, cite data, validate with operator review |
| Latency too high | Medium | Optimize database queries, cache features, profile bottlenecks |
| Time constraints | High | MVP is safety net, recommendations are enhancement, prioritize core signals first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] All Python code in organized modules
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] requirements.txt with dependencies
- [ ] .env.example with configuration placeholders
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] API documentation (if applicable)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all signal detection functions (≥10 tests)
- [ ] Integration tests for full flow
- [ ] Evaluation harness results

### Evaluation
- [ ] Metrics file (JSON/CSV)
- [ ] Summary report (1-2 pages)
- [ ] Per-user decision traces
- [ ] Fairness analysis (if applicable)

### Demo
- [ ] Demo script or UI working end-to-end
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card

---

## Notes

- **No Financial Advice**: Include disclaimer: "This is educational content, not financial advice. Consult a licensed advisor for personalized guidance."
- **No Real PII**: Synthetic data only, fake names, masked account numbers
- **Consent First**: No recommendations without explicit consent
- **Explainability Required**: Every recommendation needs a clear rationale
- **Plain Language**: No jargon, cite concrete data points

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

Adding the code evaluation/security assessment task to the remaining 3 PRDs. Creating a script to apply similar changes across all files:


```markdown
# Product Requirements Document: AI Math Tutor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Socratic Learning Assistant with OCR/Vision LLM Parsing

---

## Executive Summary

AI Math Tutor is an AI-powered educational assistant that guides students through math problems using Socratic questioning. The system accepts problems via screenshot or text input, parses them using OCR/Vision LLM, and maintains multi-turn conversations that guide students to discover solutions without providing direct answers.

**Key Deliverables:**
- ✅ Problem Input: Text entry + image upload with OCR/Vision LLM parsing
- ✅ Socratic Dialogue: Multi-turn conversation with guiding questions
- ✅ Math Rendering: Display equations properly (LaTeX/KaTeX)
- ✅ Web Interface: Clean chat UI with image upload and conversation history
- ✅ Context Management: Maintains conversation context across turns
- ✅ Adaptive Scaffolding: Adjusts to student understanding level

**Success Targets:**
- **MVP (Nov 4)**: Basic chat + LLM integration + Socratic prompting + image parsing
- **Early Submission (Nov 7)**: + Math rendering + conversation history + UI polish
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + optional stretch features

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Basic chat UI, LLM integration, Socratic prompting, image parsing working |
| **Early Submission** | Friday, November 7, 2025 | + Math rendering (LaTeX), conversation history, UI polish |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive testing, documentation, optional stretch features |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Math Rendering**: KaTeX or MathJax
- **Image Upload**: React Dropzone or similar
- **Chat UI**: Custom chat component or react-chat-elements
- **Build Tool**: Vite or Next.js (if SSR needed)

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API routes
- **Language**: TypeScript
- **AI Integration**: OpenAI GPT-4 with Vision API or Anthropic Claude with vision
- **Image Processing**: Sharp (for image optimization)
- **Storage**: SQLite (local) or PostgreSQL (if deployed)
- **Session Management**: In-memory or Redis (if deployed)

### AI Integration
- **LLM**: OpenAI GPT-4 Turbo with Vision or Anthropic Claude 3.5 Sonnet with vision
- **SDK**: OpenAI Node.js SDK or Anthropic SDK
- **Image Processing**: OpenAI Vision API for OCR or Tesseract.js (local OCR)
- **Prompt Engineering**: Structured prompts for Socratic method

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
AI-Math-Tutor/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── Chat.tsx              # Main chat interface
│   │   │   ├── MessageBubble.tsx     # Message display
│   │   │   ├── ImageUpload.tsx       # Image upload component
│   │   │   ├── MathRenderer.tsx      # LaTeX math rendering
│   │   │   └── ConversationHistory.tsx # History sidebar
│   │   ├── hooks/
│   │   │   ├── useChat.ts            # Chat state management
│   │   │   └── useImageUpload.ts     # Image upload logic
│   │   ├── services/
│   │   │   ├── api.ts                # API client
│   │   │   └── mathParser.ts         # Math parsing utilities
│   │   └── App.tsx
│   └── public/
├── backend/
│   ├── api/
│   │   ├── chat.ts                   # Chat endpoint
│   │   ├── image-upload.ts            # Image upload endpoint
│   │   └── health.ts                  # Health check
│   ├── services/
│   │   ├── llm.ts                     # LLM service
│   │   ├── imageProcessor.ts          # Image processing
│   │   ├── socratic.ts                # Socratic method logic
│   │   └── contextManager.ts         # Conversation context
│   ├── prompts/
│   │   └── socratic.ts                # Socratic prompt templates
│   └── server.ts                      # Express server
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── prompt-engineering-notes.md
```

### Data Flow

1. **Problem Input**: User uploads image or types text → Image processed via Vision API → Problem text extracted
2. **Socratic Dialogue**: Problem text → LLM with Socratic prompt → Guiding question → User response → Context updated
3. **Math Rendering**: Mathematical expressions → LaTeX parser → KaTeX renderer → Displayed in UI
4. **Context Management**: Each turn → Context stored → Previous turns retrieved → Full conversation context maintained

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic chat with LLM integration and Socratic prompting working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 20 minutes
- Create `ai-math-tutor/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install dependencies: React, TypeScript, Tailwind CSS
- Verify setup: `npm run dev`
- PowerShell: `New-Item -ItemType Directory -Path ai-math-tutor; Set-Location ai-math-tutor; npm create vite@latest frontend -- --template react-ts`

**Task 1.1.2**: Setup backend with Express
- Duration: 20 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, tsx, OpenAI SDK
- Create `server.ts` with basic Express setup
- Add TypeScript config: `tsconfig.json`
- PowerShell: `New-Item -ItemType Directory -Path backend; Set-Location backend; npm init -y; npm install express typescript tsx @types/express @types/node`

**Task 1.1.3**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files (frontend and backend)
- Add: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries
- Document setup in README

**Task 1.1.4**: Setup testing framework
- Duration: 20 minutes
- Install Vitest: `npm install -D vitest @testing-library/react`
- Create `tests/` directory structure
- Create sample test to verify setup
- Run: `npm test` to verify working

**Task 1.1.5**: Create project structure
- Duration: 15 minutes
- Create directory structure (components, services, hooks)
- Create placeholder files for main components
- Add basic README with setup instructions

#### Phase 1.2: Basic Chat UI (120 minutes)

**Task 1.2.1**: Create chat interface component
- Duration: 45 minutes
- Create `frontend/src/components/Chat.tsx`
- Basic chat UI with:
  - Message input area
  - Send button
  - Message list area
  - Simple styling with Tailwind
- Add state management for messages

**Task 1.2.2**: Create message bubble component
- Duration: 30 minutes
- Create `frontend/src/components/MessageBubble.tsx`
- Display: User messages (right-aligned), Tutor messages (left-aligned)
- Add basic styling and animations
- Show timestamps

**Task 1.2.3**: Create chat hook for state management
- Duration: 30 minutes
- Create `frontend/src/hooks/useChat.ts`
- Manage: Messages array, current input, loading state
- Functions: `sendMessage()`, `addMessage()`, `clearChat()`
- Return: Messages, loading, sendMessage function

**Task 1.2.4**: Create API client
- Duration: 15 minutes
- Create `frontend/src/services/api.ts`
- Function: `sendMessage(message: string) => Promise<string>`
- POST to `/api/chat` endpoint
- Handle errors and loading states

#### Phase 1.3: Backend API - Chat Endpoint (120 minutes)

**Task 1.3.1**: Create Express server with chat endpoint
- Duration: 30 minutes
- Create `backend/server.ts`
- Setup Express with CORS middleware
- Create `/api/chat` POST endpoint
- Receive: `{ message: string, conversationHistory?: Message[] }`
- Return: `{ response: string }`

**Task 1.3.2**: Integrate OpenAI SDK
- Duration: 30 minutes
- Create `backend/services/llm.ts`
- Setup OpenAI client with API key
- Function: `generateResponse(messages: Message[]) => Promise<string>`
- Call `openai.chat.completions.create()` with messages
- Return assistant message

**Task 1.3.3**: Create Socratic prompt template
- Duration: 30 minutes
- Create `backend/prompts/socratic.ts`
- System prompt: "You are a patient math tutor. NEVER give direct answers. Guide through questions: 'What information do we have?' 'What method might help?' If stuck >2 turns, provide concrete hint. Use encouraging language."
- Format: System message + conversation history
- Add prompt validation

**Task 1.3.4**: Implement Socratic logic
- Duration: 30 minutes
- Create `backend/services/socratic.ts`
- Function: `buildPrompt(conversationHistory: Message[]) => Message[]`
- Logic:
  - If first message: Parse problem → Inventory knowns → Identify goal
  - If subsequent: Guide method selection → Step through solution → Validate answer
- Add validation for Socratic method (no direct answers)

#### Phase 1.4: Image Upload & OCR/Vision LLM (150 minutes)

**Task 1.4.1**: Create image upload component
- Duration: 45 minutes
- Create `frontend/src/components/ImageUpload.tsx`
- Use React Dropzone or similar
- Features: Drag & drop, click to upload, preview image
- Accept: PNG, JPG, JPEG formats
- Display upload progress

**Task 1.4.2**: Create image upload API endpoint
- Duration: 30 minutes
- Create `/api/image-upload` POST endpoint
- Use Multer or similar for file handling
- Save uploaded image temporarily
- Return: Image URL or base64 data

**Task 1.4.3**: Integrate Vision API for OCR
- Duration: 45 minutes
- Create `backend/services/imageProcessor.ts`
- Function: `extractTextFromImage(imagePath: string) => Promise<string>`
- Option A: OpenAI Vision API:
  - Use `openai.chat.completions.create()` with vision model
  - Prompt: "Extract all mathematical expressions and text from this image. Return the problem statement."
  - Parse response for problem text
- Option B: Tesseract.js (local OCR):
  - Install Tesseract.js
  - Process image locally
  - Extract text (less accurate for math)

**Task 1.4.4**: Integrate image upload into chat flow
- Duration: 30 minutes
- Modify chat UI to include image upload
- When image uploaded: Extract text → Send to LLM as first message
- Handle: Image processing → Text extraction → Problem parsing → Chat start
- Add loading state for image processing

#### Phase 1.5: Conversation Context Management (90 minutes)

**Task 1.5.1**: Implement context manager
- Duration: 45 minutes
- Create `backend/services/contextManager.ts`
- Function: `buildContext(userId: string, conversationId: string) => Message[]`
- Store: Messages in-memory (Map<conversationId, Message[]>)
- Or: Use SQLite for persistence
- Retrieve: Full conversation history for context

**Task 1.5.2**: Add context to chat endpoint
- Duration: 30 minutes
- Modify `/api/chat` endpoint to accept `conversationId`
- Retrieve conversation history from context manager
- Pass full history to LLM (last 10-20 messages for token efficiency)
- Update context after each message

**Task 1.5.3**: Test context management
- Duration: 15 minutes
- Test: Multi-turn conversation maintains context
- Test: LLM references previous messages
- Test: Context persists across API calls
- Verify: Conversation history is accurate

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Write unit tests for chat logic
- Duration: 45 minutes
- Test: Message sending and receiving
- Test: Context management (add/retrieve messages)
- Test: Socratic prompt building
- Test: Image text extraction (mock)
- Run: `npm test` to verify

**Task 1.6.2**: Test with hardcoded problems
- Duration: 30 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problem: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
- Verify: Socratic method works (no direct answers)
- Verify: Context maintained across turns

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All unit tests pass
- Verify: Chat flow works end-to-end
- Document any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor MVP - Socratic learning assistant"
  - Problem Input (30s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (60s): Show multi-turn conversation, demonstrate guiding questions (no direct answers)
  - Context Management (30s): Show how tutor references previous messages
  - Conclusion (10s): "MVP demonstrates core Socratic method with image parsing and context management"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for chat interface
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add math rendering, conversation history, UI polish

#### Phase 2.1: Math Rendering with LaTeX/KaTeX (120 minutes)

**Task 2.1.1**: Install and configure KaTeX
- Duration: 20 minutes
- Install: `npm install katex react-katex`
- Add KaTeX CSS to main CSS file
- Create `frontend/src/components/MathRenderer.tsx`
- Function: `renderMath(text: string) => JSX.Element`
- Support: Inline math `$...$` and display math `$$...$$`

**Task 2.1.2**: Parse mathematical expressions from messages
- Duration: 45 minutes
- Create `frontend/src/services/mathParser.ts`
- Function: `parseMath(text: string) => string`
- Extract: LaTeX expressions from text
- Handle: Inline math and display math
- Preserve: Non-math text

**Task 2.1.3**: Integrate math rendering into message bubbles
- Duration: 35 minutes
- Modify `MessageBubble.tsx` to render math
- Parse message text for math expressions
- Render math with KaTeX component
- Handle: Error cases (invalid LaTeX)
- Style: Math expressions appropriately

**Task 2.1.4**: Test math rendering
- Duration: 20 minutes
- Test: Simple equations (`x + 5 = 13`)
- Test: Complex equations (`x² + 5x + 6 = 0`)
- Test: Fractions, exponents, integrals
- Test: Mixed text and math
- Verify: All expressions render correctly

#### Phase 2.2: Conversation History (90 minutes)

**Task 2.2.1**: Create conversation history UI
- Duration: 45 minutes
- Create `frontend/src/components/ConversationHistory.tsx`
- Display: List of past conversations
- Features:
  - Conversation titles (first problem statement)
  - Timestamp
  - Click to load conversation
  - Delete conversation
- Sidebar or dropdown menu

**Task 2.2.2**: Persist conversations in backend
- Duration: 30 minutes
- Create SQLite database for conversations
- Schema: `conversations(id, userId, title, createdAt)`, `messages(conversationId, role, content, timestamp)`
- Store: Full conversation history
- Retrieve: Conversations by user

**Task 2.2.3**: Load conversation from history
- Duration: 15 minutes
- Add API endpoint: `GET /api/conversations/:id`
- Load conversation messages into chat
- Update context manager with loaded conversation
- Test: Loading conversation works correctly

#### Phase 2.3: UI Polish & Enhancements (120 minutes)

**Task 2.3.1**: Improve chat UI design
- Duration: 45 minutes
- Enhance: Message bubble styling
- Add: Typing indicator
- Add: Loading states for LLM responses
- Add: Smooth scroll to latest message
- Add: Dark mode (optional)

**Task 2.3.2**: Add error handling and user feedback
- Duration: 30 minutes
- Display: Error messages for failed API calls
- Add: Retry button for failed requests
- Show: Network errors clearly
- Add: Input validation (empty messages, file size limits)

**Task 2.3.3**: Optimize image upload UX
- Duration: 30 minutes
- Add: Image preview before upload
- Add: Image compression (if needed)
- Add: Progress bar for upload
- Add: Error handling for unsupported formats
- Add: Maximum file size validation

**Task 2.3.4**: Add keyboard shortcuts
- Duration: 15 minutes
- Add: Enter to send message
- Add: Shift+Enter for new line
- Add: Escape to clear input
- Test: Keyboard shortcuts work correctly

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Test math rendering with various problems
- Duration: 30 minutes
- Test: 5+ problem types (arithmetic, algebra, geometry, word problems, multi-step)
- Verify: All math expressions render correctly
- Verify: LaTeX syntax is handled properly
- Document: Any rendering issues

**Task 2.4.2**: Test conversation history
- Duration: 30 minutes
- Test: Saving conversations
- Test: Loading conversations
- Test: Deleting conversations
- Test: Multiple conversations per user
- Verify: Context maintained when loading

**Task 2.4.3**: Integration tests for full flow
- Duration: 30 minutes
- Test: Upload image → Extract text → Start chat → Socratic dialogue → Math rendering
- Test: Load conversation → Continue chat → Context maintained
- Test: Error scenarios (API failures, invalid images)
- Verify: All features work together

**Task 2.4.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor Early Submission - now with math rendering and history"
  - Problem Input (25s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (50s): Show multi-turn conversation, demonstrate guiding questions, show math rendering
  - Conversation History (30s): Show past conversations, load conversation, continue dialogue
  - UI Polish (25s): Show improved UI, error handling, keyboard shortcuts
  - Conclusion (10s): "Demonstrates complete Socratic learning experience with math rendering"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for improved interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optional stretch features

#### Phase 3.1: Comprehensive Testing (120 minutes)

**Task 3.1.1**: Test all 5+ problem types
- Duration: 60 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problems: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
  6. Fractions: "1/2 + 1/3 = ?"
  7. Exponents: "2³ × 2² = ?"
- Verify: Socratic method works for all types
- Verify: No direct answers given
- Verify: Context maintained across turns

**Task 3.1.2**: Test edge cases and error scenarios
- Duration: 45 minutes
- Test: Invalid image formats
- Test: Very large images
- Test: Blurry images (OCR failure)
- Test: No text in image
- Test: Network errors
- Test: LLM API errors
- Test: Empty messages
- Test: Very long conversations (>20 turns)

**Task 3.1.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.2: Optional Stretch Features (180 minutes)

**Task 3.2.1**: Interactive Whiteboard (Optional - High Value)
- Duration: 90 minutes
- Install: react-sketch-canvas or similar
- Create: `Whiteboard.tsx` component
- Features:
  - Draw diagrams and equations
  - Share with tutor
  - Tutor can draw explanations
- Integrate: Into chat flow
- Test: Whiteboard functionality

**Task 3.2.2**: Step Visualization (Optional - High Value)
- Duration: 60 minutes
- Create: `StepVisualization.tsx` component
- Display: Solution steps animated or sequentially
- Parse: LLM response for steps
- Render: Each step with math rendering
- Test: Step visualization works

**Task 3.2.3**: Voice Interface (Optional - High Value)
- Duration: 30 minutes
- Add: Text-to-speech for tutor responses
- Add: Speech-to-text for student input
- Use: Web Speech API
- Test: Voice interface works

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - LLM Integration (OpenAI/Anthropic)
  - Socratic Method Implementation
  - Image Processing & OCR
  - Math Rendering (KaTeX)
  - Context Management
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Write prompt engineering notes
- Duration: 30 minutes
- Document: Socratic prompt design
- Document: Prompt iterations and improvements
- Document: Techniques for preventing direct answers
- Document: Context management strategies
- Include: Example prompts and responses

**Task 3.3.3**: Update README with complete setup
- Duration: 30 minutes
- Prerequisites (Node.js, npm, API keys)
- Installation steps
- Environment setup
- Running locally
- Testing instructions
- Deployment instructions (if applicable)
- Troubleshooting section

**Task 3.3.4**: Document 5+ example problem walkthroughs
- Duration: 15 minutes
- Document: 5+ complete problem walkthroughs
- Include: Problem statement, conversation flow, final answer
- Show: Socratic method in action
- Show: Context maintained across turns

**Task 3.3.5**: Document AI tools and prompts used
- Duration: 15 minutes
- List: AI tools used (Cursor, Copilot, etc.)
- Document: Key prompts that generated code
- Acknowledge: AI assistance in README

#### Phase 3.4: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.4.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend API code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and context management

**Task 3.4.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (image uploads, user messages)
- Check: Image processing security (file size limits, type validation)
- Review: LLM API usage (rate limiting, error handling)
- Review: Session management and context storage
- Check: CORS configuration (if applicable)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code

#### Phase 3.5: Final Polish (90 minutes)

**Task 3.5.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove: Debug code and console.logs
- Add: JSDoc comments to all functions

**Task 3.4.2**: Performance optimization
- Duration: 30 minutes
- Optimize: Image upload (compression if needed)
- Optimize: Math rendering (cache if possible)
- Optimize: Context management (limit history length)
- Profile: API response times
- Optimize: Re-renders in React

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All 5+ problem types tested
- ✅ Socratic method works (no direct answers)
- ✅ Context maintained across turns
- ✅ Math rendering works correctly
- ✅ Image upload and OCR working
- ✅ Conversation history working
- ✅ README complete and accurate
- ✅ Documentation complete
- ✅ Tests passing
- ✅ No API keys in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor - Complete Socratic learning assistant"
  - System Overview (15s): Architecture, key features
  - Problem Input (25s): Text input and image upload, OCR demonstration
  - Socratic Dialogue (60s): Multi-turn conversation, guiding questions, math rendering, no direct answers
  - Conversation History (20s): Past conversations, loading and continuing
  - Stretch Features (20s): Whiteboard/step visualization/voice (if built)
  - Conclusion (10s): "Complete Socratic learning experience with comprehensive features"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all features
- Highlight key technical achievements
- Show Socratic method in action

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Check: Test coverage (aim for >80%)
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Problem input working (text + image upload)
- ✅ OCR/Vision LLM parsing working
- ✅ Basic chat UI functional
- ✅ LLM integration with Socratic prompting
- ✅ Socratic method working (no direct answers)
- ✅ Context management working
- ✅ Multi-turn conversations functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Math rendering with LaTeX/KaTeX working
- ✅ Conversation history implemented
- ✅ UI polish and enhancements complete
- ✅ Integration tests passing
- ✅ Tested on 5+ problem types

### Final Submission (Phase 3) - November 9
- ✅ Comprehensive testing (5+ problem types)
- ✅ Edge cases handled gracefully
- ✅ Documentation complete (technical writeup, prompt notes, README, walkthroughs)
- ✅ Optional stretch features implemented (if time permits)
- ✅ Code clean, linted, and documented
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM gives direct answers | High | Strict Socratic prompt, validation in code, test thoroughly |
| OCR fails on images | Medium | Fallback to text input, clear error messages, test with various images |
| Context too long for LLM | Medium | Limit conversation history (last 10-20 messages), summarize if needed |
| API costs too high | Medium | Cache responses if possible, use cheaper models for iteration |
| Math rendering errors | Low | Validate LaTeX syntax, handle errors gracefully |
| Time constraints | High | MVP is safety net, stretch features are optional, prioritize core Socratic method |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Prompt engineering notes
- [ ] README with complete setup and usage
- [ ] 5+ example problem walkthroughs
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for chat logic
- [ ] Integration tests for full flow
- [ ] Tests for 5+ problem types
- [ ] Edge case tests

### Demo
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployed app or local setup instructions

---

## Notes

- **Socratic Method**: NEVER give direct answers. Guide through questions only.
- **Context Management**: Maintain full conversation history for proper context.
- **Math Rendering**: Use KaTeX for accurate math display.
- **Image Processing**: Use Vision API for best OCR results (or Tesseract.js for local).
- **Testing**: Test with diverse problem types to ensure Socratic method works.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: AI Math Tutor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Socratic Learning Assistant with OCR/Vision LLM Parsing

---

## Executive Summary

AI Math Tutor is an AI-powered educational assistant that guides students through math problems using Socratic questioning. The system accepts problems via screenshot or text input, parses them using OCR/Vision LLM, and maintains multi-turn conversations that guide students to discover solutions without providing direct answers.

**Key Deliverables:**
- ✅ Problem Input: Text entry + image upload with OCR/Vision LLM parsing
- ✅ Socratic Dialogue: Multi-turn conversation with guiding questions
- ✅ Math Rendering: Display equations properly (LaTeX/KaTeX)
- ✅ Web Interface: Clean chat UI with image upload and conversation history
- ✅ Context Management: Maintains conversation context across turns
- ✅ Adaptive Scaffolding: Adjusts to student understanding level

**Success Targets:**
- **MVP (Nov 4)**: Basic chat + LLM integration + Socratic prompting + image parsing
- **Early Submission (Nov 7)**: + Math rendering + conversation history + UI polish
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + optional stretch features

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Basic chat UI, LLM integration, Socratic prompting, image parsing working |
| **Early Submission** | Friday, November 7, 2025 | + Math rendering (LaTeX), conversation history, UI polish |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive testing, documentation, optional stretch features |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Math Rendering**: KaTeX or MathJax
- **Image Upload**: React Dropzone or similar
- **Chat UI**: Custom chat component or react-chat-elements
- **Build Tool**: Vite or Next.js (if SSR needed)

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API routes
- **Language**: TypeScript
- **AI Integration**: OpenAI GPT-4 with Vision API or Anthropic Claude with vision
- **Image Processing**: Sharp (for image optimization)
- **Storage**: SQLite (local) or PostgreSQL (if deployed)
- **Session Management**: In-memory or Redis (if deployed)

### AI Integration
- **LLM**: OpenAI GPT-4 Turbo with Vision or Anthropic Claude 3.5 Sonnet with vision
- **SDK**: OpenAI Node.js SDK or Anthropic SDK
- **Image Processing**: OpenAI Vision API for OCR or Tesseract.js (local OCR)
- **Prompt Engineering**: Structured prompts for Socratic method

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
AI-Math-Tutor/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── Chat.tsx              # Main chat interface
│   │   │   ├── MessageBubble.tsx     # Message display
│   │   │   ├── ImageUpload.tsx       # Image upload component
│   │   │   ├── MathRenderer.tsx      # LaTeX math rendering
│   │   │   └── ConversationHistory.tsx # History sidebar
│   │   ├── hooks/
│   │   │   ├── useChat.ts            # Chat state management
│   │   │   └── useImageUpload.ts     # Image upload logic
│   │   ├── services/
│   │   │   ├── api.ts                # API client
│   │   │   └── mathParser.ts         # Math parsing utilities
│   │   └── App.tsx
│   └── public/
├── backend/
│   ├── api/
│   │   ├── chat.ts                   # Chat endpoint
│   │   ├── image-upload.ts            # Image upload endpoint
│   │   └── health.ts                  # Health check
│   ├── services/
│   │   ├── llm.ts                     # LLM service
│   │   ├── imageProcessor.ts          # Image processing
│   │   ├── socratic.ts                # Socratic method logic
│   │   └── contextManager.ts         # Conversation context
│   ├── prompts/
│   │   └── socratic.ts                # Socratic prompt templates
│   └── server.ts                      # Express server
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── prompt-engineering-notes.md
```

### Data Flow

1. **Problem Input**: User uploads image or types text → Image processed via Vision API → Problem text extracted
2. **Socratic Dialogue**: Problem text → LLM with Socratic prompt → Guiding question → User response → Context updated
3. **Math Rendering**: Mathematical expressions → LaTeX parser → KaTeX renderer → Displayed in UI
4. **Context Management**: Each turn → Context stored → Previous turns retrieved → Full conversation context maintained

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic chat with LLM integration and Socratic prompting working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 20 minutes
- Create `ai-math-tutor/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install dependencies: React, TypeScript, Tailwind CSS
- Verify setup: `npm run dev`
- PowerShell: `New-Item -ItemType Directory -Path ai-math-tutor; Set-Location ai-math-tutor; npm create vite@latest frontend -- --template react-ts`

**Task 1.1.2**: Setup backend with Express
- Duration: 20 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, tsx, OpenAI SDK
- Create `server.ts` with basic Express setup
- Add TypeScript config: `tsconfig.json`
- PowerShell: `New-Item -ItemType Directory -Path backend; Set-Location backend; npm init -y; npm install express typescript tsx @types/express @types/node`

**Task 1.1.3**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files (frontend and backend)
- Add: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries
- Document setup in README

**Task 1.1.4**: Setup testing framework
- Duration: 20 minutes
- Install Vitest: `npm install -D vitest @testing-library/react`
- Create `tests/` directory structure
- Create sample test to verify setup
- Run: `npm test` to verify working

**Task 1.1.5**: Create project structure
- Duration: 15 minutes
- Create directory structure (components, services, hooks)
- Create placeholder files for main components
- Add basic README with setup instructions

#### Phase 1.2: Basic Chat UI (120 minutes)

**Task 1.2.1**: Create chat interface component
- Duration: 45 minutes
- Create `frontend/src/components/Chat.tsx`
- Basic chat UI with:
  - Message input area
  - Send button
  - Message list area
  - Simple styling with Tailwind
- Add state management for messages

**Task 1.2.2**: Create message bubble component
- Duration: 30 minutes
- Create `frontend/src/components/MessageBubble.tsx`
- Display: User messages (right-aligned), Tutor messages (left-aligned)
- Add basic styling and animations
- Show timestamps

**Task 1.2.3**: Create chat hook for state management
- Duration: 30 minutes
- Create `frontend/src/hooks/useChat.ts`
- Manage: Messages array, current input, loading state
- Functions: `sendMessage()`, `addMessage()`, `clearChat()`
- Return: Messages, loading, sendMessage function

**Task 1.2.4**: Create API client
- Duration: 15 minutes
- Create `frontend/src/services/api.ts`
- Function: `sendMessage(message: string) => Promise<string>`
- POST to `/api/chat` endpoint
- Handle errors and loading states

#### Phase 1.3: Backend API - Chat Endpoint (120 minutes)

**Task 1.3.1**: Create Express server with chat endpoint
- Duration: 30 minutes
- Create `backend/server.ts`
- Setup Express with CORS middleware
- Create `/api/chat` POST endpoint
- Receive: `{ message: string, conversationHistory?: Message[] }`
- Return: `{ response: string }`

**Task 1.3.2**: Integrate OpenAI SDK
- Duration: 30 minutes
- Create `backend/services/llm.ts`
- Setup OpenAI client with API key
- Function: `generateResponse(messages: Message[]) => Promise<string>`
- Call `openai.chat.completions.create()` with messages
- Return assistant message

**Task 1.3.3**: Create Socratic prompt template
- Duration: 30 minutes
- Create `backend/prompts/socratic.ts`
- System prompt: "You are a patient math tutor. NEVER give direct answers. Guide through questions: 'What information do we have?' 'What method might help?' If stuck >2 turns, provide concrete hint. Use encouraging language."
- Format: System message + conversation history
- Add prompt validation

**Task 1.3.4**: Implement Socratic logic
- Duration: 30 minutes
- Create `backend/services/socratic.ts`
- Function: `buildPrompt(conversationHistory: Message[]) => Message[]`
- Logic:
  - If first message: Parse problem → Inventory knowns → Identify goal
  - If subsequent: Guide method selection → Step through solution → Validate answer
- Add validation for Socratic method (no direct answers)

#### Phase 1.4: Image Upload & OCR/Vision LLM (150 minutes)

**Task 1.4.1**: Create image upload component
- Duration: 45 minutes
- Create `frontend/src/components/ImageUpload.tsx`
- Use React Dropzone or similar
- Features: Drag & drop, click to upload, preview image
- Accept: PNG, JPG, JPEG formats
- Display upload progress

**Task 1.4.2**: Create image upload API endpoint
- Duration: 30 minutes
- Create `/api/image-upload` POST endpoint
- Use Multer or similar for file handling
- Save uploaded image temporarily
- Return: Image URL or base64 data

**Task 1.4.3**: Integrate Vision API for OCR
- Duration: 45 minutes
- Create `backend/services/imageProcessor.ts`
- Function: `extractTextFromImage(imagePath: string) => Promise<string>`
- Option A: OpenAI Vision API:
  - Use `openai.chat.completions.create()` with vision model
  - Prompt: "Extract all mathematical expressions and text from this image. Return the problem statement."
  - Parse response for problem text
- Option B: Tesseract.js (local OCR):
  - Install Tesseract.js
  - Process image locally
  - Extract text (less accurate for math)

**Task 1.4.4**: Integrate image upload into chat flow
- Duration: 30 minutes
- Modify chat UI to include image upload
- When image uploaded: Extract text → Send to LLM as first message
- Handle: Image processing → Text extraction → Problem parsing → Chat start
- Add loading state for image processing

#### Phase 1.5: Conversation Context Management (90 minutes)

**Task 1.5.1**: Implement context manager
- Duration: 45 minutes
- Create `backend/services/contextManager.ts`
- Function: `buildContext(userId: string, conversationId: string) => Message[]`
- Store: Messages in-memory (Map<conversationId, Message[]>)
- Or: Use SQLite for persistence
- Retrieve: Full conversation history for context

**Task 1.5.2**: Add context to chat endpoint
- Duration: 30 minutes
- Modify `/api/chat` endpoint to accept `conversationId`
- Retrieve conversation history from context manager
- Pass full history to LLM (last 10-20 messages for token efficiency)
- Update context after each message

**Task 1.5.3**: Test context management
- Duration: 15 minutes
- Test: Multi-turn conversation maintains context
- Test: LLM references previous messages
- Test: Context persists across API calls
- Verify: Conversation history is accurate

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Write unit tests for chat logic
- Duration: 45 minutes
- Test: Message sending and receiving
- Test: Context management (add/retrieve messages)
- Test: Socratic prompt building
- Test: Image text extraction (mock)
- Run: `npm test` to verify

**Task 1.6.2**: Test with hardcoded problems
- Duration: 30 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problem: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
- Verify: Socratic method works (no direct answers)
- Verify: Context maintained across turns

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All unit tests pass
- Verify: Chat flow works end-to-end
- Document any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor MVP - Socratic learning assistant"
  - Problem Input (30s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (60s): Show multi-turn conversation, demonstrate guiding questions (no direct answers)
  - Context Management (30s): Show how tutor references previous messages
  - Conclusion (10s): "MVP demonstrates core Socratic method with image parsing and context management"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for chat interface
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add math rendering, conversation history, UI polish

#### Phase 2.1: Math Rendering with LaTeX/KaTeX (120 minutes)

**Task 2.1.1**: Install and configure KaTeX
- Duration: 20 minutes
- Install: `npm install katex react-katex`
- Add KaTeX CSS to main CSS file
- Create `frontend/src/components/MathRenderer.tsx`
- Function: `renderMath(text: string) => JSX.Element`
- Support: Inline math `$...$` and display math `$$...$$`

**Task 2.1.2**: Parse mathematical expressions from messages
- Duration: 45 minutes
- Create `frontend/src/services/mathParser.ts`
- Function: `parseMath(text: string) => string`
- Extract: LaTeX expressions from text
- Handle: Inline math and display math
- Preserve: Non-math text

**Task 2.1.3**: Integrate math rendering into message bubbles
- Duration: 35 minutes
- Modify `MessageBubble.tsx` to render math
- Parse message text for math expressions
- Render math with KaTeX component
- Handle: Error cases (invalid LaTeX)
- Style: Math expressions appropriately

**Task 2.1.4**: Test math rendering
- Duration: 20 minutes
- Test: Simple equations (`x + 5 = 13`)
- Test: Complex equations (`x² + 5x + 6 = 0`)
- Test: Fractions, exponents, integrals
- Test: Mixed text and math
- Verify: All expressions render correctly

#### Phase 2.2: Conversation History (90 minutes)

**Task 2.2.1**: Create conversation history UI
- Duration: 45 minutes
- Create `frontend/src/components/ConversationHistory.tsx`
- Display: List of past conversations
- Features:
  - Conversation titles (first problem statement)
  - Timestamp
  - Click to load conversation
  - Delete conversation
- Sidebar or dropdown menu

**Task 2.2.2**: Persist conversations in backend
- Duration: 30 minutes
- Create SQLite database for conversations
- Schema: `conversations(id, userId, title, createdAt)`, `messages(conversationId, role, content, timestamp)`
- Store: Full conversation history
- Retrieve: Conversations by user

**Task 2.2.3**: Load conversation from history
- Duration: 15 minutes
- Add API endpoint: `GET /api/conversations/:id`
- Load conversation messages into chat
- Update context manager with loaded conversation
- Test: Loading conversation works correctly

#### Phase 2.3: UI Polish & Enhancements (120 minutes)

**Task 2.3.1**: Improve chat UI design
- Duration: 45 minutes
- Enhance: Message bubble styling
- Add: Typing indicator
- Add: Loading states for LLM responses
- Add: Smooth scroll to latest message
- Add: Dark mode (optional)

**Task 2.3.2**: Add error handling and user feedback
- Duration: 30 minutes
- Display: Error messages for failed API calls
- Add: Retry button for failed requests
- Show: Network errors clearly
- Add: Input validation (empty messages, file size limits)

**Task 2.3.3**: Optimize image upload UX
- Duration: 30 minutes
- Add: Image preview before upload
- Add: Image compression (if needed)
- Add: Progress bar for upload
- Add: Error handling for unsupported formats
- Add: Maximum file size validation

**Task 2.3.4**: Add keyboard shortcuts
- Duration: 15 minutes
- Add: Enter to send message
- Add: Shift+Enter for new line
- Add: Escape to clear input
- Test: Keyboard shortcuts work correctly

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Test math rendering with various problems
- Duration: 30 minutes
- Test: 5+ problem types (arithmetic, algebra, geometry, word problems, multi-step)
- Verify: All math expressions render correctly
- Verify: LaTeX syntax is handled properly
- Document: Any rendering issues

**Task 2.4.2**: Test conversation history
- Duration: 30 minutes
- Test: Saving conversations
- Test: Loading conversations
- Test: Deleting conversations
- Test: Multiple conversations per user
- Verify: Context maintained when loading

**Task 2.4.3**: Integration tests for full flow
- Duration: 30 minutes
- Test: Upload image → Extract text → Start chat → Socratic dialogue → Math rendering
- Test: Load conversation → Continue chat → Context maintained
- Test: Error scenarios (API failures, invalid images)
- Verify: All features work together

**Task 2.4.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor Early Submission - now with math rendering and history"
  - Problem Input (25s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (50s): Show multi-turn conversation, demonstrate guiding questions, show math rendering
  - Conversation History (30s): Show past conversations, load conversation, continue dialogue
  - UI Polish (25s): Show improved UI, error handling, keyboard shortcuts
  - Conclusion (10s): "Demonstrates complete Socratic learning experience with math rendering"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for improved interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optional stretch features

#### Phase 3.1: Comprehensive Testing (120 minutes)

**Task 3.1.1**: Test all 5+ problem types
- Duration: 60 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problems: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
  6. Fractions: "1/2 + 1/3 = ?"
  7. Exponents: "2³ × 2² = ?"
- Verify: Socratic method works for all types
- Verify: No direct answers given
- Verify: Context maintained across turns

**Task 3.1.2**: Test edge cases and error scenarios
- Duration: 45 minutes
- Test: Invalid image formats
- Test: Very large images
- Test: Blurry images (OCR failure)
- Test: No text in image
- Test: Network errors
- Test: LLM API errors
- Test: Empty messages
- Test: Very long conversations (>20 turns)

**Task 3.1.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.2: Optional Stretch Features (180 minutes)

**Task 3.2.1**: Interactive Whiteboard (Optional - High Value)
- Duration: 90 minutes
- Install: react-sketch-canvas or similar
- Create: `Whiteboard.tsx` component
- Features:
  - Draw diagrams and equations
  - Share with tutor
  - Tutor can draw explanations
- Integrate: Into chat flow
- Test: Whiteboard functionality

**Task 3.2.2**: Step Visualization (Optional - High Value)
- Duration: 60 minutes
- Create: `StepVisualization.tsx` component
- Display: Solution steps animated or sequentially
- Parse: LLM response for steps
- Render: Each step with math rendering
- Test: Step visualization works

**Task 3.2.3**: Voice Interface (Optional - High Value)
- Duration: 30 minutes
- Add: Text-to-speech for tutor responses
- Add: Speech-to-text for student input
- Use: Web Speech API
- Test: Voice interface works

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - LLM Integration (OpenAI/Anthropic)
  - Socratic Method Implementation
  - Image Processing & OCR
  - Math Rendering (KaTeX)
  - Context Management
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Write prompt engineering notes
- Duration: 30 minutes
- Document: Socratic prompt design
- Document: Prompt iterations and improvements
- Document: Techniques for preventing direct answers
- Document: Context management strategies
- Include: Example prompts and responses

**Task 3.3.3**: Update README with complete setup
- Duration: 30 minutes
- Prerequisites (Node.js, npm, API keys)
- Installation steps
- Environment setup
- Running locally
- Testing instructions
- Deployment instructions (if applicable)
- Troubleshooting section

**Task 3.3.4**: Document 5+ example problem walkthroughs
- Duration: 15 minutes
- Document: 5+ complete problem walkthroughs
- Include: Problem statement, conversation flow, final answer
- Show: Socratic method in action
- Show: Context maintained across turns

**Task 3.3.5**: Document AI tools and prompts used
- Duration: 15 minutes
- List: AI tools used (Cursor, Copilot, etc.)
- Document: Key prompts that generated code
- Acknowledge: AI assistance in README

#### Phase 3.4: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.4.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend API code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and context management

**Task 3.4.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (image uploads, user messages)
- Check: Image processing security (file size limits, type validation)
- Review: LLM API usage (rate limiting, error handling)
- Review: Session management and context storage
- Check: CORS configuration (if applicable)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code

#### Phase 3.5: Final Polish (90 minutes)

**Task 3.5.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove: Debug code and console.logs
- Add: JSDoc comments to all functions

**Task 3.5.2**: Performance optimization
- Duration: 30 minutes
- Optimize: Image upload (compression if needed)
- Optimize: Math rendering (cache if possible)
- Optimize: Context management (limit history length)
- Profile: API response times
- Optimize: Re-renders in React

**Task 3.4.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All 5+ problem types tested
- ✅ Socratic method works (no direct answers)
- ✅ Context maintained across turns
- ✅ Math rendering works correctly
- ✅ Image upload and OCR working
- ✅ Conversation history working
- ✅ README complete and accurate
- ✅ Documentation complete
- ✅ Tests passing
- ✅ No API keys in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor - Complete Socratic learning assistant"
  - System Overview (15s): Architecture, key features
  - Problem Input (25s): Text input and image upload, OCR demonstration
  - Socratic Dialogue (60s): Multi-turn conversation, guiding questions, math rendering, no direct answers
  - Conversation History (20s): Past conversations, loading and continuing
  - Stretch Features (20s): Whiteboard/step visualization/voice (if built)
  - Conclusion (10s): "Complete Socratic learning experience with comprehensive features"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all features
- Highlight key technical achievements
- Show Socratic method in action

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Check: Test coverage (aim for >80%)
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Problem input working (text + image upload)
- ✅ OCR/Vision LLM parsing working
- ✅ Basic chat UI functional
- ✅ LLM integration with Socratic prompting
- ✅ Socratic method working (no direct answers)
- ✅ Context management working
- ✅ Multi-turn conversations functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Math rendering with LaTeX/KaTeX working
- ✅ Conversation history implemented
- ✅ UI polish and enhancements complete
- ✅ Integration tests passing
- ✅ Tested on 5+ problem types

### Final Submission (Phase 3) - November 9
- ✅ Comprehensive testing (5+ problem types)
- ✅ Edge cases handled gracefully
- ✅ Documentation complete (technical writeup, prompt notes, README, walkthroughs)
- ✅ Optional stretch features implemented (if time permits)
- ✅ Code clean, linted, and documented
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM gives direct answers | High | Strict Socratic prompt, validation in code, test thoroughly |
| OCR fails on images | Medium | Fallback to text input, clear error messages, test with various images |
| Context too long for LLM | Medium | Limit conversation history (last 10-20 messages), summarize if needed |
| API costs too high | Medium | Cache responses if possible, use cheaper models for iteration |
| Math rendering errors | Low | Validate LaTeX syntax, handle errors gracefully |
| Time constraints | High | MVP is safety net, stretch features are optional, prioritize core Socratic method |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Prompt engineering notes
- [ ] README with complete setup and usage
- [ ] 5+ example problem walkthroughs
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for chat logic
- [ ] Integration tests for full flow
- [ ] Tests for 5+ problem types
- [ ] Edge case tests

### Demo
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployed app or local setup instructions

---

## Notes

- **Socratic Method**: NEVER give direct answers. Guide through questions only.
- **Context Management**: Maintain full conversation history for proper context.
- **Math Rendering**: Use KaTeX for accurate math display.
- **Image Processing**: Use Vision API for best OCR results (or Tesseract.js for local).
- **Testing**: Test with diverse problem types to ensure Socratic method works.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: AI Math Tutor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Socratic Learning Assistant with OCR/Vision LLM Parsing

---

## Executive Summary

AI Math Tutor is an AI-powered educational assistant that guides students through math problems using Socratic questioning. The system accepts problems via screenshot or text input, parses them using OCR/Vision LLM, and maintains multi-turn conversations that guide students to discover solutions without providing direct answers.

**Key Deliverables:**
- ✅ Problem Input: Text entry + image upload with OCR/Vision LLM parsing
- ✅ Socratic Dialogue: Multi-turn conversation with guiding questions
- ✅ Math Rendering: Display equations properly (LaTeX/KaTeX)
- ✅ Web Interface: Clean chat UI with image upload and conversation history
- ✅ Context Management: Maintains conversation context across turns
- ✅ Adaptive Scaffolding: Adjusts to student understanding level

**Success Targets:**
- **MVP (Nov 4)**: Basic chat + LLM integration + Socratic prompting + image parsing
- **Early Submission (Nov 7)**: + Math rendering + conversation history + UI polish
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + optional stretch features

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Basic chat UI, LLM integration, Socratic prompting, image parsing working |
| **Early Submission** | Friday, November 7, 2025 | + Math rendering (LaTeX), conversation history, UI polish |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive testing, documentation, optional stretch features |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Math Rendering**: KaTeX or MathJax
- **Image Upload**: React Dropzone or similar
- **Chat UI**: Custom chat component or react-chat-elements
- **Build Tool**: Vite or Next.js (if SSR needed)

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API routes
- **Language**: TypeScript
- **AI Integration**: OpenAI GPT-4 with Vision API or Anthropic Claude with vision
- **Image Processing**: Sharp (for image optimization)
- **Storage**: SQLite (local) or PostgreSQL (if deployed)
- **Session Management**: In-memory or Redis (if deployed)

### AI Integration
- **LLM**: OpenAI GPT-4 Turbo with Vision or Anthropic Claude 3.5 Sonnet with vision
- **SDK**: OpenAI Node.js SDK or Anthropic SDK
- **Image Processing**: OpenAI Vision API for OCR or Tesseract.js (local OCR)
- **Prompt Engineering**: Structured prompts for Socratic method

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
AI-Math-Tutor/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── Chat.tsx              # Main chat interface
│   │   │   ├── MessageBubble.tsx     # Message display
│   │   │   ├── ImageUpload.tsx       # Image upload component
│   │   │   ├── MathRenderer.tsx      # LaTeX math rendering
│   │   │   └── ConversationHistory.tsx # History sidebar
│   │   ├── hooks/
│   │   │   ├── useChat.ts            # Chat state management
│   │   │   └── useImageUpload.ts     # Image upload logic
│   │   ├── services/
│   │   │   ├── api.ts                # API client
│   │   │   └── mathParser.ts         # Math parsing utilities
│   │   └── App.tsx
│   └── public/
├── backend/
│   ├── api/
│   │   ├── chat.ts                   # Chat endpoint
│   │   ├── image-upload.ts            # Image upload endpoint
│   │   └── health.ts                  # Health check
│   ├── services/
│   │   ├── llm.ts                     # LLM service
│   │   ├── imageProcessor.ts          # Image processing
│   │   ├── socratic.ts                # Socratic method logic
│   │   └── contextManager.ts         # Conversation context
│   ├── prompts/
│   │   └── socratic.ts                # Socratic prompt templates
│   └── server.ts                      # Express server
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── prompt-engineering-notes.md
```

### Data Flow

1. **Problem Input**: User uploads image or types text → Image processed via Vision API → Problem text extracted
2. **Socratic Dialogue**: Problem text → LLM with Socratic prompt → Guiding question → User response → Context updated
3. **Math Rendering**: Mathematical expressions → LaTeX parser → KaTeX renderer → Displayed in UI
4. **Context Management**: Each turn → Context stored → Previous turns retrieved → Full conversation context maintained

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic chat with LLM integration and Socratic prompting working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 20 minutes
- Create `ai-math-tutor/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install dependencies: React, TypeScript, Tailwind CSS
- Verify setup: `npm run dev`
- PowerShell: `New-Item -ItemType Directory -Path ai-math-tutor; Set-Location ai-math-tutor; npm create vite@latest frontend -- --template react-ts`

**Task 1.1.2**: Setup backend with Express
- Duration: 20 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, tsx, OpenAI SDK
- Create `server.ts` with basic Express setup
- Add TypeScript config: `tsconfig.json`
- PowerShell: `New-Item -ItemType Directory -Path backend; Set-Location backend; npm init -y; npm install express typescript tsx @types/express @types/node`

**Task 1.1.3**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files (frontend and backend)
- Add: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries
- Document setup in README

**Task 1.1.4**: Setup testing framework
- Duration: 20 minutes
- Install Vitest: `npm install -D vitest @testing-library/react`
- Create `tests/` directory structure
- Create sample test to verify setup
- Run: `npm test` to verify working

**Task 1.1.5**: Create project structure
- Duration: 15 minutes
- Create directory structure (components, services, hooks)
- Create placeholder files for main components
- Add basic README with setup instructions

#### Phase 1.2: Basic Chat UI (120 minutes)

**Task 1.2.1**: Create chat interface component
- Duration: 45 minutes
- Create `frontend/src/components/Chat.tsx`
- Basic chat UI with:
  - Message input area
  - Send button
  - Message list area
  - Simple styling with Tailwind
- Add state management for messages

**Task 1.2.2**: Create message bubble component
- Duration: 30 minutes
- Create `frontend/src/components/MessageBubble.tsx`
- Display: User messages (right-aligned), Tutor messages (left-aligned)
- Add basic styling and animations
- Show timestamps

**Task 1.2.3**: Create chat hook for state management
- Duration: 30 minutes
- Create `frontend/src/hooks/useChat.ts`
- Manage: Messages array, current input, loading state
- Functions: `sendMessage()`, `addMessage()`, `clearChat()`
- Return: Messages, loading, sendMessage function

**Task 1.2.4**: Create API client
- Duration: 15 minutes
- Create `frontend/src/services/api.ts`
- Function: `sendMessage(message: string) => Promise<string>`
- POST to `/api/chat` endpoint
- Handle errors and loading states

#### Phase 1.3: Backend API - Chat Endpoint (120 minutes)

**Task 1.3.1**: Create Express server with chat endpoint
- Duration: 30 minutes
- Create `backend/server.ts`
- Setup Express with CORS middleware
- Create `/api/chat` POST endpoint
- Receive: `{ message: string, conversationHistory?: Message[] }`
- Return: `{ response: string }`

**Task 1.3.2**: Integrate OpenAI SDK
- Duration: 30 minutes
- Create `backend/services/llm.ts`
- Setup OpenAI client with API key
- Function: `generateResponse(messages: Message[]) => Promise<string>`
- Call `openai.chat.completions.create()` with messages
- Return assistant message

**Task 1.3.3**: Create Socratic prompt template
- Duration: 30 minutes
- Create `backend/prompts/socratic.ts`
- System prompt: "You are a patient math tutor. NEVER give direct answers. Guide through questions: 'What information do we have?' 'What method might help?' If stuck >2 turns, provide concrete hint. Use encouraging language."
- Format: System message + conversation history
- Add prompt validation

**Task 1.3.4**: Implement Socratic logic
- Duration: 30 minutes
- Create `backend/services/socratic.ts`
- Function: `buildPrompt(conversationHistory: Message[]) => Message[]`
- Logic:
  - If first message: Parse problem → Inventory knowns → Identify goal
  - If subsequent: Guide method selection → Step through solution → Validate answer
- Add validation for Socratic method (no direct answers)

#### Phase 1.4: Image Upload & OCR/Vision LLM (150 minutes)

**Task 1.4.1**: Create image upload component
- Duration: 45 minutes
- Create `frontend/src/components/ImageUpload.tsx`
- Use React Dropzone or similar
- Features: Drag & drop, click to upload, preview image
- Accept: PNG, JPG, JPEG formats
- Display upload progress

**Task 1.4.2**: Create image upload API endpoint
- Duration: 30 minutes
- Create `/api/image-upload` POST endpoint
- Use Multer or similar for file handling
- Save uploaded image temporarily
- Return: Image URL or base64 data

**Task 1.4.3**: Integrate Vision API for OCR
- Duration: 45 minutes
- Create `backend/services/imageProcessor.ts`
- Function: `extractTextFromImage(imagePath: string) => Promise<string>`
- Option A: OpenAI Vision API:
  - Use `openai.chat.completions.create()` with vision model
  - Prompt: "Extract all mathematical expressions and text from this image. Return the problem statement."
  - Parse response for problem text
- Option B: Tesseract.js (local OCR):
  - Install Tesseract.js
  - Process image locally
  - Extract text (less accurate for math)

**Task 1.4.4**: Integrate image upload into chat flow
- Duration: 30 minutes
- Modify chat UI to include image upload
- When image uploaded: Extract text → Send to LLM as first message
- Handle: Image processing → Text extraction → Problem parsing → Chat start
- Add loading state for image processing

#### Phase 1.5: Conversation Context Management (90 minutes)

**Task 1.5.1**: Implement context manager
- Duration: 45 minutes
- Create `backend/services/contextManager.ts`
- Function: `buildContext(userId: string, conversationId: string) => Message[]`
- Store: Messages in-memory (Map<conversationId, Message[]>)
- Or: Use SQLite for persistence
- Retrieve: Full conversation history for context

**Task 1.5.2**: Add context to chat endpoint
- Duration: 30 minutes
- Modify `/api/chat` endpoint to accept `conversationId`
- Retrieve conversation history from context manager
- Pass full history to LLM (last 10-20 messages for token efficiency)
- Update context after each message

**Task 1.5.3**: Test context management
- Duration: 15 minutes
- Test: Multi-turn conversation maintains context
- Test: LLM references previous messages
- Test: Context persists across API calls
- Verify: Conversation history is accurate

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Write unit tests for chat logic
- Duration: 45 minutes
- Test: Message sending and receiving
- Test: Context management (add/retrieve messages)
- Test: Socratic prompt building
- Test: Image text extraction (mock)
- Run: `npm test` to verify

**Task 1.6.2**: Test with hardcoded problems
- Duration: 30 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problem: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
- Verify: Socratic method works (no direct answers)
- Verify: Context maintained across turns

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All unit tests pass
- Verify: Chat flow works end-to-end
- Document any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor MVP - Socratic learning assistant"
  - Problem Input (30s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (60s): Show multi-turn conversation, demonstrate guiding questions (no direct answers)
  - Context Management (30s): Show how tutor references previous messages
  - Conclusion (10s): "MVP demonstrates core Socratic method with image parsing and context management"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for chat interface
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add math rendering, conversation history, UI polish

#### Phase 2.1: Math Rendering with LaTeX/KaTeX (120 minutes)

**Task 2.1.1**: Install and configure KaTeX
- Duration: 20 minutes
- Install: `npm install katex react-katex`
- Add KaTeX CSS to main CSS file
- Create `frontend/src/components/MathRenderer.tsx`
- Function: `renderMath(text: string) => JSX.Element`
- Support: Inline math `$...$` and display math `$$...$$`

**Task 2.1.2**: Parse mathematical expressions from messages
- Duration: 45 minutes
- Create `frontend/src/services/mathParser.ts`
- Function: `parseMath(text: string) => string`
- Extract: LaTeX expressions from text
- Handle: Inline math and display math
- Preserve: Non-math text

**Task 2.1.3**: Integrate math rendering into message bubbles
- Duration: 35 minutes
- Modify `MessageBubble.tsx` to render math
- Parse message text for math expressions
- Render math with KaTeX component
- Handle: Error cases (invalid LaTeX)
- Style: Math expressions appropriately

**Task 2.1.4**: Test math rendering
- Duration: 20 minutes
- Test: Simple equations (`x + 5 = 13`)
- Test: Complex equations (`x² + 5x + 6 = 0`)
- Test: Fractions, exponents, integrals
- Test: Mixed text and math
- Verify: All expressions render correctly

#### Phase 2.2: Conversation History (90 minutes)

**Task 2.2.1**: Create conversation history UI
- Duration: 45 minutes
- Create `frontend/src/components/ConversationHistory.tsx`
- Display: List of past conversations
- Features:
  - Conversation titles (first problem statement)
  - Timestamp
  - Click to load conversation
  - Delete conversation
- Sidebar or dropdown menu

**Task 2.2.2**: Persist conversations in backend
- Duration: 30 minutes
- Create SQLite database for conversations
- Schema: `conversations(id, userId, title, createdAt)`, `messages(conversationId, role, content, timestamp)`
- Store: Full conversation history
- Retrieve: Conversations by user

**Task 2.2.3**: Load conversation from history
- Duration: 15 minutes
- Add API endpoint: `GET /api/conversations/:id`
- Load conversation messages into chat
- Update context manager with loaded conversation
- Test: Loading conversation works correctly

#### Phase 2.3: UI Polish & Enhancements (120 minutes)

**Task 2.3.1**: Improve chat UI design
- Duration: 45 minutes
- Enhance: Message bubble styling
- Add: Typing indicator
- Add: Loading states for LLM responses
- Add: Smooth scroll to latest message
- Add: Dark mode (optional)

**Task 2.3.2**: Add error handling and user feedback
- Duration: 30 minutes
- Display: Error messages for failed API calls
- Add: Retry button for failed requests
- Show: Network errors clearly
- Add: Input validation (empty messages, file size limits)

**Task 2.3.3**: Optimize image upload UX
- Duration: 30 minutes
- Add: Image preview before upload
- Add: Image compression (if needed)
- Add: Progress bar for upload
- Add: Error handling for unsupported formats
- Add: Maximum file size validation

**Task 2.3.4**: Add keyboard shortcuts
- Duration: 15 minutes
- Add: Enter to send message
- Add: Shift+Enter for new line
- Add: Escape to clear input
- Test: Keyboard shortcuts work correctly

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Test math rendering with various problems
- Duration: 30 minutes
- Test: 5+ problem types (arithmetic, algebra, geometry, word problems, multi-step)
- Verify: All math expressions render correctly
- Verify: LaTeX syntax is handled properly
- Document: Any rendering issues

**Task 2.4.2**: Test conversation history
- Duration: 30 minutes
- Test: Saving conversations
- Test: Loading conversations
- Test: Deleting conversations
- Test: Multiple conversations per user
- Verify: Context maintained when loading

**Task 2.4.3**: Integration tests for full flow
- Duration: 30 minutes
- Test: Upload image → Extract text → Start chat → Socratic dialogue → Math rendering
- Test: Load conversation → Continue chat → Context maintained
- Test: Error scenarios (API failures, invalid images)
- Verify: All features work together

**Task 2.4.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor Early Submission - now with math rendering and history"
  - Problem Input (25s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (50s): Show multi-turn conversation, demonstrate guiding questions, show math rendering
  - Conversation History (30s): Show past conversations, load conversation, continue dialogue
  - UI Polish (25s): Show improved UI, error handling, keyboard shortcuts
  - Conclusion (10s): "Demonstrates complete Socratic learning experience with math rendering"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for improved interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optional stretch features

#### Phase 3.1: Comprehensive Testing (120 minutes)

**Task 3.1.1**: Test all 5+ problem types
- Duration: 60 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problems: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
  6. Fractions: "1/2 + 1/3 = ?"
  7. Exponents: "2³ × 2² = ?"
- Verify: Socratic method works for all types
- Verify: No direct answers given
- Verify: Context maintained across turns

**Task 3.1.2**: Test edge cases and error scenarios
- Duration: 45 minutes
- Test: Invalid image formats
- Test: Very large images
- Test: Blurry images (OCR failure)
- Test: No text in image
- Test: Network errors
- Test: LLM API errors
- Test: Empty messages
- Test: Very long conversations (>20 turns)

**Task 3.1.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.2: Optional Stretch Features (180 minutes)

**Task 3.2.1**: Interactive Whiteboard (Optional - High Value)
- Duration: 90 minutes
- Install: react-sketch-canvas or similar
- Create: `Whiteboard.tsx` component
- Features:
  - Draw diagrams and equations
  - Share with tutor
  - Tutor can draw explanations
- Integrate: Into chat flow
- Test: Whiteboard functionality

**Task 3.2.2**: Step Visualization (Optional - High Value)
- Duration: 60 minutes
- Create: `StepVisualization.tsx` component
- Display: Solution steps animated or sequentially
- Parse: LLM response for steps
- Render: Each step with math rendering
- Test: Step visualization works

**Task 3.2.3**: Voice Interface (Optional - High Value)
- Duration: 30 minutes
- Add: Text-to-speech for tutor responses
- Add: Speech-to-text for student input
- Use: Web Speech API
- Test: Voice interface works

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - LLM Integration (OpenAI/Anthropic)
  - Socratic Method Implementation
  - Image Processing & OCR
  - Math Rendering (KaTeX)
  - Context Management
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Write prompt engineering notes
- Duration: 30 minutes
- Document: Socratic prompt design
- Document: Prompt iterations and improvements
- Document: Techniques for preventing direct answers
- Document: Context management strategies
- Include: Example prompts and responses

**Task 3.3.3**: Update README with complete setup
- Duration: 30 minutes
- Prerequisites (Node.js, npm, API keys)
- Installation steps
- Environment setup
- Running locally
- Testing instructions
- Deployment instructions (if applicable)
- Troubleshooting section

**Task 3.3.4**: Document 5+ example problem walkthroughs
- Duration: 15 minutes
- Document: 5+ complete problem walkthroughs
- Include: Problem statement, conversation flow, final answer
- Show: Socratic method in action
- Show: Context maintained across turns

**Task 3.3.5**: Document AI tools and prompts used
- Duration: 15 minutes
- List: AI tools used (Cursor, Copilot, etc.)
- Document: Key prompts that generated code
- Acknowledge: AI assistance in README

#### Phase 3.4: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.4.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend API code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and context management

**Task 3.4.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (image uploads, user messages)
- Check: Image processing security (file size limits, type validation)
- Review: LLM API usage (rate limiting, error handling)
- Review: Session management and context storage
- Check: CORS configuration (if applicable)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code

#### Phase 3.5: Final Polish (90 minutes)

**Task 3.5.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove: Debug code and console.logs
- Add: JSDoc comments to all functions

**Task 3.5.2**: Performance optimization
- Duration: 30 minutes
- Optimize: Image upload (compression if needed)
- Optimize: Math rendering (cache if possible)
- Optimize: Context management (limit history length)
- Profile: API response times
- Optimize: Re-renders in React

**Task 3.5.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All 5+ problem types tested
- ✅ Socratic method works (no direct answers)
- ✅ Context maintained across turns
- ✅ Math rendering works correctly
- ✅ Image upload and OCR working
- ✅ Conversation history working
- ✅ README complete and accurate
- ✅ Documentation complete
- ✅ Tests passing
- ✅ No API keys in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor - Complete Socratic learning assistant"
  - System Overview (15s): Architecture, key features
  - Problem Input (25s): Text input and image upload, OCR demonstration
  - Socratic Dialogue (60s): Multi-turn conversation, guiding questions, math rendering, no direct answers
  - Conversation History (20s): Past conversations, loading and continuing
  - Stretch Features (20s): Whiteboard/step visualization/voice (if built)
  - Conclusion (10s): "Complete Socratic learning experience with comprehensive features"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all features
- Highlight key technical achievements
- Show Socratic method in action

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Check: Test coverage (aim for >80%)
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Problem input working (text + image upload)
- ✅ OCR/Vision LLM parsing working
- ✅ Basic chat UI functional
- ✅ LLM integration with Socratic prompting
- ✅ Socratic method working (no direct answers)
- ✅ Context management working
- ✅ Multi-turn conversations functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Math rendering with LaTeX/KaTeX working
- ✅ Conversation history implemented
- ✅ UI polish and enhancements complete
- ✅ Integration tests passing
- ✅ Tested on 5+ problem types

### Final Submission (Phase 3) - November 9
- ✅ Comprehensive testing (5+ problem types)
- ✅ Edge cases handled gracefully
- ✅ Documentation complete (technical writeup, prompt notes, README, walkthroughs)
- ✅ Optional stretch features implemented (if time permits)
- ✅ Code clean, linted, and documented
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM gives direct answers | High | Strict Socratic prompt, validation in code, test thoroughly |
| OCR fails on images | Medium | Fallback to text input, clear error messages, test with various images |
| Context too long for LLM | Medium | Limit conversation history (last 10-20 messages), summarize if needed |
| API costs too high | Medium | Cache responses if possible, use cheaper models for iteration |
| Math rendering errors | Low | Validate LaTeX syntax, handle errors gracefully |
| Time constraints | High | MVP is safety net, stretch features are optional, prioritize core Socratic method |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Prompt engineering notes
- [ ] README with complete setup and usage
- [ ] 5+ example problem walkthroughs
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for chat logic
- [ ] Integration tests for full flow
- [ ] Tests for 5+ problem types
- [ ] Edge case tests

### Demo
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployed app or local setup instructions

---

## Notes

- **Socratic Method**: NEVER give direct answers. Guide through questions only.
- **Context Management**: Maintain full conversation history for proper context.
- **Math Rendering**: Use KaTeX for accurate math display.
- **Image Processing**: Use Vision API for best OCR results (or Tesseract.js for local).
- **Testing**: Test with diverse problem types to ensure Socratic method works.

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: AI Math Tutor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Socratic Learning Assistant with OCR/Vision LLM Parsing

---

## Executive Summary

AI Math Tutor is an AI-powered educational assistant that guides students through math problems using Socratic questioning. The system accepts problems via screenshot or text input, parses them using OCR/Vision LLM, and maintains multi-turn conversations that guide students to discover solutions without providing direct answers.

**Key Deliverables:**
- ✅ Problem Input: Text entry + image upload with OCR/Vision LLM parsing
- ✅ Socratic Dialogue: Multi-turn conversation with guiding questions
- ✅ Math Rendering: Display equations properly (LaTeX/KaTeX)
- ✅ Web Interface: Clean chat UI with image upload and conversation history
- ✅ Context Management: Maintains conversation context across turns
- ✅ Adaptive Scaffolding: Adjusts to student understanding level

**Success Targets:**
- **MVP (Nov 4)**: Basic chat + LLM integration + Socratic prompting + image parsing
- **Early Submission (Nov 7)**: + Math rendering + conversation history + UI polish
- **Final Submission (Nov 9)**: + Comprehensive testing + documentation + optional stretch features

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Basic chat UI, LLM integration, Socratic prompting, image parsing working |
| **Early Submission** | Friday, November 7, 2025 | + Math rendering (LaTeX), conversation history, UI polish |
| **Final Submission** | Sunday, November 9, 2025 | + Comprehensive testing, documentation, optional stretch features |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Math Rendering**: KaTeX or MathJax
- **Image Upload**: React Dropzone or similar
- **Chat UI**: Custom chat component or react-chat-elements
- **Build Tool**: Vite or Next.js (if SSR needed)

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API routes
- **Language**: TypeScript
- **AI Integration**: OpenAI GPT-4 with Vision API or Anthropic Claude with vision
- **Image Processing**: Sharp (for image optimization)
- **Storage**: SQLite (local) or PostgreSQL (if deployed)
- **Session Management**: In-memory or Redis (if deployed)

### AI Integration
- **LLM**: OpenAI GPT-4 Turbo with Vision or Anthropic Claude 3.5 Sonnet with vision
- **SDK**: OpenAI Node.js SDK or Anthropic SDK
- **Image Processing**: OpenAI Vision API for OCR or Tesseract.js (local OCR)
- **Prompt Engineering**: Structured prompts for Socratic method

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
AI-Math-Tutor/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── Chat.tsx              # Main chat interface
│   │   │   ├── MessageBubble.tsx     # Message display
│   │   │   ├── ImageUpload.tsx       # Image upload component
│   │   │   ├── MathRenderer.tsx      # LaTeX math rendering
│   │   │   └── ConversationHistory.tsx # History sidebar
│   │   ├── hooks/
│   │   │   ├── useChat.ts            # Chat state management
│   │   │   └── useImageUpload.ts     # Image upload logic
│   │   ├── services/
│   │   │   ├── api.ts                # API client
│   │   │   └── mathParser.ts         # Math parsing utilities
│   │   └── App.tsx
│   └── public/
├── backend/
│   ├── api/
│   │   ├── chat.ts                   # Chat endpoint
│   │   ├── image-upload.ts            # Image upload endpoint
│   │   └── health.ts                  # Health check
│   ├── services/
│   │   ├── llm.ts                     # LLM service
│   │   ├── imageProcessor.ts          # Image processing
│   │   ├── socratic.ts                # Socratic method logic
│   │   └── contextManager.ts         # Conversation context
│   ├── prompts/
│   │   └── socratic.ts                # Socratic prompt templates
│   └── server.ts                      # Express server
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    └── prompt-engineering-notes.md
```

### Data Flow

1. **Problem Input**: User uploads image or types text → Image processed via Vision API → Problem text extracted
2. **Socratic Dialogue**: Problem text → LLM with Socratic prompt → Guiding question → User response → Context updated
3. **Math Rendering**: Mathematical expressions → LaTeX parser → KaTeX renderer → Displayed in UI
4. **Context Management**: Each turn → Context stored → Previous turns retrieved → Full conversation context maintained

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic chat with LLM integration and Socratic prompting working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 20 minutes
- Create `ai-math-tutor/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install dependencies: React, TypeScript, Tailwind CSS
- Verify setup: `npm run dev`
- PowerShell: `New-Item -ItemType Directory -Path ai-math-tutor; Set-Location ai-math-tutor; npm create vite@latest frontend -- --template react-ts`

**Task 1.1.2**: Setup backend with Express
- Duration: 20 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, tsx, OpenAI SDK
- Create `server.ts` with basic Express setup
- Add TypeScript config: `tsconfig.json`
- PowerShell: `New-Item -ItemType Directory -Path backend; Set-Location backend; npm init -y; npm install express typescript tsx @types/express @types/node`

**Task 1.1.3**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files (frontend and backend)
- Add: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries
- Document setup in README

**Task 1.1.4**: Setup testing framework
- Duration: 20 minutes
- Install Vitest: `npm install -D vitest @testing-library/react`
- Create `tests/` directory structure
- Create sample test to verify setup
- Run: `npm test` to verify working

**Task 1.1.5**: Create project structure
- Duration: 15 minutes
- Create directory structure (components, services, hooks)
- Create placeholder files for main components
- Add basic README with setup instructions

#### Phase 1.2: Basic Chat UI (120 minutes)

**Task 1.2.1**: Create chat interface component
- Duration: 45 minutes
- Create `frontend/src/components/Chat.tsx`
- Basic chat UI with:
  - Message input area
  - Send button
  - Message list area
  - Simple styling with Tailwind
- Add state management for messages

**Task 1.2.2**: Create message bubble component
- Duration: 30 minutes
- Create `frontend/src/components/MessageBubble.tsx`
- Display: User messages (right-aligned), Tutor messages (left-aligned)
- Add basic styling and animations
- Show timestamps

**Task 1.2.3**: Create chat hook for state management
- Duration: 30 minutes
- Create `frontend/src/hooks/useChat.ts`
- Manage: Messages array, current input, loading state
- Functions: `sendMessage()`, `addMessage()`, `clearChat()`
- Return: Messages, loading, sendMessage function

**Task 1.2.4**: Create API client
- Duration: 15 minutes
- Create `frontend/src/services/api.ts`
- Function: `sendMessage(message: string) => Promise<string>`
- POST to `/api/chat` endpoint
- Handle errors and loading states

#### Phase 1.3: Backend API - Chat Endpoint (120 minutes)

**Task 1.3.1**: Create Express server with chat endpoint
- Duration: 30 minutes
- Create `backend/server.ts`
- Setup Express with CORS middleware
- Create `/api/chat` POST endpoint
- Receive: `{ message: string, conversationHistory?: Message[] }`
- Return: `{ response: string }`

**Task 1.3.2**: Integrate OpenAI SDK
- Duration: 30 minutes
- Create `backend/services/llm.ts`
- Setup OpenAI client with API key
- Function: `generateResponse(messages: Message[]) => Promise<string>`
- Call `openai.chat.completions.create()` with messages
- Return assistant message

**Task 1.3.3**: Create Socratic prompt template
- Duration: 30 minutes
- Create `backend/prompts/socratic.ts`
- System prompt: "You are a patient math tutor. NEVER give direct answers. Guide through questions: 'What information do we have?' 'What method might help?' If stuck >2 turns, provide concrete hint. Use encouraging language."
- Format: System message + conversation history
- Add prompt validation

**Task 1.3.4**: Implement Socratic logic
- Duration: 30 minutes
- Create `backend/services/socratic.ts`
- Function: `buildPrompt(conversationHistory: Message[]) => Message[]`
- Logic:
  - If first message: Parse problem → Inventory knowns → Identify goal
  - If subsequent: Guide method selection → Step through solution → Validate answer
- Add validation for Socratic method (no direct answers)

#### Phase 1.4: Image Upload & OCR/Vision LLM (150 minutes)

**Task 1.4.1**: Create image upload component
- Duration: 45 minutes
- Create `frontend/src/components/ImageUpload.tsx`
- Use React Dropzone or similar
- Features: Drag & drop, click to upload, preview image
- Accept: PNG, JPG, JPEG formats
- Display upload progress

**Task 1.4.2**: Create image upload API endpoint
- Duration: 30 minutes
- Create `/api/image-upload` POST endpoint
- Use Multer or similar for file handling
- Save uploaded image temporarily
- Return: Image URL or base64 data

**Task 1.4.3**: Integrate Vision API for OCR
- Duration: 45 minutes
- Create `backend/services/imageProcessor.ts`
- Function: `extractTextFromImage(imagePath: string) => Promise<string>`
- Option A: OpenAI Vision API:
  - Use `openai.chat.completions.create()` with vision model
  - Prompt: "Extract all mathematical expressions and text from this image. Return the problem statement."
  - Parse response for problem text
- Option B: Tesseract.js (local OCR):
  - Install Tesseract.js
  - Process image locally
  - Extract text (less accurate for math)

**Task 1.4.4**: Integrate image upload into chat flow
- Duration: 30 minutes
- Modify chat UI to include image upload
- When image uploaded: Extract text → Send to LLM as first message
- Handle: Image processing → Text extraction → Problem parsing → Chat start
- Add loading state for image processing

#### Phase 1.5: Conversation Context Management (90 minutes)

**Task 1.5.1**: Implement context manager
- Duration: 45 minutes
- Create `backend/services/contextManager.ts`
- Function: `buildContext(userId: string, conversationId: string) => Message[]`
- Store: Messages in-memory (Map<conversationId, Message[]>)
- Or: Use SQLite for persistence
- Retrieve: Full conversation history for context

**Task 1.5.2**: Add context to chat endpoint
- Duration: 30 minutes
- Modify `/api/chat` endpoint to accept `conversationId`
- Retrieve conversation history from context manager
- Pass full history to LLM (last 10-20 messages for token efficiency)
- Update context after each message

**Task 1.5.3**: Test context management
- Duration: 15 minutes
- Test: Multi-turn conversation maintains context
- Test: LLM references previous messages
- Test: Context persists across API calls
- Verify: Conversation history is accurate

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Write unit tests for chat logic
- Duration: 45 minutes
- Test: Message sending and receiving
- Test: Context management (add/retrieve messages)
- Test: Socratic prompt building
- Test: Image text extraction (mock)
- Run: `npm test` to verify

**Task 1.6.2**: Test with hardcoded problems
- Duration: 30 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problem: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
- Verify: Socratic method works (no direct answers)
- Verify: Context maintained across turns

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All unit tests pass
- Verify: Chat flow works end-to-end
- Document any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor MVP - Socratic learning assistant"
  - Problem Input (30s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (60s): Show multi-turn conversation, demonstrate guiding questions (no direct answers)
  - Context Management (30s): Show how tutor references previous messages
  - Conclusion (10s): "MVP demonstrates core Socratic method with image parsing and context management"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for chat interface
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add math rendering, conversation history, UI polish

#### Phase 2.1: Math Rendering with LaTeX/KaTeX (120 minutes)

**Task 2.1.1**: Install and configure KaTeX
- Duration: 20 minutes
- Install: `npm install katex react-katex`
- Add KaTeX CSS to main CSS file
- Create `frontend/src/components/MathRenderer.tsx`
- Function: `renderMath(text: string) => JSX.Element`
- Support: Inline math `$...$` and display math `$$...$$`

**Task 2.1.2**: Parse mathematical expressions from messages
- Duration: 45 minutes
- Create `frontend/src/services/mathParser.ts`
- Function: `parseMath(text: string) => string`
- Extract: LaTeX expressions from text
- Handle: Inline math and display math
- Preserve: Non-math text

**Task 2.1.3**: Integrate math rendering into message bubbles
- Duration: 35 minutes
- Modify `MessageBubble.tsx` to render math
- Parse message text for math expressions
- Render math with KaTeX component
- Handle: Error cases (invalid LaTeX)
- Style: Math expressions appropriately

**Task 2.1.4**: Test math rendering
- Duration: 20 minutes
- Test: Simple equations (`x + 5 = 13`)
- Test: Complex equations (`x² + 5x + 6 = 0`)
- Test: Fractions, exponents, integrals
- Test: Mixed text and math
- Verify: All expressions render correctly

#### Phase 2.2: Conversation History (90 minutes)

**Task 2.2.1**: Create conversation history UI
- Duration: 45 minutes
- Create `frontend/src/components/ConversationHistory.tsx`
- Display: List of past conversations
- Features:
  - Conversation titles (first problem statement)
  - Timestamp
  - Click to load conversation
  - Delete conversation
- Sidebar or dropdown menu

**Task 2.2.2**: Persist conversations in backend
- Duration: 30 minutes
- Create SQLite database for conversations
- Schema: `conversations(id, userId, title, createdAt)`, `messages(conversationId, role, content, timestamp)`
- Store: Full conversation history
- Retrieve: Conversations by user

**Task 2.2.3**: Load conversation from history
- Duration: 15 minutes
- Add API endpoint: `GET /api/conversations/:id`
- Load conversation messages into chat
- Update context manager with loaded conversation
- Test: Loading conversation works correctly

#### Phase 2.3: UI Polish & Enhancements (120 minutes)

**Task 2.3.1**: Improve chat UI design
- Duration: 45 minutes
- Enhance: Message bubble styling
- Add: Typing indicator
- Add: Loading states for LLM responses
- Add: Smooth scroll to latest message
- Add: Dark mode (optional)

**Task 2.3.2**: Add error handling and user feedback
- Duration: 30 minutes
- Display: Error messages for failed API calls
- Add: Retry button for failed requests
- Show: Network errors clearly
- Add: Input validation (empty messages, file size limits)

**Task 2.3.3**: Optimize image upload UX
- Duration: 30 minutes
- Add: Image preview before upload
- Add: Image compression (if needed)
- Add: Progress bar for upload
- Add: Error handling for unsupported formats
- Add: Maximum file size validation

**Task 2.3.4**: Add keyboard shortcuts
- Duration: 15 minutes
- Add: Enter to send message
- Add: Shift+Enter for new line
- Add: Escape to clear input
- Test: Keyboard shortcuts work correctly

#### Phase 2.4: Enhanced Testing (90 minutes)

**Task 2.4.1**: Test math rendering with various problems
- Duration: 30 minutes
- Test: 5+ problem types (arithmetic, algebra, geometry, word problems, multi-step)
- Verify: All math expressions render correctly
- Verify: LaTeX syntax is handled properly
- Document: Any rendering issues

**Task 2.4.2**: Test conversation history
- Duration: 30 minutes
- Test: Saving conversations
- Test: Loading conversations
- Test: Deleting conversations
- Test: Multiple conversations per user
- Verify: Context maintained when loading

**Task 2.4.3**: Integration tests for full flow
- Duration: 30 minutes
- Test: Upload image → Extract text → Start chat → Socratic dialogue → Math rendering
- Test: Load conversation → Continue chat → Context maintained
- Test: Error scenarios (API failures, invalid images)
- Verify: All features work together

**Task 2.4.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.5: Early Submission Demo Video Preparation (45 minutes)

**Task 2.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor Early Submission - now with math rendering and history"
  - Problem Input (25s): Show text input and image upload, demonstrate OCR
  - Socratic Dialogue (50s): Show multi-turn conversation, demonstrate guiding questions, show math rendering
  - Conversation History (30s): Show past conversations, load conversation, continue dialogue
  - UI Polish (25s): Show improved UI, error handling, keyboard shortcuts
  - Conclusion (10s): "Demonstrates complete Socratic learning experience with math rendering"

**Task 2.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for improved interface
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, documentation, optional stretch features

#### Phase 3.1: Comprehensive Testing (120 minutes)

**Task 3.1.1**: Test all 5+ problem types
- Duration: 60 minutes
- Test problems:
  1. Simple arithmetic: "2x + 5 = 13"
  2. Algebra: "x² + 5x + 6 = 0"
  3. Geometry: "Find the area of a circle with radius 5"
  4. Word problems: "A train travels 60 mph for 3 hours..."
  5. Multi-step: "Solve for x: 2x + 3 = 4x - 1"
  6. Fractions: "1/2 + 1/3 = ?"
  7. Exponents: "2³ × 2² = ?"
- Verify: Socratic method works for all types
- Verify: No direct answers given
- Verify: Context maintained across turns

**Task 3.1.2**: Test edge cases and error scenarios
- Duration: 45 minutes
- Test: Invalid image formats
- Test: Very large images
- Test: Blurry images (OCR failure)
- Test: No text in image
- Test: Network errors
- Test: LLM API errors
- Test: Empty messages
- Test: Very long conversations (>20 turns)

**Task 3.1.3**: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Check: Test coverage (aim for >80%)
- Document: Test results summary

#### Phase 3.2: Optional Stretch Features (180 minutes)

**Task 3.2.1**: Interactive Whiteboard (Optional - High Value)
- Duration: 90 minutes
- Install: react-sketch-canvas or similar
- Create: `Whiteboard.tsx` component
- Features:
  - Draw diagrams and equations
  - Share with tutor
  - Tutor can draw explanations
- Integrate: Into chat flow
- Test: Whiteboard functionality

**Task 3.2.2**: Step Visualization (Optional - High Value)
- Duration: 60 minutes
- Create: `StepVisualization.tsx` component
- Display: Solution steps animated or sequentially
- Parse: LLM response for steps
- Render: Each step with math rendering
- Test: Step visualization works

**Task 3.2.3**: Voice Interface (Optional - High Value)
- Duration: 30 minutes
- Add: Text-to-speech for tutor responses
- Add: Speech-to-text for student input
- Use: Web Speech API
- Test: Voice interface works

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - LLM Integration (OpenAI/Anthropic)
  - Socratic Method Implementation
  - Image Processing & OCR
  - Math Rendering (KaTeX)
  - Context Management
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Write prompt engineering notes
- Duration: 30 minutes
- Document: Socratic prompt design
- Document: Prompt iterations and improvements
- Document: Techniques for preventing direct answers
- Document: Context management strategies
- Include: Example prompts and responses

**Task 3.3.3**: Update README with complete setup
- Duration: 30 minutes
- Prerequisites (Node.js, npm, API keys)
- Installation steps
- Environment setup
- Running locally
- Testing instructions
- Deployment instructions (if applicable)
- Troubleshooting section

**Task 3.3.4**: Document 5+ example problem walkthroughs
- Duration: 15 minutes
- Document: 5+ complete problem walkthroughs
- Include: Problem statement, conversation flow, final answer
- Show: Socratic method in action
- Show: Context maintained across turns

**Task 3.3.5**: Document AI tools and prompts used
- Duration: 15 minutes
- List: AI tools used (Cursor, Copilot, etc.)
- Document: Key prompts that generated code
- Acknowledge: AI assistance in README

#### Phase 3.4: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.4.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend API code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and context management

**Task 3.4.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (image uploads, user messages)
- Check: Image processing security (file size limits, type validation)
- Review: LLM API usage (rate limiting, error handling)
- Review: Session management and context storage
- Check: CORS configuration (if applicable)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code

#### Phase 3.5: Final Polish (90 minutes)

**Task 3.5.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove: Debug code and console.logs
- Add: JSDoc comments to all functions

**Task 3.5.2**: Performance optimization
- Duration: 30 minutes
- Optimize: Image upload (compression if needed)
- Optimize: Math rendering (cache if possible)
- Optimize: Context management (limit history length)
- Profile: API response times
- Optimize: Re-renders in React

**Task 3.5.3**: Final verification checklist
- Duration: 30 minutes
- ✅ All 5+ problem types tested
- ✅ Socratic method works (no direct answers)
- ✅ Context maintained across turns
- ✅ Math rendering works correctly
- ✅ Image upload and OCR working
- ✅ Conversation history working
- ✅ README complete and accurate
- ✅ Documentation complete
- ✅ Tests passing
- ✅ No API keys in code or commits

#### Phase 3.5: Final Submission Demo Video Preparation (45 minutes)

**Task 3.5.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "AI Math Tutor - Complete Socratic learning assistant"
  - System Overview (15s): Architecture, key features
  - Problem Input (25s): Text input and image upload, OCR demonstration
  - Socratic Dialogue (60s): Multi-turn conversation, guiding questions, math rendering, no direct answers
  - Conversation History (20s): Past conversations, loading and continuing
  - Stretch Features (20s): Whiteboard/step visualization/voice (if built)
  - Conclusion (10s): "Complete Socratic learning experience with comprehensive features"

**Task 3.5.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all features
- Highlight key technical achievements
- Show Socratic method in action

#### Phase 3.6: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Check: Test coverage (aim for >80%)
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Problem input working (text + image upload)
- ✅ OCR/Vision LLM parsing working
- ✅ Basic chat UI functional
- ✅ LLM integration with Socratic prompting
- ✅ Socratic method working (no direct answers)
- ✅ Context management working
- ✅ Multi-turn conversations functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Math rendering with LaTeX/KaTeX working
- ✅ Conversation history implemented
- ✅ UI polish and enhancements complete
- ✅ Integration tests passing
- ✅ Tested on 5+ problem types

### Final Submission (Phase 3) - November 9
- ✅ Comprehensive testing (5+ problem types)
- ✅ Edge cases handled gracefully
- ✅ Documentation complete (technical writeup, prompt notes, README, walkthroughs)
- ✅ Optional stretch features implemented (if time permits)
- ✅ Code clean, linted, and documented
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM gives direct answers | High | Strict Socratic prompt, validation in code, test thoroughly |
| OCR fails on images | Medium | Fallback to text input, clear error messages, test with various images |
| Context too long for LLM | Medium | Limit conversation history (last 10-20 messages), summarize if needed |
| API costs too high | Medium | Cache responses if possible, use cheaper models for iteration |
| Math rendering errors | Low | Validate LaTeX syntax, handle errors gracefully |
| Time constraints | High | MVP is safety net, stretch features are optional, prioritize core Socratic method |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Prompt engineering notes
- [ ] README with complete setup and usage
- [ ] 5+ example problem walkthroughs
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for chat logic
- [ ] Integration tests for full flow
- [ ] Tests for 5+ problem types
- [ ] Edge case tests

### Demo
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Deployed app or local setup instructions

---

## Notes

- **Socratic Method**: NEVER give direct answers. Guide through questions only.
- **Context Management**: Maintain full conversation history for proper context.
- **Math Rendering**: Use KaTeX for accurate math display.
- **Image Processing**: Use Vision API for best OCR results (or Tesseract.js for local).
- **Testing**: Test with diverse problem types to ensure Socratic method works.

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

Adding the code evaluation/security assessment task to DreamUp and Varsity Tutors:


```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.5.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.5.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.6: Final Submission Demo Video Preparation (45 minutes)

**Task 3.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.5.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.6: Final Submission Demo Video Preparation (45 minutes)

**Task 3.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.6: Final Submission Demo Video Preparation (45 minutes)

**Task 3.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: DreamUp Browser Game QA Pipeline

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Autonomous AI Agent for Browser Game Testing

---

## Executive Summary

DreamUp Browser Game QA Pipeline is an AI agent that autonomously tests browser games by simulating user interactions, capturing visual evidence, and evaluating playability metrics. The system works with any web-hosted game URL and generates structured reports with playability assessments.

**Key Deliverables:**
- ✅ Browser Automation Agent (load game, detect UI, interact)
- ✅ Evidence Capture (screenshots, console logs)
- ✅ AI Evaluation (LLM analysis of evidence)
- ✅ Execution Interface (TypeScript file or CLI)
- ✅ Structured Output (JSON with status, playability_score, issues)

**Success Targets:**
- **MVP (Nov 4)**: Basic agent loads games, takes screenshots, generates basic reports
- **Early Submission (Nov 7)**: + Interaction system, AI evaluation, structured output
- **Final Submission (Nov 9)**: + Comprehensive testing, error handling, documentation

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | Browser automation working, screenshots captured, basic report generation |
| **Early Submission** | Friday, November 7, 2025 | + Interaction system, AI evaluation, structured JSON output |
| **Final Submission** | Sunday, November 9, 2025 | + Error handling, comprehensive testing, documentation |

---

## Technology Stack

### Core Stack
- **Language**: TypeScript (required per assignment)
- **Runtime**: Node.js with `bun run qa.ts` or `npx tsx qa.ts`
- **Browser Automation**: Browserbase with Stagehand (recommended) or Puppeteer/Playwright
- **AI Framework**: Vercel AI SDK (recommended) or OpenAI SDK directly
- **LLM**: OpenAI GPT-4 Vision or Anthropic Claude with vision

### Browser Automation Options
- **Option A (Recommended)**: Browserbase + Stagehand
  - Browserbase: Cloud browser infrastructure
  - Stagehand: High-level automation library
  - Free tier: 1 browser-hour included
- **Option B**: Puppeteer (local)
  - Full browser control
  - Requires Chrome/Chromium installed
- **Option C**: Playwright
  - Cross-browser support
  - Better reliability than Puppeteer

### Development Tools
- **Package Manager**: npm or bun
- **Build Tool**: TypeScript compiler (tsc) or tsx
- **Testing**: Vitest or Jest
- **Linting**: ESLint + Prettier
- **File Handling**: fs-extra for file operations

---

## Architecture Overview

### System Components

```
dreamup-qa/
├── src/
│   ├── agent/
│   │   ├── browserAgent.ts         # Main browser automation agent
│   │   ├── interaction.ts           # Game interaction logic
│   │   └── monitoring.ts           # Crash/freeze detection
│   ├── capture/
│   │   ├── screenshots.ts          # Screenshot capture
│   │   ├── console.ts              # Console log capture
│   │   └── artifacts.ts            # Artifact management
│   ├── evaluation/
│   │   ├── llmEvaluator.ts         # LLM evaluation service
│   │   ├── prompts.ts               # Evaluation prompts
│   │   └── scoring.ts               # Playability scoring
│   ├── output/
│   │   ├── reportGenerator.ts      # JSON report generation
│   │   └── formatters.ts           # Output formatting
│   └── qa.ts                        # Main entry point
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── test-games/                      # Sample games for testing
│   └── urls.json                    # Test game URLs
└── docs/
    ├── technical-writeup.md
    └── decision-log.md
```

### Agent Flow

1. **Initialize**: Connect to browser (Browserbase or local)
2. **Load Game**: Navigate to game URL, wait for load
3. **Observe**: Capture baseline screenshot, wait for render
4. **Interact**: Find UI elements, click buttons, simulate gameplay
5. **Monitor**: Detect crashes, freezes, errors via console and screenshots
6. **Evaluate**: Submit evidence to LLM for assessment
7. **Report**: Generate structured JSON output

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Basic browser automation and evidence capture working

#### Phase 1.1: Project Setup & Foundation (90 minutes)

**Task 1.1.1**: Initialize TypeScript project
- Duration: 20 minutes
- Create `dreamup-qa/` directory
- Initialize npm: `npm init -y`
- Install TypeScript: `npm install -D typescript @types/node`
- Install tsx: `npm install -D tsx` (for `npx tsx qa.ts`)
- Create `tsconfig.json` with proper settings
- PowerShell: `New-Item -ItemType Directory -Path dreamup-qa; Set-Location dreamup-qa; npm init -y`

**Task 1.1.2**: Setup Browserbase account and API key
- Duration: 15 minutes
- Sign up for Browserbase (free tier)
- Get API key from dashboard
- Create `.env` file with `BROWSERBASE_API_KEY`
- Add `.env.example` with placeholders
- Create `.gitignore` with `.env` entries

**Task 1.1.3**: Install browser automation libraries
- Duration: 20 minutes
- Install Browserbase SDK: `npm install @browserbasehq/sdk`
- Install Stagehand: `npm install @browserbasehq/stagehand`
- Install OpenAI SDK (for evaluation): `npm install openai`
- Install Vercel AI SDK (optional): `npm install ai`
- Verify installations: `npm list`

**Task 1.1.4**: Create project structure
- Duration: 20 minutes
- Create directory structure (agent/, capture/, evaluation/, output/)
- Create placeholder files for main modules
- Create `src/qa.ts` as main entry point
- Create basic README with setup instructions

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest @types/node`
- Create `tests/` directory structure
- Create sample test to verify setup
- Add test script to `package.json`: `"test": "vitest"`

#### Phase 1.2: Browser Automation - Basic Agent (150 minutes)

**Task 1.2.1**: Connect to Browserbase
- Duration: 30 minutes
- Create `src/agent/browserAgent.ts`
- Function: `createBrowserSession() => Promise<Browser>`
- Initialize Browserbase client with API key
- Create browser session
- Return browser instance
- Test: Connection works

**Task 1.2.2**: Load game URL
- Duration: 30 minutes
- Function: `loadGame(browser: Browser, url: string) => Promise<Page>`
- Navigate to game URL
- Wait for page load (waitForLoadState or timeout)
- Wait for initial render (wait for specific element or timeout)
- Handle: Load failures, timeouts
- Return: Page object

**Task 1.2.3**: Wait for initial render
- Duration: 30 minutes
- Function: `waitForRender(page: Page, timeout: number) => Promise<void>`
- Wait for: DOMContentLoaded, page load
- Wait for: Game canvas or main container element
- Timeout: 30 seconds max
- Retry: Up to 3 times if initial load fails
- Handle: Network errors, slow loads

**Task 1.2.4**: Basic error detection
- Duration: 30 minutes
- Function: `detectErrors(page: Page) => Promise<Error[]>`
- Listen for: Console errors
- Listen for: Network errors
- Detect: Page crashes (page closed unexpectedly)
- Return: Array of error messages
- Store: Errors for evaluation

**Task 1.2.5**: Test basic agent flow
- Duration: 30 minutes
- Create test game URL (e.g., simple HTML5 game from itch.io)
- Test: Load game → Wait for render → Detect errors
- Verify: All steps work correctly
- Document: Any issues encountered

#### Phase 1.3: Evidence Capture - Screenshots (120 minutes)

**Task 1.3.1**: Capture baseline screenshot
- Duration: 30 minutes
- Create `src/capture/screenshots.ts`
- Function: `captureScreenshot(page: Page, timestamp: number) => Promise<string>`
- Capture: Full page screenshot
- Save: To `artifacts/screenshots/` directory
- Filename: `screenshot-{timestamp}.png`
- Return: File path
- Use: Playwright's `screenshot()` or Puppeteer's `screenshot()`

**Task 1.3.2**: Capture multiple screenshots during session
- Duration: 45 minutes
- Function: `captureSessionScreenshots(page: Page, count: number) => Promise<string[]>`
- Capture: 3-5 screenshots at key moments
  - After initial load
  - After first interaction
  - During gameplay
  - After navigation (if applicable)
  - Final state
- Timestamp: Each screenshot
- Store: All screenshot paths

**Task 1.3.3**: Create artifacts directory structure
- Duration: 15 minutes
- Create `src/capture/artifacts.ts`
- Function: `setupArtifactsDirectory(gameUrl: string) => Promise<string>`
- Create: `artifacts/{game-id}/` directory
- Subdirectories: `screenshots/`, `console-logs/`, `reports/`
- Return: Base artifacts path
- Clean: Old artifacts before new run (optional)

**Task 1.3.4**: Test screenshot capture
- Duration: 30 minutes
- Test: Screenshot capture on test game
- Verify: Screenshots saved correctly
- Verify: Timestamps accurate
- Verify: Multiple screenshots captured
- Check: Image quality and size

#### Phase 1.4: Evidence Capture - Console Logs (90 minutes)

**Task 1.4.1**: Capture console logs
- Duration: 45 minutes
- Create `src/capture/console.ts`
- Function: `captureConsoleLogs(page: Page) => Promise<string[]>`
- Listen for: Console messages (log, error, warn, info)
- Capture: All console output during session
- Store: Logs in `artifacts/{game-id}/console-logs/`
- Format: JSON or text file with timestamps
- Include: Error stack traces

**Task 1.4.2**: Filter and format console logs
- Duration: 30 minutes
- Function: `formatConsoleLogs(logs: ConsoleMessage[]) => string`
- Filter: Relevant errors and warnings
- Format: Human-readable or JSON
- Include: Timestamp, level, message, stack (if error)
- Save: To file

**Task 1.4.3**: Test console log capture
- Duration: 15 minutes
- Test: Console log capture on test game
- Verify: Logs captured correctly
- Verify: Errors detected
- Verify: Logs saved to file

#### Phase 1.5: Basic Report Generation (90 minutes)

**Task 1.5.1**: Create basic report structure
- Duration: 30 minutes
- Create `src/output/reportGenerator.ts`
- Define: Report interface
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score?: number;
    issues: string[];
    screenshots: string[];
    timestamp: string;
  }
  ```

**Task 1.5.2**: Generate basic JSON report
- Duration: 30 minutes
- Function: `generateReport(evidence: Evidence) => QAReport`
- Input: Screenshots, console logs, errors
- Output: Basic report with:
  - Status (based on errors detected)
  - Issues (from console errors)
  - Screenshot paths
  - Timestamp
- Save: To `artifacts/{game-id}/report.json`

**Task 1.5.3**: Create main entry point (qa.ts)
- Duration: 30 minutes
- Create `src/qa.ts` as main entry point
- Function: `main(gameUrl: string) => Promise<void>`
- Flow:
  1. Setup artifacts directory
  2. Create browser session
  3. Load game
  4. Capture screenshots
  5. Capture console logs
  6. Generate report
  7. Save report
- CLI: Accept URL as argument: `npx tsx qa.ts <game-url>`
- PowerShell: `npx tsx src/qa.ts "https://example.com/game"`

#### Phase 1.6: Basic Testing & Validation (90 minutes)

**Task 1.6.1**: Test with simple game
- Duration: 30 minutes
- Find: Simple HTML5 game (e.g., tic-tac-toe from itch.io)
- Test: Full flow (load → screenshot → console → report)
- Verify: Report generated correctly
- Verify: Screenshots captured
- Verify: Console logs captured

**Task 1.6.2**: Test error scenarios
- Duration: 30 minutes
- Test: Invalid URL (should handle gracefully)
- Test: Non-existent game (should detect load failure)
- Test: Slow-loading game (should timeout appropriately)
- Test: Game with errors (should capture in report)
- Verify: Error handling works correctly

**Task 1.6.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Browser connection
  - Screenshot capture
  - Console log capture
  - Report generation
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.7: MVP Demo Video Preparation (45 minutes)

**Task 1.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline MVP - autonomous browser game testing"
  - Setup (20s): Show project structure, Browserbase setup
  - Game Loading (40s): Load game URL, wait for render, capture baseline screenshot
  - Evidence Capture (50s): Show multiple screenshots captured, console logs captured
  - Report Generation (30s): Show JSON report with status, issues, screenshots
  - Conclusion (10s): "MVP demonstrates basic automation, evidence capture, and reporting"

**Task 1.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key CLI outputs/features
- Visual cues for terminal commands
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add interaction system, AI evaluation, structured output

#### Phase 2.1: Game Interaction System (180 minutes)

**Task 2.1.1**: Detect common UI patterns
- Duration: 60 minutes
- Create `src/agent/interaction.ts`
- Function: `detectUIElements(page: Page) => Promise<UIElements>`
- Detect:
  - Start/Play buttons (text: "Start", "Play", "Begin")
  - Menus (navigation elements)
  - Game over screens (text: "Game Over", "You Win", "You Lose")
  - Controls (keyboard hints, button indicators)
- Use: Stagehand's element detection or Playwright's selectors
- Return: Detected elements

**Task 2.1.2**: Implement click interactions
- Duration: 45 minutes
- Function: `clickElement(page: Page, selector: string) => Promise<void>`
- Click: Start/Play buttons
- Click: Menu items
- Wait: For action to complete (waitForNavigation or timeout)
- Handle: Element not found, click failures
- Retry: Up to 3 times if click fails

**Task 2.1.3**: Implement keyboard interactions
- Duration: 45 minutes
- Function: `simulateGameplay(page: Page) => Promise<void>`
- Simulate: Arrow keys (movement)
- Simulate: Spacebar (jump/action)
- Simulate: Enter (confirm/select)
- Sequence: Execute action sequence (e.g., move → jump → move)
- Wait: Between actions for game to respond
- Duration: 5-10 seconds of gameplay simulation

**Task 2.1.4**: Navigate multiple screens/levels
- Duration: 30 minutes
- Function: `navigateScreens(page: Page) => Promise<void>`
- Logic: If game has multiple screens/levels, navigate through 2-3
- Detect: Level completion, screen transitions
- Capture: Screenshots at each screen
- Handle: Games with single screen (skip navigation)

#### Phase 2.2: Monitoring & Error Detection (120 minutes)

**Task 2.2.1**: Detect crashes and freezes
- Duration: 45 minutes
- Create `src/agent/monitoring.ts`
- Function: `monitorGameplay(page: Page) => Promise<MonitorResult>`
- Detect:
  - Crashes: Page closed unexpectedly, JavaScript errors
  - Freezes: No response to interactions, timeout
  - Rendering issues: Blank screens, distorted graphics
- Use: Screenshot comparison (compare consecutive screenshots)
- Use: Console error monitoring
- Return: Status (crashed, frozen, running)

**Task 2.2.2**: Implement timeout and retry logic
- Duration: 45 minutes
- Function: `executeWithTimeout<T>(fn: () => Promise<T>, timeout: number) => Promise<T>`
- Max execution time: 5 minutes per game
- Retry: Failed loads up to 3 times
- Timeout: Individual operations (load, interaction, screenshot)
- Handle: Timeout errors gracefully
- Return: Result or timeout error

**Task 2.2.3**: Enhanced error detection
- Duration: 30 minutes
- Detect: Network errors (failed resource loads)
- Detect: Rendering errors (WebGL errors, canvas issues)
- Detect: Performance issues (slow frame rate)
- Log: All errors with context
- Include: Errors in final report

#### Phase 2.3: AI Evaluation with LLM (150 minutes)

**Task 2.3.1**: Setup LLM evaluation service
- Duration: 45 minutes
- Create `src/evaluation/llmEvaluator.ts`
- Function: `evaluateEvidence(evidence: Evidence) => Promise<Evaluation>`
- Setup: OpenAI client or Vercel AI SDK
- Use: GPT-4 Vision or Claude with vision
- Input: Screenshots, console logs, errors
- Output: Structured evaluation

**Task 2.3.2**: Create evaluation prompts
- Duration: 45 minutes
- Create `src/evaluation/prompts.ts`
- Prompt template:
  ```
  "Analyze these screenshots and console logs from a browser game test.
  Assess:
  1. Does the game load successfully?
  2. Are controls responsive?
  3. Did the game complete without crashes?
  
  Provide: pass/fail status, confidence score (0-1), and issue descriptions."
  ```
- Include: Screenshots (base64 or URLs)
- Include: Console logs summary
- Format: Structured JSON response

**Task 2.3.3**: Implement playability scoring
- Duration: 30 minutes
- Create `src/evaluation/scoring.ts`
- Function: `calculatePlayabilityScore(evaluation: Evaluation) => number`
- Factors:
  - Successful load (0.3 weight)
  - Responsive controls (0.3 weight)
  - No crashes (0.4 weight)
- Score: 0-1 (0 = unplayable, 1 = fully playable)
- Return: Numeric score

**Task 2.3.4**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation with known good game
- Test: Evaluation with broken game
- Verify: LLM provides accurate assessments
- Verify: Scores are reasonable
- Handle: LLM API errors gracefully

#### Phase 2.4: Structured Output (90 minutes)

**Task 2.4.1**: Enhance report structure
- Duration: 30 minutes
- Update report interface:
  ```typescript
  interface QAReport {
    status: 'pass' | 'fail' | 'partial';
    playability_score: number;
    issues: Issue[];
    screenshots: string[];
    timestamp: string;
    confidence?: number;
  }
  
  interface Issue {
    type: 'crash' | 'freeze' | 'error' | 'performance';
    description: string;
    severity: 'high' | 'medium' | 'low';
  }
  ```

**Task 2.4.2**: Generate comprehensive JSON report
- Duration: 30 minutes
- Function: `generateFullReport(evidence: Evidence, evaluation: Evaluation) => QAReport`
- Include:
  - Status (pass/fail/partial)
  - Playability score (0-1)
  - Issues array (detailed)
  - Screenshot paths
  - Timestamp
  - Confidence score (from LLM)
- Format: Pretty-printed JSON
- Save: To artifacts directory

**Task 2.4.3**: Create output formatters
- Duration: 30 minutes
- Create `src/output/formatters.ts`
- Function: `formatReport(report: QAReport) => string`
- Options: JSON (default), human-readable text
- Include: Summary at top, details below
- Add: Color coding for status (if terminal output)

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test interaction system
- Duration: 30 minutes
- Test: UI element detection on various games
- Test: Click interactions work
- Test: Keyboard interactions work
- Test: Navigation through screens
- Verify: All interaction types work correctly

**Task 2.5.2**: Test AI evaluation
- Duration: 30 minutes
- Test: Evaluation on 3+ different game types
- Test: Playability scores are accurate
- Test: Issues detected correctly
- Test: Confidence scores reasonable
- Verify: LLM evaluation works consistently

**Task 2.5.3**: Integration test for full flow
- Duration: 30 minutes
- Test: Full end-to-end flow:
  - Load game → Interact → Monitor → Evaluate → Report
- Test: Multiple game types (puzzle, platformer, idle)
- Verify: Complete report generated
- Verify: All artifacts saved correctly

**Task 2.5.4**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline Early Submission - now with AI evaluation"
  - Game Loading (25s): Load game, wait for render, detect UI elements
  - Interaction Demo (50s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (30s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Report Output (25s): Show structured JSON report with status, score, issues
  - Conclusion (10s): "Demonstrates complete autonomous game testing with AI evaluation"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for terminal/CLI output
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Comprehensive testing, error handling, documentation

#### Phase 3.1: Test on Diverse Game Types (180 minutes)

**Task 3.1.1**: Test on 3-5 different game types
- Duration: 120 minutes
- Test games:
  1. Simple Puzzle (tic-tac-toe) - Basic click interactions
  2. Platformer (Mario clone) - Keyboard controls, physics
  3. Idle/Clicker - Minimal interaction, persistent state
  4. Broken Game (intentionally buggy) - Test failure detection
  5. Complex Game (RPG demo) - Multiple levels/screens
- Find games at: itch.io/games/html5
- Verify: Agent works with all types
- Document: Results for each game

**Task 3.1.2**: Generate test reports
- Duration: 30 minutes
- Run: QA agent on each test game
- Save: Reports to `test-results/` directory
- Include: Screenshots, console logs, JSON reports
- Analyze: Accuracy of playability assessments
- Target: 80%+ accuracy on playability assessment

**Task 3.1.3**: Document test results
- Duration: 30 minutes
- Create: Test results summary document
- Include: Game URLs, playability scores, issues found
- Compare: Agent assessment vs. manual assessment
- Identify: False positives/negatives
- Document: Known limitations

#### Phase 3.2: Error Handling & Robustness (120 minutes)

**Task 3.2.1**: Handle edge cases
- Duration: 60 minutes
- Handle: Games that don't load (timeout, network error)
- Handle: Games with no interactive elements
- Handle: Games that crash immediately
- Handle: Games with infinite loops
- Handle: Games that require user input (username, etc.)
- Add: Graceful error messages

**Task 3.2.2**: Implement graceful degradation
- Duration: 30 minutes
- If: Screenshot capture fails → Continue with console logs
- If: LLM evaluation fails → Fallback to rule-based evaluation
- If: Interaction fails → Report as "non-interactive"
- Always: Generate report (even if partial)

**Task 3.2.3**: Add retry and recovery logic
- Duration: 30 minutes
- Retry: Failed loads up to 3 times
- Retry: Failed interactions (different selectors)
- Retry: LLM API calls (with exponential backoff)
- Recovery: Continue testing even if one step fails
- Log: All retry attempts

#### Phase 3.3: Documentation (120 minutes)

**Task 3.3.1**: Write technical writeup
- Duration: 45 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Browser Automation Approach (Browserbase vs. alternatives)
  - AI Evaluation Strategy
  - Interaction Detection Logic
  - Error Handling Strategy
  - Known Limitations
  - Future Improvements

**Task 3.3.2**: Update README
- Duration: 30 minutes
- Prerequisites (Node.js, Browserbase account, API keys)
- Installation steps
- Configuration (environment variables)
- Usage examples
- Test game URLs
- Troubleshooting section
- Example reports

**Task 3.3.3**: Create decision log
- Duration: 30 minutes
- Document key decisions:
  - Why Browserbase over Puppeteer
  - Why TypeScript over JavaScript
  - Why GPT-4 Vision over Claude
  - Why structured JSON output
  - Tradeoffs and alternatives considered

**Task 3.3.4**: Document AI tools and prompts
- Duration: 15 minutes
- List AI tools used
- Document key prompts
- Acknowledge AI assistance

#### Phase 3.4: Optional Stretch Features (120 minutes)

**Task 3.4.1**: GIF Recording (Optional)
- Duration: 60 minutes
- Install: gifshot or similar library
- Capture: Screenshot sequence during gameplay
- Generate: Animated GIF
- Include: In report output
- Test: GIF generation works

**Task 3.4.2**: Batch Testing (Optional)
- Duration: 30 minutes
- Function: `batchTest(urls: string[]) => Promise<Report[]>`
- Process: Multiple games sequentially
- Aggregate: Summary report across all games
- Output: Individual reports + aggregate summary

**Task 3.4.3**: Advanced Metrics (Optional)
- Duration: 30 minutes
- Add: FPS monitoring (if possible)
- Add: Load time analysis
- Add: Accessibility checks (basic)
- Include: In report output

#### Phase 3.5: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.5.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All TypeScript code for best practices
- Review: Browser automation code for efficiency
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows TypeScript best practices
- Review: LLM integration code for efficiency
- Optimize: API calls and screenshot processing

**Task 3.5.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in code)
- Check: Input validation (game URLs, user inputs)
- Check: File system security (screenshot storage)
- Review: Browser automation security (isolated browser sessions)
- Review: LLM API usage (rate limiting, error handling)
- Check: Environment variable handling
- Document: Security findings and mitigations
- Verify: No sensitive data in logs or outputs

#### Phase 3.6: Final Polish (90 minutes)

**Task 3.6.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Screenshot capture (compress if needed)
- Optimize: LLM API calls (batch if possible)
- Optimize: File I/O operations
- Profile: Execution time
- Target: <5 minutes per game

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ Agent tests 3+ diverse games successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Error handling works gracefully
- ✅ Structured JSON output correct
- ✅ README complete and accurate
- ✅ Technical writeup complete
- ✅ Test results documented
- ✅ No API keys in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "DreamUp QA Pipeline - Complete autonomous game testing"
  - System Overview (15s): Architecture, key components
  - Game Loading (20s): Load game, detect UI, wait for render
  - Interaction Demo (40s): Show click interactions, keyboard gameplay, navigation
  - Monitoring (20s): Show crash/freeze detection, error monitoring
  - AI Evaluation (40s): Show LLM analyzing evidence, generating playability score
  - Diverse Testing (25s): Show testing on 3+ different game types
  - Report Output (20s): Show structured JSON reports
  - Conclusion (10s): "Complete autonomous testing system with AI evaluation"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of terminal/CLI
- Highlight key technical achievements
- Show test results from diverse games

#### Phase 3.7: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass
- Run: QA agent on test games
- Verify: Reports generated correctly
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ Browser automation agent loads games
- ✅ Screenshots captured (3-5 per session)
- ✅ Console logs captured
- ✅ Basic JSON report generated
- ✅ CLI interface functional (`npx tsx qa.ts <url>`)
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Game interaction system working (clicks, keyboard)
- ✅ UI element detection working
- ✅ Monitoring and error detection working
- ✅ AI evaluation integrated (LLM analysis)
- ✅ Structured JSON output with playability score
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ Tested on 3-5 diverse game types successfully
- ✅ AI evaluation accuracy ≥80%
- ✅ Comprehensive error handling
- ✅ Documentation complete (technical writeup, README, decision log)
- ✅ Optional stretch features implemented (if time permits)
- ✅ All tests passing
- ✅ Code clean, linted, and documented

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Agent loops infinitely | High | Max action count, total timeout (5 min), action limits |
| LLM gives inconsistent results | Medium | Structured prompts, confidence thresholds, fallback heuristics |
| Games don't load in headless mode | High | Test with headed mode option, screenshot comparison, retry logic |
| API costs exceed budget | Low | Cache responses if possible, use cheaper models for iteration |
| Interaction detection fails | Medium | Multiple selector strategies, fallback to generic clicks |
| Time constraints | High | MVP is safety net, interaction is enhancement, prioritize core automation |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] TypeScript source code in `src/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] `.env.example` with required variables
- [ ] `.gitignore` configured properly
- [ ] `qa.ts` executable with `npx tsx qa.ts <url>` or `bun run qa.ts <url>`

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] Test results summary (3-5 games)
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for agent logic
- [ ] Integration tests for full flow
- [ ] Test results for 3-5 diverse games
- [ ] QA reports with screenshots

### Demo
- [ ] Demo video (2-5 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Example reports and artifacts

---

## Notes

- **TypeScript Required**: Assignment requires TypeScript (`bun run qa.ts` or `npx tsx qa.ts`)
- **Browserbase Recommended**: Free tier includes 1 browser-hour
- **Structured Output**: Must output JSON with `{status, playability_score, issues[], screenshots[], timestamp}`
- **Max Execution Time**: 5 minutes per game
- **Test Games**: Find at itch.io/games/html5 or create simple test cases

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.6.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.6.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.7.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.7: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.7.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.7.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.7.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.7.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.7.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.8: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.7.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.9: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.0  
**Created**: October 31, 2025  
**Last Updated**: October 31, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

```markdown
# Product Requirements Document: Varsity Tutors 10x K Factor

**Version:** 1.1  
**Date:** November 3, 2025  
**Project Duration:** 7 Days (Nov 3 - Nov 9, 2025)  
**Project Start:** November 3, 2025  
**Framework:** Based on October 16, 2025 success path methodology  
**Target:** Viral, Gamified, Supercharged Growth System

---

## Executive Summary

Varsity Tutors 10x K Factor is a production-ready growth system that makes learning feel fun, social, and "alive," turning every touchpoint into a shareable, referable moment across students, parents, and tutors. The system implements ≥4 closed-loop viral mechanics, presence signals, activity feeds, and agentic actions powered by session transcription to achieve K ≥ 1.20.

**Key Deliverables:**
- ✅ ≥4 closed-loop viral mechanics (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight, etc.)
- ✅ "Alive" Layer (presence signals, activity feed, mini-leaderboards, cohort rooms)
- ✅ Async Results as Viral Surfaces (share cards, deep links, cohort challenges)
- ✅ Required Agents (Loop Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- ✅ ≥4 Agentic Actions (≥2 student, ≥2 tutor) from session transcription
- ✅ Signed smart links + attribution service
- ✅ Event schema & dashboards (K, invites, conversion, FVM, retention)
- ✅ Results-page share packs (cards, reels, deep links)
- ✅ Compliance memo (COPPA/FERPA safe)

**Success Targets:**
- **MVP (Nov 4)**: Core viral loops + basic agents + presence layer working
- **Early Submission (Nov 7)**: + Agentic actions + smart links + results-page sharing
- **Final Submission (Nov 9)**: + Analytics + compliance + comprehensive testing

---

## Project Timeline

| Milestone | Date | Deliverable |
|-----------|------|-------------|
| **Phase 1: MVP** | Tuesday, November 4, 2025 | ≥4 viral loops, basic agents, presence layer, operator demo |
| **Early Submission** | Friday, November 7, 2025 | + Agentic actions, smart links, results-page sharing, analytics |
| **Final Submission** | Sunday, November 9, 2025 | + Compliance memo, comprehensive testing, documentation |

---

## Technology Stack

### Frontend Stack
- **Framework**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **UI Library**: shadcn/ui or Material-UI
- **Real-time**: Socket.io-client or WebSockets
- **State Management**: Zustand or Redux
- **Build Tool**: Vite or Next.js

### Backend Stack
- **Framework**: Node.js + Express.js or Next.js API
- **Language**: TypeScript
- **Real-time**: Socket.io for presence and activity feed
- **Database**: PostgreSQL or MongoDB (for production) / SQLite (for prototype)
- **Queue**: Bull (Redis) for job processing
- **Storage**: AWS S3 or local for assets (share cards, reels)

### AI/Agent Stack
- **LLM**: OpenAI GPT-4 or Anthropic Claude
- **SDK**: OpenAI SDK or Anthropic SDK
- **Agent Framework**: Model Context Protocol (MCP) servers or custom agent framework
- **Transcription**: OpenAI Whisper API or Web Speech API
- **Summarization**: LLM-based summarization of transcripts

### Analytics Stack
- **Event Tracking**: Custom event bus or PostHog/Mixpanel
- **Database**: PostgreSQL for event storage
- **Dashboards**: Custom React dashboard or Grafana
- **Streaming**: Redis Streams or Kafka for event processing

### Development Tools
- **Package Manager**: npm or yarn
- **Linting**: ESLint + Prettier
- **Testing**: Vitest or Jest
- **Documentation**: JSDoc + README

---

## Architecture Overview

### System Components

```
varsity-tutors-growth/
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ViralLoops/          # Viral loop UI components
│   │   │   ├── Presence/             # Presence indicators
│   │   │   ├── ActivityFeed/         # Activity feed
│   │   │   ├── Leaderboards/        # Mini-leaderboards
│   │   │   ├── ResultsPages/        # Results page sharing
│   │   │   └── ShareCards/          # Share card components
│   │   ├── hooks/
│   │   │   ├── usePresence.ts
│   │   │   ├── useActivityFeed.ts
│   │   │   └── useViralLoops.ts
│   │   └── App.tsx
├── backend/
│   ├── agents/
│   │   ├── loopOrchestrator.ts      # Required: Loop Orchestrator
│   │   ├── personalization.ts        # Required: Personalization
│   │   ├── incentives.ts              # Incentives & Economy
│   │   ├── socialPresence.ts         # Social Presence
│   │   ├── tutorAdvocacy.ts          # Tutor Advocacy
│   │   ├── trustSafety.ts            # Trust & Safety
│   │   └── experimentation.ts       # Required: Experimentation
│   ├── services/
│   │   ├── transcription.ts         # Session transcription
│   │   ├── summarization.ts          # Transcript summarization
│   │   ├── agenticActions.ts         # Agentic action triggers
│   │   ├── smartLinks.ts             # Signed smart links
│   │   ├── attribution.ts             # Attribution tracking
│   │   └── analytics.ts              # Event tracking
│   ├── api/
│   │   ├── viral-loops.ts           # Viral loop endpoints
│   │   ├── presence.ts               # Presence endpoints
│   │   ├── analytics.ts              # Analytics endpoints
│   │   └── share-cards.ts            # Share card generation
│   └── server.ts
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
    ├── technical-writeup.md
    ├── compliance-memo.md
    └── decision-log.md
```

### Data Flow

1. **Session Trigger**: User completes session → Transcription → Summarization → Agentic action triggers
2. **Loop Orchestration**: Agentic action → Loop Orchestrator → Chooses loop → Personalization → Incentives
3. **Viral Loop**: Share invitation → Smart link → Attribution → Join → FVM tracking
4. **Presence**: User activity → Presence service → Broadcast to cohort → Activity feed
5. **Analytics**: All events → Event bus → Stream processing → Warehouse → Dashboard

---

## Viral Loop Selection

**Selected Loops (≥4):**
1. **Buddy Challenge** (Student → Student) - After practice, share "Beat-my-score" micro-deck
2. **Results Rally** (Async → Social) - Results page generates rank vs. peers + challenge link
3. **Proud Parent** (Parent → Parent) - Weekly recap + shareable progress reel + invite
4. **Tutor Spotlight** (Tutor → Family/Peers) - After 5★ session, generate tutor card + invite link

**Rationale**: Covers all personas (student, parent, tutor), includes async results integration, has clear first-value moments, measurable K-factor potential.

---

## Phase Breakdown

### Phase 1: MVP (Due Tuesday, November 4, 2025)

**Objective**: Core viral loops and basic agents working

#### Phase 1.1: Project Setup & Foundation (120 minutes)

**Task 1.1.1**: Initialize React + TypeScript project
- Duration: 30 minutes
- Create `varsity-tutors-growth/` directory
- Initialize React app: `npm create vite@latest frontend -- --template react-ts`
- Install: React, TypeScript, Tailwind CSS, Socket.io-client
- PowerShell: `New-Item -ItemType Directory -Path varsity-tutors-growth; Set-Location varsity-tutors-growth`

**Task 1.1.2**: Setup backend with Express
- Duration: 30 minutes
- Create `backend/` directory
- Initialize Node.js: `npm init -y`
- Install: Express, TypeScript, Socket.io, OpenAI SDK, PostgreSQL/SQLite
- Create `server.ts` with Express + Socket.io
- Add TypeScript config

**Task 1.1.3**: Setup database schema
- Duration: 30 minutes
- Create database schema:
  - `users(id, persona, subject_preferences)`
  - `sessions(id, user_id, transcript, summary)`
  - `viral_loops(id, type, user_id, invite_id, status)`
  - `invites(id, loop_id, invitee_email, smart_link, status)`
  - `events(id, user_id, event_type, data, timestamp)`
  - `presence(id, user_id, status, last_seen)`
- Initialize database

**Task 1.1.4**: Configure environment variables
- Duration: 15 minutes
- Create `.env` files
- Add: `OPENAI_API_KEY`, `DATABASE_URL`, `JWT_SECRET`, `S3_BUCKET` (optional)
- Add `.env.example`
- Create `.gitignore`

**Task 1.1.5**: Setup testing framework
- Duration: 15 minutes
- Install Vitest: `npm install -D vitest`
- Create `tests/` directory structure
- Create sample test
- Add test script to `package.json`

#### Phase 1.2: Required Agents - Loop Orchestrator (150 minutes)

**Task 1.2.1**: Create Loop Orchestrator Agent
- Duration: 60 minutes
- Create `backend/agents/loopOrchestrator.ts`
- Function: `chooseLoop(context: Context) => LoopType`
- Logic:
  - Input: Session summary, user persona, current state
  - Check: Loop eligibility (user has friends, has results, etc.)
  - Check: Throttling (not too many invites sent)
  - Choose: Best loop based on context
  - Return: LoopType + rationale
- Implement MCP server pattern or custom agent pattern

**Task 1.2.2**: Implement loop eligibility checking
- Duration: 45 minutes
- Function: `checkEligibility(loop: LoopType, user: User) => boolean`
- Checks:
  - Buddy Challenge: User has completed practice, has friends
  - Results Rally: User has results page, has subject peers
  - Proud Parent: User is parent, has weekly progress
  - Tutor Spotlight: User is tutor, has 5★ session
- Return: Eligible (true/false) with reason

**Task 1.2.3**: Implement throttling logic
- Duration: 45 minutes
- Function: `checkThrottle(user: User, loop: LoopType) => boolean`
- Logic:
  - Track: Invites sent per day per user
  - Limit: Max invites/day (configurable, default: 5)
  - Cooldown: Between same loop type (1 hour)
- Return: Throttled (true/false)

#### Phase 1.3: Required Agents - Personalization Agent (120 minutes)

**Task 1.3.1**: Create Personalization Agent
- Duration: 60 minutes
- Create `backend/agents/personalization.ts`
- Function: `personalizeInvite(loop: LoopType, user: User, context: Context) => InviteContent`
- Personalize:
  - Copy: By persona (student/parent/tutor)
  - Subject: Match user's subject preferences
  - Intent: Based on current activity (practice, results, session)
- Return: Personalized invite content (title, message, CTA)

**Task 1.3.2**: Implement persona-based personalization
- Duration: 30 minutes
- Function: `personalizeByPersona(persona: string, loop: LoopType) => Copy`
- Templates:
  - Student: "Challenge your friend to beat your score!"
  - Parent: "Share your child's progress with another parent"
  - Tutor: "Share your tutoring success with other families"
- Return: Persona-specific copy

**Task 1.3.3**: Implement subject-based personalization
- Duration: 30 minutes
- Function: `personalizeBySubject(subject: string, loop: LoopType) => Copy`
- Match: Subject to loop (Algebra practice → Algebra challenge)
- Include: Subject-specific context in invite
- Return: Subject-specific copy

#### Phase 1.4: Viral Loop 1 - Buddy Challenge (150 minutes)

**Task 1.4.1**: Implement Buddy Challenge backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/buddyChallenge.ts`
- Function: `createBuddyChallenge(userId: string, practiceId: string) => Challenge`
- Logic:
  - Generate: 5-question micro-deck from skill gaps
  - Create: Challenge record in database
  - Generate: Share link (smart link)
  - Return: Challenge object with share link

**Task 1.4.2**: Create Buddy Challenge UI component
- Duration: 45 minutes
- Create `frontend/src/components/ViralLoops/BuddyChallenge.tsx`
- Display: Challenge card with "Share Challenge" button
- Show: Skill deck preview
- Include: "Both get streak shields if friend reaches FVM" message
- Integrate: Share functionality

**Task 1.4.3**: Implement challenge acceptance flow
- Duration: 45 minutes
- Function: `acceptChallenge(challengeId: string, inviteeId: string) => Result`
- Logic:
  - Verify: Challenge exists and not expired
  - Create: Challenge attempt for invitee
  - Track: Challenge completion
  - Award: Streak shields to both users when invitee reaches FVM
- Return: Challenge started confirmation

#### Phase 1.5: Viral Loop 2 - Results Rally (120 minutes)

**Task 1.5.1**: Implement Results Rally backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/resultsRally.ts`
- Function: `createResultsRally(userId: string, resultsId: string) => Rally`
- Logic:
  - Calculate: Rank vs. peers (from results data)
  - Generate: Challenge link (beat-my-rank)
  - Create: Rally record in database
  - Generate: Share link
- Return: Rally object with rank and share link

**Task 1.5.2**: Create Results Rally UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ResultsRally.tsx`
- Display: Rank vs. peers, "Challenge a friend" CTA
- Show: Leaderboard preview
- Integrate: Share functionality

**Task 1.5.3**: Integrate with results pages
- Duration: 30 minutes
- Modify: Results page component to include Results Rally
- Trigger: After results page load
- Display: Rally component if eligible
- Test: Integration works

#### Phase 1.6: Viral Loop 3 - Proud Parent (120 minutes)

**Task 1.6.1**: Implement Proud Parent backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/proudParent.ts`
- Function: `createProudParentInvite(parentId: string, childId: string) => Invite`
- Logic:
  - Generate: Weekly recap card (progress summary)
  - Generate: Shareable progress reel (20-30s summary)
  - Create: Invite record
  - Generate: Referral link for class pass
- Return: Invite object with reel and link

**Task 1.6.2**: Create Proud Parent UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/ProudParent.tsx`
- Display: Weekly recap card, progress reel preview
- Show: "Invite a parent" CTA
- Include: Class pass offer
- Integrate: Share functionality

**Task 1.6.3**: Generate progress reel
- Duration: 30 minutes
- Function: `generateProgressReel(childId: string) => Reel`
- Logic:
  - Collect: Key moments and wins from week
  - Compile: 20-30s video/slideshow
  - Make: Privacy-safe (no PII, only achievements)
  - Store: Reel asset
- Return: Reel URL

#### Phase 1.7: Viral Loop 4 - Tutor Spotlight (120 minutes)

**Task 1.7.1**: Implement Tutor Spotlight backend logic
- Duration: 60 minutes
- Create `backend/services/viralLoops/tutorSpotlight.ts`
- Function: `createTutorSpotlight(tutorId: string, sessionId: string) => Spotlight`
- Logic:
  - Verify: 5★ session rating
  - Generate: Tutor card with achievements
  - Create: Invite link for tutor to share
  - Track: Referral XP for tutor
- Return: Spotlight object with card and link

**Task 1.7.2**: Create Tutor Spotlight UI component
- Duration: 30 minutes
- Create `frontend/src/components/ViralLoops/TutorSpotlight.tsx`
- Display: Tutor card with achievements
- Show: "Share your success" CTA
- Include: Referral XP benefits
- Integrate: Share functionality (WhatsApp/SMS)

**Task 1.7.3**: Implement referral XP tracking
- Duration: 30 minutes
- Function: `trackReferralXP(tutorId: string, joinId: string) => XP`
- Logic:
  - Verify: Join came from tutor's referral
  - Calculate: XP amount (based on conversion)
  - Award: XP to tutor
  - Update: Tutor leaderboard
- Return: XP awarded

#### Phase 1.8: "Alive" Layer - Presence Signals (90 minutes)

**Task 1.8.1**: Implement presence service
- Duration: 45 minutes
- Create `backend/services/presence.ts`
- Function: `updatePresence(userId: string, status: string) => void`
- Logic:
  - Track: User status (online, offline, practicing)
  - Broadcast: Presence updates via Socket.io
  - Store: Last seen timestamp
  - Cleanup: Offline users after timeout
- Socket.io: Emit presence updates to cohort

**Task 1.8.2**: Create presence UI component
- Duration: 30 minutes
- Create `frontend/src/components/Presence/PresenceIndicator.tsx`
- Display: "28 peers practicing Algebra now" message
- Show: Friends online indicator
- Update: Real-time via Socket.io
- Style: Active/inactive states

**Task 1.8.3**: Implement cohort presence broadcasting
- Duration: 15 minutes
- Function: `broadcastCohortPresence(subject: string) => Presence[]`
- Logic:
  - Query: Users practicing same subject
  - Count: Active users
  - Broadcast: Count and active users to cohort
- Return: Presence array

#### Phase 1.9: "Alive" Layer - Activity Feed (90 minutes)

**Task 1.9.1**: Implement activity feed service
- Duration: 45 minutes
- Create `backend/services/activityFeed.ts`
- Function: `addActivity(activity: Activity) => void`
- Activities:
  - "Friend completed Algebra practice"
  - "Friend earned a badge"
  - "Friend reached a streak milestone"
- Store: Activities in database
- Broadcast: Via Socket.io

**Task 1.9.2**: Create activity feed UI component
- Duration: 30 minutes
- Create `frontend/src/components/ActivityFeed/ActivityFeed.tsx`
- Display: List of recent activities
- Update: Real-time via Socket.io
- Filter: By friend or all
- Style: Timeline or list view

**Task 1.9.3**: Integrate activity feed into main UI
- Duration: 15 minutes
- Add: Activity feed sidebar or component
- Connect: To Socket.io for real-time updates
- Test: Activity feed displays correctly

#### Phase 1.10: "Alive" Layer - Mini-Leaderboards (90 minutes)

**Task 1.10.1**: Implement leaderboard service
- Duration: 45 minutes
- Create `backend/services/leaderboards.ts`
- Function: `getLeaderboard(subject: string, metric: string) => Leaderboard`
- Metrics:
  - Practice score
  - Streak days
  - XP earned
- Logic:
  - Query: Top N users by metric
  - Include: User ranking
  - Return: Leaderboard data
- Handle: Age bands, fairness (new vs. veterans)

**Task 1.10.2**: Create leaderboard UI component
- Duration: 30 minutes
- Create `frontend/src/components/Leaderboards/MiniLeaderboard.tsx`
- Display: Top 10 users with rankings
- Show: Current user's position
- Update: Periodically (every 30 seconds)
- Style: Clean, gamified design

**Task 1.10.3**: Integrate leaderboards into results pages
- Duration: 15 minutes
- Add: Mini-leaderboard to results pages
- Show: User's rank vs. peers
- Include: "Challenge a friend" CTA
- Test: Leaderboard displays correctly

#### Phase 1.11: Basic Testing & Validation (120 minutes)

**Task 1.11.1**: Test viral loops end-to-end
- Duration: 60 minutes
- Test: Buddy Challenge (create → share → accept → complete)
- Test: Results Rally (generate → share → join)
- Test: Proud Parent (generate → share → invite)
- Test: Tutor Spotlight (generate → share → referral)
- Verify: All loops work correctly

**Task 1.11.2**: Test presence and activity feed
- Duration: 30 minutes
- Test: Presence updates broadcast correctly
- Test: Activity feed displays activities
- Test: Leaderboards update correctly
- Verify: Real-time updates work

**Task 1.11.3**: Integrated Unit Testing for Phase 1
- Duration: 30 minutes
- Write unit tests for:
  - Loop Orchestrator logic
  - Personalization logic
  - Viral loop creation
  - Presence updates
- Run: `npm test` to verify
- Document: Any failing tests

#### Phase 1.12: MVP Demo Video Preparation (45 minutes)

**Task 1.12.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor MVP - viral growth system"
  - System Overview (20s): Architecture, viral loops, agents
  - Viral Loop Demo (80s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight
  - Presence Layer (30s): Show presence signals, activity feed, leaderboards
  - Conclusion (10s): "MVP demonstrates ≥4 viral loops and 'alive' layer"

**Task 1.12.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key UI elements/features
- Visual cues for viral loop triggers
- Timing markers for 3-minute target

---

### Phase 2: Early Submission (Due Friday, November 7, 2025)

**Objective**: Add agentic actions, smart links, results-page sharing, analytics

#### Phase 2.1: Session Transcription & Summarization (180 minutes)

**Task 2.1.1**: Implement transcription service
- Duration: 60 minutes
- Create `backend/services/transcription.ts`
- Function: `transcribeSession(audioFile: File) => Transcript`
- Option A: OpenAI Whisper API
  - Upload audio file
  - Call Whisper API
  - Get transcript
- Option B: Web Speech API (browser-based, less accurate)
- Store: Transcript in database
- Return: Transcript text

**Task 2.1.2**: Implement summarization service
- Duration: 60 minutes
- Create `backend/services/summarization.ts`
- Function: `summarizeTranscript(transcript: string) => Summary`
- Use: LLM (GPT-4 or Claude) to summarize
- Prompt: "Summarize this tutoring session. Identify: skill gaps, upcoming exams, stuck concepts, wins."
- Extract: Key information (skill gaps, exam dates, concepts)
- Store: Summary in database
- Return: Summary object

**Task 2.1.3**: Trigger agentic actions from summaries
- Duration: 60 minutes
- Create `backend/services/agenticActions.ts`
- Function: `triggerAgenticActions(sessionId: string, summary: Summary) => Actions[]`
- Logic:
  - Parse: Summary for triggers
  - For Students (≥2 actions):
    - "Beat-My-Skill" Challenge: If skill gaps detected → Generate 5-question deck
    - Study Buddy Nudge: If exam coming → Create co-practice invite
  - For Tutors (≥2 actions):
    - Parent Progress Reel: After session → Generate reel + invite
    - Next-Session Prep Pack: After session → Generate prep pack + share link
- Return: Array of triggered actions

#### Phase 2.2: Smart Links & Attribution (120 minutes)

**Task 2.2.1**: Implement signed smart links
- Duration: 60 minutes
- Create `backend/services/smartLinks.ts`
- Function: `generateSmartLink(inviteId: string, utm: object) => SmartLink`
- Logic:
  - Generate: Short code (unique identifier)
  - Sign: Link with JWT or HMAC
  - Include: UTM parameters (source, medium, campaign)
  - Include: Invite context (loop type, user id)
  - Store: Link in database
  - Return: Shortened link (e.g., `vtutor.io/invite/abc123`)

**Task 2.2.2**: Implement attribution tracking
- Duration: 60 minutes
- Create `backend/services/attribution.ts`
- Function: `trackAttribution(smartLink: string, userId: string) => Attribution`
- Logic:
  - Parse: Smart link to get invite context
  - Track: Last-touch attribution (join)
  - Store: Multi-touch attribution (view → sign-up → FVM)
  - Record: Referrer, loop type, campaign
  - Update: Viral loop status
- Return: Attribution object

**Task 2.2.3**: Handle smart link clicks
- Duration: 30 minutes
- Create API endpoint: `GET /invite/:code`
- Function: `handleInvite(code: string) => Redirect`
- Logic:
  - Validate: Link signature
  - Retrieve: Invite context
  - Track: Click event
  - Redirect: To deep link (pre-filled context)
- Return: Redirect to appropriate page

#### Phase 2.3: Results-Page Sharing (150 minutes)

**Task 2.3.1**: Create share card generator
- Duration: 60 minutes
- Create `backend/services/shareCards.ts`
- Function: `generateShareCard(resultsId: string, persona: string) => ShareCard`
- Logic:
  - Query: Results data (score, skills, recommendations)
  - Generate: Privacy-safe share card (no PII, only achievements)
  - Variants: Student, Parent, Tutor variants
  - Include: Deep link to results page
  - Render: As image or HTML
- Return: Share card URL

**Task 2.3.2**: Integrate sharing into results pages
- Duration: 45 minutes
- Modify: Results page components
- Add: "Share Results" button
- Generate: Share card on demand
- Include: "Challenge a friend" CTA
- Include: Deep link to skill check
- Test: Sharing works correctly

**Task 2.3.3**: Create deep link handler
- Duration: 45 minutes
- Function: `handleDeepLink(link: string) => Page`
- Logic:
  - Parse: Deep link to get context
  - Pre-fill: Skill deck, challenge, or practice set
  - Redirect: To appropriate page with context
  - Track: Deep link click
- Return: Pre-filled page

#### Phase 2.4: Analytics & Event Tracking (150 minutes)

**Task 2.4.1**: Implement event schema
- Duration: 45 minutes
- Create `backend/services/analytics.ts`
- Define event types:
  - `invite_sent` (invite_id, loop_type, user_id)
  - `invite_opened` (invite_id, user_id)
  - `account_created` (user_id, referrer_id)
  - `fvm_reached` (user_id, fvm_type)
  - `retention` (user_id, day)
- Store: Events in database or event bus

**Task 2.4.2**: Calculate K-factor
- Duration: 60 minutes
- Function: `calculateKFactor(cohort: Cohort) => number`
- Formula: K = (invites_per_user × invite_conversion_rate)
- Track:
  - `invites_per_user`: Average invites sent per user
  - `invite_conversion_rate`: % of invites that convert to joins
- Return: K-factor value
- Target: K ≥ 1.20

**Task 2.4.3**: Create analytics dashboard
- Duration: 45 minutes
- Create `frontend/src/components/Analytics/Dashboard.tsx`
- Display:
  - K-factor over time
  - Invites per user
  - Conversion rates
  - FVM rates
  - Retention curves (D1/D7/D28)
  - Loop funnel drop-offs
- Update: Real-time or periodic refresh

#### Phase 2.5: Enhanced Testing (90 minutes)

**Task 2.5.1**: Test agentic actions
- Duration: 45 minutes
- Test: Session transcription → Summarization → Agentic action triggers
- Test: ≥2 student actions (Beat-My-Skill, Study Buddy)
- Test: ≥2 tutor actions (Parent Reel, Prep Pack)
- Verify: Actions trigger correctly from summaries
- Verify: Actions feed viral loops

**Task 2.5.2**: Test smart links and attribution
- Duration: 30 minutes
- Test: Generate smart link → Click link → Attribution tracked
- Test: Last-touch attribution works
- Test: Multi-touch attribution stored
- Verify: Deep links work correctly

**Task 2.5.3**: Integrated Unit Testing for Phase 2
- Duration: 15 minutes
- Run full test suite: `npm test`
- Verify: All Phase 1 + Phase 2 tests pass
- Document: Any edge cases

#### Phase 2.6: Early Submission Demo Video Preparation (45 minutes)

**Task 2.6.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor Early Submission - now with agentic actions"
  - Agentic Actions (50s): Show session transcription → Summary → Action triggers (student + tutor)
  - Smart Links (30s): Show smart link generation, click tracking, attribution
  - Results-Page Sharing (40s): Show share card generation, deep links, cohort challenges
  - Analytics Dashboard (40s): Show K-factor, conversion rates, retention curves
  - Conclusion (10s): "Demonstrates complete viral growth system with agentic actions and analytics"

**Task 2.6.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Include narration for each section
- Bullet points for key features/outputs
- Visual cues for dashboard and analytics
- Timing markers for 3-minute target

---

### Phase 3: Final Submission (Due Sunday, November 9, 2025)

**Objective**: Compliance memo, comprehensive testing, documentation

#### Phase 3.1: Compliance & Safety (120 minutes)

**Task 3.1.1**: Write compliance memo
- Duration: 60 minutes
- Create `docs/compliance-memo.md` (1-page)
- Sections:
  - Data flows (what data collected, where stored)
  - Consent flows (explicit opt-in, revocable)
  - COPPA/FERPA compliance (parental gating for minors)
  - Privacy-safe defaults (no PII in share cards)
  - Child data segregation
  - Risk mitigation strategies
- Ensure: All recommendations are COPPA/FERPA safe

**Task 3.1.2**: Implement consent flows
- Duration: 30 minutes
- Add: Explicit consent UI for data processing
- Add: Consent tracking in database
- Add: Revocation flow
- Ensure: No processing without consent
- Test: Consent flows work correctly

**Task 3.1.3**: Implement parental gating
- Duration: 30 minutes
- Function: `checkParentalConsent(userId: string) => boolean`
- Logic:
  - Check: User age (if <13, require parental consent)
  - Gate: Certain features (sharing, viral loops) for minors
  - Require: Parental approval for child accounts
- Test: Parental gating works correctly

#### Phase 3.2: Trust & Safety Agent (120 minutes)

**Task 3.2.1**: Implement fraud detection
- Duration: 45 minutes
- Create `backend/agents/trustSafety.ts`
- Function: `detectFraud(invite: Invite) => FraudRisk`
- Checks:
  - Duplicate device/email (same user creating multiple accounts)
  - Suspicious invite patterns (too many invites in short time)
  - Invalid email domains
  - Known spam patterns
- Return: Fraud risk level

**Task 3.2.2**: Implement rate limiting
- Duration: 30 minutes
- Function: `checkRateLimit(userId: string, action: string) => boolean`
- Limits:
  - Max invites/day: 5
  - Cooldown between same loop: 1 hour
  - Max actions/hour: 20
- Return: Rate limit status

**Task 3.2.3**: Implement report/undo functionality
- Duration: 45 minutes
- Function: `reportInvite(inviteId: string, reason: string) => void`
- Function: `undoInvite(inviteId: string) => void`
- Logic:
  - Report: Store report in database
  - Undo: Revoke invite, remove from tracking
  - Track: Report rate (abuse metric)
- Return: Success confirmation

#### Phase 3.3: Experimentation Agent (90 minutes)

**Task 3.3.1**: Implement traffic allocation
- Duration: 45 minutes
- Create `backend/agents/experimentation.ts`
- Function: `allocateTraffic(userId: string) => Variant`
- Logic:
  - A/B test: Control vs. Treatment groups
  - Allocation: 50/50 split (configurable)
  - Track: User variant assignment
- Return: Variant (A or B)

**Task 3.3.2**: Calculate uplift and guardrail metrics
- Duration: 45 minutes
- Function: `calculateUplift(experiment: Experiment) => Metrics`
- Metrics:
  - K-factor: Treatment vs. Control
  - FVM lift: +20% target
  - Retention lift: +10% D7 target
  - Guardrails: Complaint rate, opt-out rate, abuse rate
- Return: Metrics object
- Target: K ≥ 1.20, abuse <0.5%

#### Phase 3.4: Comprehensive Testing (120 minutes)

**Task 3.4.1**: Test all viral loops end-to-end
- Duration: 60 minutes
- Test: Complete flow for each loop:
  1. Buddy Challenge: Practice → Share → Accept → Complete → FVM
  2. Results Rally: Results → Share → Join → FVM
  3. Proud Parent: Weekly recap → Share → Invite → Join
  4. Tutor Spotlight: 5★ session → Share → Referral → Join
- Verify: All loops complete successfully
- Verify: Attribution tracked correctly

**Task 3.4.2**: Test agentic actions
- Duration: 30 minutes
- Test: Session → Transcription → Summary → Action trigger
- Test: All ≥4 agentic actions (≥2 student, ≥2 tutor)
- Verify: Actions feed viral loops correctly
- Verify: Actions are privacy-safe

**Task 3.4.3**: Test analytics and K-factor calculation
- Duration: 30 minutes
- Test: K-factor calculation with seeded cohort
- Test: Event tracking for all events
- Test: Dashboard displays correct metrics
- Verify: K ≥ 1.20 for at least one loop
- Document: K-factor results

#### Phase 3.5: Documentation (120 minutes)

**Task 3.5.1**: Write technical writeup
- Duration: 60 minutes
- Sections:
  - System Overview
  - Architecture Decisions
  - Viral Loop Design (why these 4+ loops)
  - Agent Architecture (MCP or custom)
  - Smart Links & Attribution
  - Analytics & K-factor Calculation
  - Known Limitations
  - Future Improvements

**Task 3.5.2**: Update README
- Duration: 30 minutes
- Complete setup instructions
- Usage examples
- API documentation
- Testing instructions
- Deployment instructions
- Troubleshooting section

**Task 3.5.3**: Document AI tools and prompts
- Duration: 30 minutes
- List: AI tools used
- Document: Key prompts for agents
- Document: Prompt engineering for agentic actions
- Acknowledge: AI assistance

#### Phase 3.6: Code Evaluation, Refactoring & Security Assessment (90 minutes)

**Task 3.6.1**: Code evaluation and refactoring
- Duration: 45 minutes
- Review: All React/TypeScript code for best practices
- Review: Backend and agent code for best practices
- Refactor: Duplicate code, improve naming conventions
- Refactor: Improve code organization and structure
- Refactor: Enhance error handling and validation
- Verify: Code follows React and TypeScript best practices
- Review: Agent architecture for efficiency
- Optimize: API calls and real-time updates

**Task 3.6.2**: Security assessment
- Duration: 45 minutes
- Assess: API key security (never expose in frontend)
- Check: Input validation (user inputs, smart links)
- Check: SQL injection vulnerabilities (if using SQL)
- Review: Authentication and authorization
- Review: COPPA/FERPA compliance (parental gating, consent)
- Review: Data privacy (PII handling, child data segregation)
- Review: Smart link security (signature validation)
- Document: Security findings and mitigations
- Verify: No sensitive data in client-side code or logs

#### Phase 3.7: Final Polish (90 minutes)

**Task 3.7.1**: Code cleanup and linting
- Duration: 30 minutes
- Run linter: `npm run lint`
- Fix all linting errors
- Format code: `npm run format`
- Remove debug code
- Add JSDoc comments

**Task 3.7.2**: Optimize performance
- Duration: 30 minutes
- Optimize: Database queries
- Optimize: Real-time updates (Socket.io)
- Optimize: LLM API calls (batch if possible)
- Profile: Response times
- Target: <150ms decision SLA for in-app triggers

**Task 3.7.3**: Final verification checklist
- Duration: 30 minutes
- ✅ ≥4 viral loops functioning end-to-end
- ✅ ≥4 agentic actions (≥2 tutor, ≥2 student) working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20)
- ✅ Presence UI and leaderboards working
- ✅ Compliance memo complete
- ✅ Results-page sharing active
- ✅ Analytics dashboard working
- ✅ All tests passing
- ✅ Documentation complete
- ✅ No secrets in code or commits

#### Phase 3.8: Final Submission Demo Video Preparation (45 minutes)

**Task 3.8.1**: Create 3-minute demo video script
- Duration: 20 minutes
- Script should cover:
  - Introduction (10s): "Varsity Tutors 10x K Factor - Complete viral growth system"
  - System Overview (20s): Architecture, viral loops, agents, presence layer
  - Viral Loop Demo (60s): Show Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight with full flow
  - Agentic Actions (40s): Show session transcription → Summary → Action triggers → Viral loops
  - Smart Links & Analytics (30s): Show smart links, attribution, K-factor dashboard
  - Compliance & Safety (10s): Show consent flows, parental gating
  - Conclusion (10s): "Complete viral growth system achieving K ≥ 1.20"

**Task 3.8.2**: Create cue card with narration and highlights
- Duration: 15 minutes
- Final polished version with all features
- Include visual demonstrations of all components
- Highlight key metrics (K-factor, conversion rates)
- Show compliance features

#### Phase 3.9: Integrated Unit Testing for Phase 3
- Duration: 15 minutes
- Run complete test suite: `npm test`
- Verify: All tests pass (100% of required scenarios)
- Run: End-to-end tests for all loops
- Verify: K-factor calculation works
- Document: Final test results summary

---

## Success Criteria

### MVP (Phase 1) - November 4
- ✅ ≥4 viral loops implemented (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- ✅ Required agents implemented (Loop Orchestrator, Personalization)
- ✅ "Alive" layer working (presence, activity feed, leaderboards)
- ✅ Basic operator demo functional
- ✅ Unit tests for core functions passing

### Early Submission (Phase 2) - November 7
- ✅ Session transcription and summarization working
- ✅ ≥4 agentic actions implemented (≥2 student, ≥2 tutor)
- ✅ Smart links and attribution tracking working
- ✅ Results-page sharing active (share cards, deep links)
- ✅ Analytics dashboard with K-factor calculation
- ✅ Integration tests passing

### Final Submission (Phase 3) - November 9
- ✅ All ≥4 viral loops functioning end-to-end
- ✅ All ≥4 agentic actions working
- ✅ K-factor measured (pass/fail vs K ≥ 1.20 target)
- ✅ Compliance memo complete (COPPA/FERPA safe)
- ✅ Trust & Safety agent working (fraud detection, rate limiting)
- ✅ Experimentation agent working (traffic allocation, uplift calculation)
- ✅ Comprehensive testing complete
- ✅ Documentation complete (technical writeup, README, compliance memo)
- ✅ All tests passing

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| K-factor < 1.20 | High | Optimize loops, improve personalization, test with seeded cohort |
| Agentic actions fail | Medium | Fallback to default copy/reward, log errors, graceful degradation |
| Compliance violations | High | Clear consent flows, parental gating, privacy-safe defaults, review compliance memo |
| Fraud/abuse | Medium | Trust & Safety agent, rate limiting, report/undo functionality |
| API costs too high | Medium | Cache responses, use cheaper models for iteration, optimize prompts |
| Time constraints | High | MVP is safety net, prioritize core loops and agents first |

---

## Deliverables Checklist

### Code Repository
- [ ] GitHub repository with clean structure
- [ ] Frontend code in `frontend/` directory
- [ ] Backend code in `backend/` directory
- [ ] Agent code in `backend/agents/` directory
- [ ] All tests in `tests/` directory
- [ ] README.md with setup instructions
- [ ] .env.example with required variables
- [ ] .gitignore configured properly

### Documentation
- [ ] Technical writeup (1-2 pages)
- [ ] Compliance memo (1-page)
- [ ] Decision log with key architectural choices
- [ ] README with complete setup and usage
- [ ] AI tools and prompts documentation

### Testing
- [ ] Unit tests for all agents
- [ ] Integration tests for all viral loops
- [ ] E2E tests for full flows
- [ ] K-factor calculation tests

### Demo
- [ ] Thin-slice prototype (web/mobile) with ≥4 loops
- [ ] Demo video (3 minutes)
- [ ] Demo video script
- [ ] Demo video cue card
- [ ] Run-of-show demo (3-minute journey)

### Analytics
- [ ] Event schema documented
- [ ] Analytics dashboard functional
- [ ] K-factor calculated and documented
- [ ] Metrics report generated

---

## Notes

- **COPPA/FERPA Compliance**: All features must be COPPA/FERPA safe, with parental gating for minors
- **Privacy-Safe Defaults**: No PII in share cards, explicit consent required
- **K-Factor Target**: K ≥ 1.20 for at least one loop over 14-day cohort
- **Agentic Actions**: ≥4 total (≥2 student, ≥2 tutor) from session transcription
- **MCP Agents**: Agents communicate via Model Context Protocol (MCP) servers
- **Explainability**: Each agent decision must include rationale for auditability

---

**Document Version**: 1.1  
**Created**: October 31, 2025  
**Last Updated**: November 3, 2025  
**Project Start Date**: November 3, 2025  
**Framework**: Based on October 16, 2025 success path methodology


```

Creating risk assessment documents for each project, starting with ChainEquity:


```markdown
# Risk Assessment: ChainEquity

**Project:** ChainEquity - Tokenized Security Prototype  
**Date:** November 3, 2025  
**Project Start:** November 3, 2025  
**Project End:** November 9, 2025  
**Duration:** 7 Days

---

## **Probability of Success Rating**

### **Overall Probability of Success: 85% (High Confidence)**

**Breakdown:**
- **MVP Success (Nov 4)**: 90% - Core functionality achievable in 2 days
- **Early Submission Success (Nov 7)**: 85% - Corporate actions add complexity
- **Final Submission Success (Nov 9)**: 85% - Comprehensive testing may be tight

**Confidence Level:** High  
**Key Success Factors:** Familiar tech stack (Solidity, Node.js), well-documented ERC-20 standard, clear requirements  
**Primary Risk:** Gas optimization and comprehensive test coverage time constraints

---

## Risk Analysis

### 1. Technical Risks

#### Risk 1.1: Smart Contract Security Vulnerabilities
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- Reentrancy attacks on transfer functions
- Access control bypass vulnerabilities
- Integer overflow/underflow issues
- Allowlist logic bugs

**Mitigation:**
- Use OpenZeppelin's battle-tested contracts
- Implement checks-effects-interactions pattern
- Use Solidity 0.8.20+ (built-in overflow protection)
- Run security linters (Slither) if time permits
- Manual code review in Phase 3.6 security assessment

**Residual Risk:** Low (15%) - OpenZeppelin contracts reduce attack surface

---

#### Risk 1.2: Gas Costs Exceeding Targets
**Probability:** Medium (50%)  
**Impact:** Medium  
**Risk Score:** 125 (Medium)

**Description:**
- Corporate actions (split) may require high gas for on-chain iteration
- Batch operations may exceed target gas limits
- Testnet gas costs may differ from mainnet

**Mitigation:**
- Choose virtual split approach (no on-chain iteration)
- Implement batch operations efficiently
- Document actual costs vs. targets
- Propose optimizations if targets exceeded
- Gas benchmarks included in deliverables (acceptable if justified)

**Residual Risk:** Low (20%) - Virtual split avoids expensive operations

---

#### Risk 1.3: Event Indexer Gaps or Reorg Handling
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 110 (Medium)

**Description:**
- Missed events during indexer downtime
- Blockchain reorgs invalidating indexed data
- Slow indexing affecting cap-table export accuracy

**Mitigation:**
- Query from genesis block for accuracy
- Wait for block confirmations (12+ blocks for Sepolia)
- Implement retry logic for failed event processing
- Test with reorg scenarios (if possible on testnet)
- Accept "as-of block" snapshots as valid

**Residual Risk:** Low (15%) - Query from genesis ensures accuracy

---

#### Risk 1.4: Testnet RPC Rate Limits
**Probability:** Medium (45%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Free RPC endpoints have rate limits
- Test suite may exceed rate limits
- Deployment failures due to RPC issues

**Mitigation:**
- Use local Hardhat network for development
- Deploy to Sepolia only for final demo
- Use paid RPC if free tier insufficient
- Implement retry logic with exponential backoff
- Cache responses where possible

**Residual Risk:** Medium (25%) - Multiple RPC providers available

---

### 2. Timeline & Scope Risks

#### Risk 2.1: Insufficient Time for Comprehensive Testing
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- 7-day timeline is compressed (from original 10-day plan)
- Comprehensive test suite may be incomplete
- Edge cases may be missed

**Mitigation:**
- Prioritize required test scenarios (10 scenarios listed)
- Write tests incrementally (Phase 1.6, 2.4, 3.1)
- Use test templates for common patterns
- Accept partial test coverage if time constrained (document)
- Focus on happy path + critical failure scenarios

**Residual Risk:** Medium (30%) - Required scenarios are clear and achievable

---

#### Risk 2.2: Corporate Actions Implementation Complexity
**Probability:** Low (25%)  
**Impact:** Medium  
**Risk Score:** 80 (Low-Medium)

**Description:**
- Virtual split approach may have edge cases
- Symbol change logic may require careful testing
- Integration with cap-table export may have bugs

**Mitigation:**
- Choose simplest approach (virtual split, mutable symbol)
- Document tradeoffs clearly
- Test thoroughly in Phase 2.1-2.2
- Accept MVP approach (not production-ready)

**Residual Risk:** Low (10%) - Virtual approach simplifies implementation

---

#### Risk 2.3: Operator Demo UI/CLI Polish Time
**Probability:** Medium (35%)  
**Impact:** Low  
**Risk Score:** 70 (Low-Medium)

**Description:**
- UI polish may consume too much time
- CLI interface may need more features than planned

**Mitigation:**
- Prioritize CLI over UI (faster to build)
- Accept minimal UI if time constrained
- Focus on functional demo over polished UI
- Scripted demo acceptable (automated flow)

**Residual Risk:** Low (15%) - CLI + scripted demo sufficient for MVP

---

### 3. Dependency & Integration Risks

#### Risk 3.1: OpenZeppelin Contract Compatibility
**Probability:** Low (15%)  
**Impact:** Medium  
**Risk Score:** 60 (Low)

**Description:**
- OpenZeppelin v5.0 may have breaking changes
- Integration with custom gating logic may conflict

**Mitigation:**
- Use well-documented OpenZeppelin contracts
- Test integration early (Phase 1.2)
- Use stable version (v5.0 is stable)
- Document any compatibility issues

**Residual Risk:** Low (5%) - OpenZeppelin is mature and well-tested

---

#### Risk 3.2: Backend-Contract Integration Issues
**Probability:** Medium (30%)  
**Impact:** Medium  
**Risk Score:** 105 (Medium)

**Description:**
- ethers.js v6 API changes from v5
- Transaction confirmation timing issues
- Event listening setup complexity

**Mitigation:**
- Use ethers.js v6 (current stable version)
- Test integration early (Phase 1.3)
- Implement proper wait logic for confirmations
- Use WebSocket subscriptions for events (more reliable)

**Residual Risk:** Low (15%) - ethers.js v6 is well-documented

---

#### Risk 3.3: SQLite Performance with Large Datasets
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Cap-table queries may be slow with many events
- SQLite may not scale well for production (but OK for prototype)

**Mitigation:**
- Add proper indexes (Phase 1.4)
- Accept SQLite limitations (prototype scope)
- Document migration path to PostgreSQL if needed
- Optimize queries in Phase 3.6

**Residual Risk:** Low (5%) - SQLite sufficient for prototype scale

---

### 4. Resource & Knowledge Risks

#### Risk 4.1: Solidity Learning Curve
**Probability:** Low (20%)  
**Impact:** Medium  
**Risk Score:** 80 (Low-Medium)

**Description:**
- Limited Solidity experience may slow development
- Best practices may not be followed initially

**Mitigation:**
- Use OpenZeppelin contracts (reduce custom code)
- Follow Solidity style guide
- Review security practices in Phase 3.6
- Accept learning curve (document in decision log)

**Residual Risk:** Low (10%) - OpenZeppelin reduces complexity

---

#### Risk 4.2: Blockchain Development Environment Setup
**Probability:** Low (25%)  
**Impact:** Medium  
**Risk Score:** 75 (Low-Medium)

**Description:**
- Hardhat setup may have issues
- Testnet deployment may require troubleshooting
- Private key management complexity

**Mitigation:**
- Use Hardhat's official setup guide
- Test deployment early (Phase 1.6)
- Use .env for secrets (never commit)
- Document setup issues in README

**Residual Risk:** Low (15%) - Hardhat has excellent documentation

---

### 5. External & Compliance Risks

#### Risk 5.1: False Compliance Claims
**Probability:** Low (10%)  
**Impact:** High  
**Risk Score:** 100 (Medium)

**Description:**
- Accidentally implying regulatory compliance
- Missing required disclaimers

**Mitigation:**
- Include explicit disclaimer in all documentation
- Review all documentation for compliance claims
- State clearly: "NOT regulatory-compliant"
- Document in technical writeup

**Residual Risk:** Low (5%) - Clear requirement to avoid compliance claims

---

#### Risk 5.2: Testnet Fund Depletion
**Probability:** Low (15%)  
**Impact:** Low  
**Risk Score:** 30 (Low)

**Description:**
- Sepolia testnet faucet may be slow
- Deployment transactions may require more gas than expected

**Mitigation:**
- Request testnet ETH early (before Phase 1.6)
- Use multiple faucets if needed
- Optimize gas usage
- Use local Hardhat network for development

**Residual Risk:** Low (5%) - Testnet ETH readily available

---

## Risk Summary Table

| Risk Category | Count | Total Risk Score | Avg Risk Score |
|---------------|-------|------------------|----------------|
| Technical Risks | 4 | 530 | 132.5 (Medium) |
| Timeline & Scope Risks | 3 | 310 | 103.3 (Medium) |
| Dependency & Integration Risks | 3 | 205 | 68.3 (Low-Medium) |
| Resource & Knowledge Risks | 2 | 155 | 77.5 (Low-Medium) |
| External & Compliance Risks | 2 | 130 | 65.0 (Low-Medium) |
| **Total** | **14** | **1,330** | **95.0 (Medium)** |

---

## Critical Path Risks

**Top 3 Highest Risk Items:**
1. **Smart Contract Security Vulnerabilities** (160) - Mitigate with OpenZeppelin
2. **Insufficient Time for Comprehensive Testing** (160) - Prioritize required scenarios
3. **Gas Costs Exceeding Targets** (135) - Use virtual split approach

**Focus Areas:**
- Security assessment in Phase 3.6 (mandatory)
- Incremental testing throughout phases
- Gas optimization as separate task

---

## Success Probability Breakdown

### Phase 1: MVP (Nov 4) - 90% Success Probability
**Confidence:** High
- Core requirements are well-defined
- Technology stack is familiar
- OpenZeppelin reduces complexity
- **Key Risk:** Time constraint (2 days)

### Phase 2: Early Submission (Nov 7) - 85% Success Probability
**Confidence:** Medium-High
- Corporate actions add complexity
- Operator demo is straightforward
- **Key Risk:** Virtual split edge cases

### Phase 3: Final Submission (Nov 9) - 85% Success Probability
**Confidence:** Medium-High
- Testing comprehensive but time-constrained
- Documentation requirements are clear
- **Key Risk:** Complete test coverage

---

## Recommended Risk Mitigation Actions

### Immediate Actions (Day 1)
1. ✅ Setup Hardhat project and verify deployment to local network
2. ✅ Test OpenZeppelin contract integration
3. ✅ Request Sepolia testnet ETH early
4. ✅ Setup ethers.js v6 and verify API

### Ongoing Actions (Throughout Project)
1. ✅ Write tests incrementally (after each feature)
2. ✅ Document security considerations as implemented
3. ✅ Run security linters if time permits
4. ✅ Review gas costs after each contract operation

### Final Actions (Phase 3.6)
1. ✅ Comprehensive security assessment (mandatory)
2. ✅ Code refactoring and cleanup
3. ✅ Gas optimization review
4. ✅ Final security review of all contracts

---

## Contingency Plans

### If Time Runs Short
- **Priority 1:** Core gated token contract (required)
- **Priority 2:** Basic issuer service and indexer (required)
- **Priority 3:** One corporate action (split OR symbol change)
- **Priority 4:** Basic CLI demo (skip UI if needed)

### If Security Issues Found
- Document all findings in security assessment
- Prioritize high-severity fixes
- Accept medium-severity issues with documented mitigations
- Defer low-severity issues to "future improvements"

### If Gas Targets Exceeded
- Document actual costs with justification
- Propose optimization strategies
- Accept if reasonable for prototype scope
- Include in technical writeup

---

## Overall Assessment

**Project Feasibility:** High ✅  
**Technical Complexity:** Medium  
**Timeline Adequacy:** Adequate (with prioritization)  
**Resource Availability:** High (all tools/APIs available)

**Key Strengths:**
- Well-defined requirements
- Mature technology stack
- OpenZeppelin reduces security risk
- Clear phase structure

**Key Challenges:**
- Compressed 7-day timeline (was 10 days)
- Comprehensive testing may be tight
- Security assessment must be thorough

**Recommendation:** **Proceed with High Confidence**

The project is highly feasible with 85% success probability. Key risks are manageable through mitigation strategies. Focus on incremental testing and security assessment in Phase 3.6.

---

**Document Version:** 1.0  
**Created:** November 3, 2025  
**Last Updated:** November 3, 2025

```

Creating the remaining risk assessment documents:


```markdown
# Risk Assessment: SpendSense

**Project:** SpendSense - Financial Education System  
**Date:** November 3, 2025  
**Project Start:** November 3, 2025  
**Project End:** November 9, 2025  
**Duration:** 7 Days

---

## **Probability of Success Rating**

### **Overall Probability of Success: 88% (High Confidence)**

**Breakdown:**
- **MVP Success (Nov 4)**: 92% - Data generation and signal detection are straightforward
- **Early Submission Success (Nov 7)**: 88% - Recommendation engine adds complexity
- **Final Submission Success (Nov 9)**: 85% - Evaluation harness may be time-constrained

**Confidence Level:** High  
**Key Success Factors:** Python/Pandas familiar stack, well-defined signal detection logic, clear evaluation metrics  
**Primary Risk:** Recommendation quality and evaluation harness completeness

---

## Risk Analysis

### 1. Technical Risks

#### Risk 1.1: Signal Detection Accuracy Issues
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Subscription detection may have false positives
- Credit utilization calculation may miss edge cases
- Income stability detection may misclassify regular vs. variable income

**Mitigation:**
- Test with known data patterns (Phase 1.3-1.6)
- Tune thresholds based on test data
- Validate with manual review
- Document edge cases and limitations

**Residual Risk:** Low (15%) - Rules-based approach is predictable

---

#### Risk 1.2: Persona Assignment Logic Errors
**Probability:** Low (25%)  
**Impact:** Medium  
**Risk Score:** 80 (Low-Medium)

**Description:**
- Multiple personas may match simultaneously
- Prioritization logic may have bugs
- Custom persona (5th) criteria may conflict

**Mitigation:**
- Document prioritization logic clearly
- Test with users matching multiple personas
- Validate persona assignment with known users
- Accept "first match" if prioritization is ambiguous

**Residual Risk:** Low (10%) - Logic is rule-based and testable

---

#### Risk 1.3: Recommendation Quality Issues
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- Recommendations may not match persona accurately
- Rationales may not cite concrete data points
- Plain language may include jargon unintentionally

**Mitigation:**
- Use structured recommendation templates
- Validate rationales cite specific data
- Run tone checker to catch jargon
- Operator review in Phase 2.6
- Test recommendations for each persona

**Residual Risk:** Medium (25%) - Quality requires careful attention

---

#### Risk 1.4: Evaluation Harness Complexity
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Metrics calculation may be time-consuming
- Fairness analysis may require demographic data
- Report generation may have bugs

**Mitigation:**
- Simplify fairness analysis (basic check if no demographics)
- Prioritize core metrics (coverage, explainability, latency)
- Use templates for report generation
- Accept basic fairness analysis if time-constrained

**Residual Risk:** Medium (20%) - Metrics are well-defined but implementation may be tight

---

### 2. Timeline & Scope Risks

#### Risk 2.1: Synthetic Data Generation Time
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Generating 50-100 realistic users may take time
- Transaction patterns may not be diverse enough

**Mitigation:**
- Use templates for common patterns
- Generate in phases (start with 50, add more if time)
- Reuse patterns across users
- Focus on diversity over realism

**Residual Risk:** Low (10%) - Data generation is straightforward

---

#### Risk 2.2: Recommendation Engine Implementation Time
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Content catalog creation may take time
- Rationale generation may require iteration
- Operator view may need more features than planned

**Mitigation:**
- Use simple rule-based matching (no ML needed)
- Create content templates early (Phase 2.1)
- Prioritize CLI over web UI if time-constrained
- Accept basic operator view

**Residual Risk:** Medium (20%) - Recommendation logic is straightforward

---

#### Risk 2.3: Evaluation Harness Completeness
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- All metrics may not be calculated correctly
- Report generation may be incomplete
- Fairness analysis may be too basic

**Mitigation:**
- Prioritize required metrics (coverage, explainability, latency, auditability)
- Use simple fairness check (if no demographics, document)
- Accept basic report format if time-constrained
- Document limitations in evaluation report

**Residual Risk:** Medium (25%) - Metrics are well-defined

---

### 3. Data & Compliance Risks

#### Risk 3.1: Synthetic Data Unrealistic
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Generated patterns may not reflect real-world behavior
- Edge cases may not be covered

**Mitigation:**
- Use real-world patterns as reference
- Include diverse scenarios (high/low utilization, etc.)
- Document data generation approach
- Accept prototype-quality data (not production)

**Residual Risk:** Low (10%) - Prototype scope allows simplification

---

#### Risk 3.2: Financial Advice Boundary Issues
**Probability:** Low (15%)  
**Impact:** High  
**Risk Score:** 100 (Medium)

**Description:**
- Recommendations may accidentally cross into financial advice
- Disclaimers may be missing or unclear

**Mitigation:**
- Include explicit disclaimer on all recommendations
- Review all recommendations for advice language
- Use educational language only
- Document compliance approach in Phase 3.1

**Residual Risk:** Low (5%) - Clear requirement to avoid financial advice

---

#### Risk 3.3: Consent Management Implementation
**Probability:** Low (20%)  
**Impact:** Medium  
**Risk Score:** 80 (Low-Medium)

**Description:**
- Consent tracking may have bugs
- Consent enforcement may miss edge cases

**Mitigation:**
- Implement simple consent flag in database
- Check consent before all data processing
- Test consent revocation flow
- Document consent flow clearly

**Residual Risk:** Low (10%) - Consent logic is straightforward

---

### 4. Quality & Performance Risks

#### Risk 4.1: Latency Exceeding 5 Seconds
**Probability:** Medium (30%)  
**Impact:** Low  
**Risk Score:** 60 (Low-Medium)

**Description:**
- Feature detection may be slow with large datasets
- Recommendation generation may exceed target

**Mitigation:**
- Optimize database queries (add indexes)
- Cache features where possible
- Profile bottlenecks in Phase 3.6
- Accept slightly higher latency if reasonable

**Residual Risk:** Low (15%) - Dataset is manageable (50-100 users)

---

#### Risk 4.2: Coverage Not Achieving 100%
**Probability:** Low (20%)  
**Impact:** Medium  
**Risk Score:** 70 (Low-Medium)

**Description:**
- Some users may not match any persona
- Some users may not have ≥3 detected behaviors

**Mitigation:**
- Tune persona criteria to be inclusive
- Ensure synthetic data has diverse patterns
- Accept slightly lower coverage if justified
- Document coverage limitations

**Residual Risk:** Low (10%) - Criteria can be adjusted

---

### 5. Integration & Testing Risks

#### Risk 5.1: Guardrails Not Working Correctly
**Probability:** Low (25%)  
**Impact:** High  
**Risk Score:** 100 (Medium)

**Description:**
- Eligibility checking may miss edge cases
- Tone checker may allow shaming language
- Consent enforcement may have bugs

**Mitigation:**
- Test guardrails thoroughly (Phase 2.4-2.5)
- Use simple rule-based checks
- Review recommendations manually
- Test all guardrails in Phase 3.4

**Residual Risk:** Low (15%) - Rules-based guardrails are testable

---

#### Risk 5.2: Operator View Implementation Time
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Web UI may take more time than planned
- CLI may need more features than planned

**Mitigation:**
- Prioritize CLI over web UI
- Use simple CLI interface (basic commands)
- Accept minimal operator view if time-constrained
- Focus on functional over polished

**Residual Risk:** Low (10%) - CLI is faster to build

---

## Risk Summary Table

| Risk Category | Count | Total Risk Score | Avg Risk Score |
|---------------|-------|------------------|----------------|
| Technical Risks | 4 | 480 | 120.0 (Medium) |
| Timeline & Scope Risks | 3 | 295 | 98.3 (Low-Medium) |
| Data & Compliance Risks | 3 | 220 | 73.3 (Low-Medium) |
| Quality & Performance Risks | 2 | 130 | 65.0 (Low-Medium) |
| Integration & Testing Risks | 2 | 140 | 70.0 (Low-Medium) |
| **Total** | **14** | **1,265** | **90.4 (Low-Medium)** |

---

## Critical Path Risks

**Top 3 Highest Risk Items:**
1. **Recommendation Quality Issues** (160) - Mitigate with templates and operator review
2. **Evaluation Harness Completeness** (135) - Prioritize core metrics
3. **Signal Detection Accuracy Issues** (120) - Test thoroughly with known data

**Focus Areas:**
- Recommendation engine quality (Phase 2.2)
- Evaluation harness implementation (Phase 3.1)
- Guardrails testing (Phase 2.4-2.5)

---

## Success Probability Breakdown

### Phase 1: MVP (Nov 4) - 92% Success Probability
**Confidence:** Very High
- Data generation is straightforward
- Signal detection logic is clear
- Persona assignment is rule-based
- **Key Risk:** Signal detection tuning

### Phase 2: Early Submission (Nov 7) - 88% Success Probability
**Confidence:** High
- Recommendation engine requires careful attention
- Guardrails must be thorough
- **Key Risk:** Recommendation quality

### Phase 3: Final Submission (Nov 9) - 85% Success Probability
**Confidence:** Medium-High
- Evaluation harness may be time-constrained
- Custom persona adds complexity
- **Key Risk:** Complete evaluation harness

---

## Recommended Risk Mitigation Actions

### Immediate Actions (Day 1)
1. ✅ Generate synthetic data early (Phase 1.2)
2. ✅ Test signal detection with known patterns
3. ✅ Create content catalog templates
4. ✅ Setup evaluation harness structure

### Ongoing Actions (Throughout Project)
1. ✅ Test persona assignment with diverse users
2. ✅ Review recommendations for quality
3. ✅ Validate guardrails after each implementation
4. ✅ Run evaluation metrics incrementally

### Final Actions (Phase 3.6)
1. ✅ Comprehensive security assessment (data privacy)
2. ✅ Code refactoring and cleanup
3. ✅ Performance optimization review
4. ✅ Final quality review of recommendations

---

## Contingency Plans

### If Time Runs Short
- **Priority 1:** Core signal detection (required)
- **Priority 2:** Basic persona assignment (required)
- **Priority 3:** Simple recommendation engine (basic matching)
- **Priority 4:** Core evaluation metrics only

### If Recommendation Quality Issues
- Use operator review to catch quality issues
- Simplify recommendation matching
- Focus on required personas only
- Document quality limitations

### If Evaluation Harness Incomplete
- Prioritize required metrics (coverage, explainability, latency, auditability)
- Simplify fairness analysis
- Accept basic report format
- Document incomplete metrics

---

## Overall Assessment

**Project Feasibility:** Very High ✅✅  
**Technical Complexity:** Medium  
**Timeline Adequacy:** Adequate (with prioritization)  
**Resource Availability:** High (all tools/libraries available)

**Key Strengths:**
- Well-defined requirements
- Familiar technology stack (Python/Pandas)
- Rule-based logic (predictable)
- Clear evaluation metrics

**Key Challenges:**
- Recommendation quality requires attention
- Evaluation harness may be tight
- Guardrails must be thorough

**Recommendation:** **Proceed with High Confidence**

The project is very feasible with 88% success probability. Key risks are manageable through testing and quality reviews. Focus on recommendation quality and evaluation harness completeness.

---

**Document Version:** 1.0  
**Created:** November 3, 2025  
**Last Updated:** November 3, 2025

```

```markdown
# Risk Assessment: AI Math Tutor

**Project:** AI Math Tutor - Socratic Learning Assistant  
**Date:** November 3, 2025  
**Project Start:** November 3, 2025  
**Project End:** November 9, 2025  
**Duration:** 7 Days

---

## **Probability of Success Rating**

### **Overall Probability of Success: 82% (Medium-High Confidence)**

**Breakdown:**
- **MVP Success (Nov 4)**: 85% - Core chat and Socratic prompting achievable
- **Early Submission Success (Nov 7)**: 82% - Math rendering adds complexity
- **Final Submission Success (Nov 9)**: 80% - Comprehensive testing may be tight

**Confidence Level:** Medium-High  
**Key Success Factors:** Familiar React/TypeScript stack, clear Socratic method requirements, well-documented LLM APIs  
**Primary Risk:** LLM giving direct answers despite Socratic prompt, OCR accuracy issues

---

## Risk Analysis

### 1. Technical Risks

#### Risk 1.1: LLM Gives Direct Answers Despite Socratic Prompt
**Probability:** High (60%)  
**Impact:** High  
**Risk Score:** 240 (High)

**Description:**
- LLM may ignore Socratic prompt and give direct answers
- Prompt engineering may require multiple iterations
- Context may influence LLM to provide answers

**Mitigation:**
- Use strict Socratic system prompt (Phase 1.3)
- Validate responses before sending to user
- Add response filtering (check for direct answers)
- Test thoroughly with multiple problem types
- Iterate on prompt until Socratic method works

**Residual Risk:** Medium (30%) - LLM behavior is somewhat unpredictable

---

#### Risk 1.2: OCR/Vision LLM Parsing Accuracy
**Probability:** Medium (45%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Handwritten text may not parse correctly
- Complex math expressions may be misread
- Blurry images may fail OCR

**Mitigation:**
- Use printed text for initial testing (easier)
- Use OpenAI Vision API (more accurate than Tesseract)
- Pre-process images (enhance contrast if needed)
- Fallback to text input if OCR fails
- Test with various image qualities

**Residual Risk:** Medium (25%) - Vision API is good but not perfect

---

#### Risk 1.3: Math Rendering Issues (LaTeX/KaTeX)
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- LaTeX syntax errors in messages
- KaTeX may not render all expressions correctly
- Mixed text and math parsing may fail

**Mitigation:**
- Validate LaTeX syntax before rendering
- Handle errors gracefully (display raw LaTeX if render fails)
- Test with various math expressions
- Use KaTeX's error handling features
- Document LaTeX syntax limitations

**Residual Risk:** Low (15%) - KaTeX is mature and well-tested

---

#### Risk 1.4: Context Management Token Limits
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Long conversations may exceed LLM token limits
- Context may be truncated, losing important information
- Token costs may be high with full context

**Mitigation:**
- Limit conversation history (last 10-20 messages)
- Summarize older context if needed
- Use GPT-4 Turbo (higher token limit)
- Monitor token usage
- Accept partial context if token limits exceeded

**Residual Risk:** Medium (20%) - Token limits require careful management

---

### 2. Timeline & Scope Risks

#### Risk 2.1: Socratic Prompt Engineering Time
**Probability:** High (55%)  
**Impact:** Medium  
**Risk Score:** 165 (Medium-High)

**Description:**
- Prompt engineering may require multiple iterations
- Testing Socratic method across problem types may take time
- LLM behavior may be inconsistent

**Mitigation:**
- Start with strong Socratic prompt (test early)
- Validate Socratic method before building full UI
- Test with multiple problem types incrementally
- Document prompt iterations
- Accept 80-90% Socratic adherence (not perfect)

**Residual Risk:** Medium (30%) - Prompt engineering is iterative

---

#### Risk 2.2: Image Upload & OCR Integration Time
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Vision API integration may have complexity
- Image processing pipeline may require debugging
- Error handling for failed OCR may take time

**Mitigation:**
- Use OpenAI Vision API (simpler than Tesseract)
- Test image upload early (Phase 1.4)
- Fallback to text input if OCR fails
- Accept basic OCR (printed text only initially)

**Residual Risk:** Medium (25%) - Vision API is well-documented

---

#### Risk 2.3: Math Rendering Implementation Time
**Probability:** Low (25%)  
**Impact:** Low  
**Risk Score:** 50 (Low)

**Description:**
- KaTeX integration may require setup time
- Math parsing from messages may need refinement
- Edge cases in math rendering may need fixing

**Mitigation:**
- Use react-katex library (simple integration)
- Test math rendering early (Phase 2.1)
- Handle errors gracefully
- Accept basic math rendering (common expressions)

**Residual Risk:** Low (10%) - KaTeX is straightforward to integrate

---

#### Risk 2.4: Conversation History Implementation Time
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Database schema for conversations may take time
- Loading conversations from history may have bugs
- Context management with history may be complex

**Mitigation:**
- Use simple database schema (conversations, messages)
- Test conversation loading early (Phase 2.2)
- Accept basic history (load recent conversations only)

**Residual Risk:** Low (10%) - Database storage is straightforward

---

### 3. Quality & Testing Risks

#### Risk 3.1: Socratic Method Not Working Across All Problem Types
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- LLM may give direct answers for some problem types
- Geometry problems may require different prompting
- Word problems may be challenging for Socratic method

**Mitigation:**
- Test with all 5+ problem types early (Phase 1.6)
- Adjust prompt for different problem types if needed
- Use problem-type-specific prompts if necessary
- Accept 80-90% Socratic adherence (not 100%)
- Document problem types where Socratic method works best

**Residual Risk:** Medium (25%) - Some problem types may be harder

---

#### Risk 3.2: Context Not Maintained Across Turns
**Probability:** Medium (30%)  
**Impact:** Medium  
**Risk Score:** 105 (Medium)

**Description:**
- LLM may lose context in long conversations
- Context manager may have bugs
- Token limits may truncate context

**Mitigation:**
- Test context management thoroughly
- Limit conversation history appropriately
- Summarize older context if needed
- Test multi-turn conversations in Phase 1.5

**Residual Risk:** Medium (20%) - Context management requires testing

---

#### Risk 3.3: Comprehensive Testing Time Constraint
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Testing 5+ problem types may take time
- Edge cases may not be covered
- Integration tests may be incomplete

**Mitigation:**
- Prioritize required problem types
- Test incrementally (after each feature)
- Focus on happy path + critical failures
- Accept partial test coverage if time-constrained

**Residual Risk:** Medium (25%) - Testing requires thoroughness

---

### 4. Integration & API Risks

#### Risk 4.1: OpenAI API Rate Limits or Costs
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- API rate limits may slow development
- High API costs may exceed budget
- API downtime may block testing

**Mitigation:**
- Use cheaper models for iteration (GPT-3.5)
- Use GPT-4 Turbo only for final testing
- Cache responses where possible
- Monitor API usage
- Have backup API key if needed

**Residual Risk:** Medium (20%) - API availability is generally reliable

---

#### Risk 4.2: Vision API Processing Time
**Probability:** Low (25%)  
**Impact:** Low  
**Risk Score:** 50 (Low)

**Description:**
- Vision API may be slow for large images
- Multiple images may cause rate limiting

**Mitigation:**
- Compress images before upload
- Limit image size (reasonable limits)
- Show loading state during processing
- Accept processing time (acceptable UX)

**Residual Risk:** Low (10%) - Vision API is generally fast

---

### 5. External & Resource Risks

#### Risk 5.1: Insufficient Time for Stretch Features
**Probability:** High (60%)  
**Impact:** Low  
**Risk Score:** 90 (Low-Medium)

**Description:**
- Whiteboard, step visualization, voice may not be implemented
- Stretch features are optional but high-value

**Mitigation:**
- Prioritize core features first
- Add stretch features only if time permits
- Focus on one stretch feature if time allows
- Accept MVP without stretch features

**Residual Risk:** Low (15%) - Stretch features are optional

---

#### Risk 5.2: Deployment Complexity
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Local deployment may require additional setup
- Production deployment may be complex

**Mitigation:**
- Accept local deployment (clear setup instructions)
- Use Vite dev server for demo
- Document setup thoroughly
- Skip production deployment if time-constrained

**Residual Risk:** Low (5%) - Local deployment is acceptable

---

## Risk Summary Table

| Risk Category | Count | Total Risk Score | Avg Risk Score |
|---------------|-------|------------------|----------------|
| Technical Risks | 4 | 615 | 153.8 (Medium-High) |
| Timeline & Scope Risks | 4 | 390 | 97.5 (Low-Medium) |
| Quality & Testing Risks | 3 | 400 | 133.3 (Medium) |
| Integration & API Risks | 2 | 170 | 85.0 (Low-Medium) |
| External & Resource Risks | 2 | 130 | 65.0 (Low-Medium) |
| **Total** | **15** | **1,705** | **113.7 (Medium)** |

---

## Critical Path Risks

**Top 3 Highest Risk Items:**
1. **LLM Gives Direct Answers Despite Socratic Prompt** (240) - Mitigate with strict prompts and validation
2. **Socratic Prompt Engineering Time** (165) - Start early, iterate quickly
3. **Socratic Method Not Working Across All Problem Types** (160) - Test thoroughly, adjust prompts

**Focus Areas:**
- Socratic prompt engineering (Phase 1.3 - critical)
- Response validation (prevent direct answers)
- Comprehensive testing across problem types (Phase 3.1)

---

## Success Probability Breakdown

### Phase 1: MVP (Nov 4) - 85% Success Probability
**Confidence:** Medium-High
- Core chat UI is straightforward
- LLM integration is well-documented
- Socratic prompting requires careful attention
- **Key Risk:** LLM giving direct answers

### Phase 2: Early Submission (Nov 7) - 82% Success Probability
**Confidence:** Medium-High
- Math rendering adds complexity
- Conversation history is straightforward
- **Key Risk:** Math parsing and rendering edge cases

### Phase 3: Final Submission (Nov 9) - 80% Success Probability
**Confidence:** Medium
- Comprehensive testing may be tight
- Documentation requirements are clear
- **Key Risk:** Testing all problem types thoroughly

---

## Recommended Risk Mitigation Actions

### Immediate Actions (Day 1)
1. ✅ Test Socratic prompt with LLM early (before building UI)
2. ✅ Validate Socratic method works with hardcoded problem
3. ✅ Test image upload and Vision API integration
4. ✅ Setup KaTeX integration and test basic rendering

### Ongoing Actions (Throughout Project)
1. ✅ Test Socratic method after each prompt change
2. ✅ Validate responses don't contain direct answers
3. ✅ Test context management with multi-turn conversations
4. ✅ Test math rendering with various expressions

### Final Actions (Phase 3.6)
1. ✅ Comprehensive security assessment (API key security)
2. ✅ Code refactoring and cleanup
3. ✅ Performance optimization review
4. ✅ Final Socratic method validation across all problem types

---

## Contingency Plans

### If LLM Gives Direct Answers
- Use stricter Socratic prompt
- Add response filtering (check for answer patterns)
- Post-process LLM responses to remove answers
- Accept 80-90% Socratic adherence (not perfect)
- Document prompt engineering in notes

### If OCR Fails Consistently
- Fallback to text input only
- Accept printed text only (not handwritten)
- Document OCR limitations
- Focus on text input for demo

### If Time Runs Short
- **Priority 1:** Core chat and Socratic prompting (required)
- **Priority 2:** Basic image upload (OCR optional)
- **Priority 3:** Math rendering (basic expressions only)
- **Priority 4:** Conversation history (basic loading)

---

## Overall Assessment

**Project Feasibility:** High ✅  
**Technical Complexity:** Medium-High  
**Timeline Adequacy:** Adequate (with prioritization)  
**Resource Availability:** High (all APIs/tools available)

**Key Strengths:**
- Well-defined requirements
- Familiar React/TypeScript stack
- Clear Socratic method requirements
- Well-documented LLM APIs

**Key Challenges:**
- LLM behavior is somewhat unpredictable
- Socratic prompt engineering is iterative
- OCR accuracy may vary
- Testing across problem types requires time

**Recommendation:** **Proceed with Medium-High Confidence**

The project is feasible with 82% success probability. Key risk is LLM giving direct answers - mitigate with strict prompts and validation. Start Socratic prompt engineering early and test thoroughly across problem types.

---

**Document Version:** 1.0  
**Created:** November 3, 2025  
**Last Updated:** November 3, 2025

```

Creating the remaining two risk assessment documents:


```markdown
# Risk Assessment: DreamUp Browser Game QA Pipeline

**Project:** DreamUp - Autonomous Browser Game QA Agent  
**Date:** November 3, 2025  
**Project Start:** November 3, 2025  
**Project End:** November 9, 2025  
**Duration:** 7 Days

---

## **Probability of Success Rating**

### **Overall Probability of Success: 80% (Medium-High Confidence)**

**Breakdown:**
- **MVP Success (Nov 4)**: 85% - Basic automation and evidence capture achievable
- **Early Submission Success (Nov 7)**: 80% - AI evaluation adds complexity
- **Final Submission Success (Nov 9)**: 75% - Comprehensive testing may be tight

**Confidence Level:** Medium-High  
**Key Success Factors:** TypeScript familiarity, Browserbase free tier available, well-defined agent flow  
**Primary Risk:** Browser automation reliability, AI evaluation accuracy, time constraints

---

## Risk Analysis

### 1. Technical Risks

#### Risk 1.1: Agent Loops Infinitely
**Probability:** Medium (45%)  
**Impact:** High  
**Risk Score:** 180 (Medium-High)

**Description:**
- Agent may get stuck in interaction loops
- UI element detection may fail, causing retries
- Game state may not change as expected

**Mitigation:**
- Implement max action count (e.g., 50 actions)
- Set total timeout (5 minutes per game)
- Add action history tracking (prevent loops)
- Detect stuck states (screenshot comparison)
- Test with various game types early

**Residual Risk:** Medium (25%) - Automation reliability requires careful logic

---

#### Risk 1.2: LLM Evaluation Inconsistency
**Probability:** High (55%)  
**Impact:** Medium  
**Risk Score:** 165 (Medium-High)

**Description:**
- LLM may give inconsistent playability assessments
- Confidence scores may be unreliable
- Different LLM calls may produce different results

**Mitigation:**
- Use structured prompts with specific questions
- Set confidence thresholds (accept if >0.7)
- Use fallback heuristics if LLM fails
- Test with known good/bad games
- Cache LLM responses if possible

**Residual Risk:** Medium (30%) - LLM behavior is somewhat unpredictable

---

#### Risk 1.3: Games Don't Load in Headless Mode
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- Some games may require user interaction before loading
- Headless browser may not render games correctly
- Games may detect headless mode and fail

**Mitigation:**
- Test with headed mode option (if Browserbase supports)
- Use screenshot comparison to detect failures
- Retry failed loads up to 3 times
- Document games that don't work in headless
- Accept that some games may not be testable

**Residual Risk:** Medium (25%) - Not all games work in headless mode

---

#### Risk 1.4: UI Element Detection Failures
**Probability:** Medium (45%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Start/play buttons may not be detected correctly
- Selectors may be too specific or too generic
- Games with custom UI may not match patterns

**Mitigation:**
- Use multiple selector strategies (text, class, id)
- Use Stagehand's element detection (if available)
- Test with various game types
- Implement fallback selectors
- Accept partial interaction if detection fails

**Residual Risk:** Medium (25%) - Element detection requires robust strategies

---

### 2. Timeline & Scope Risks

#### Risk 2.1: Browserbase Setup and Learning Curve
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Browserbase API may require learning time
- Stagehand library may have documentation gaps
- Integration may have unexpected complexity

**Mitigation:**
- Start with Browserbase's free tier early (Phase 1.2)
- Test basic browser automation immediately
- Use official documentation and examples
- Have Puppeteer as backup (if needed)
- Accept basic automation if advanced features fail

**Residual Risk:** Medium (20%) - Browserbase should be straightforward

---

#### Risk 2.2: AI Evaluation Integration Time
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- LLM evaluation prompt engineering may take time
- Screenshot analysis may require multiple iterations
- Structured output formatting may need refinement

**Mitigation:**
- Start with simple evaluation prompts
- Test LLM evaluation early (mock if needed)
- Use structured output templates
- Iterate on prompts based on test results
- Accept basic evaluation if time-constrained

**Residual Risk:** Medium (25%) - LLM integration requires iteration

---

#### Risk 2.3: Error Handling and Robustness Time
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Comprehensive error handling may take time
- Edge case handling may be extensive
- Recovery logic may need refinement

**Mitigation:**
- Prioritize critical error cases (crashes, timeouts)
- Implement graceful degradation early
- Accept basic error handling if time-constrained
- Document known limitations

**Residual Risk:** Medium (20%) - Core error handling is manageable

---

#### Risk 2.4: Testing on Diverse Game Types Time
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Finding suitable test games may take time
- Testing 3-5 different game types may be time-consuming
- Each game type may require different strategies

**Mitigation:**
- Find test games early (itch.io/html5)
- Use simple test games for initial testing
- Create test cases for each game type
- Accept partial testing if time-constrained

**Residual Risk:** Medium (25%) - Testing requires time investment

---

### 3. Integration & API Risks

#### Risk 3.1: Browserbase API Rate Limits or Downtime
**Probability:** Low (25%)  
**Impact:** Medium  
**Risk Score:** 75 (Low-Medium)

**Description:**
- Free tier may have rate limits
- API may be slow during peak times
- Browserbase service may experience downtime

**Mitigation:**
- Use free tier efficiently (1 browser-hour)
- Test locally with Puppeteer as backup
- Cache browser sessions if possible
- Have alternative automation library ready

**Residual Risk:** Low (15%) - Free tier should be sufficient

---

#### Risk 3.2: LLM API Costs or Rate Limits
**Probability:** Medium (35%)  
**Impact:** Low  
**Risk Score:** 70 (Low-Medium)

**Description:**
- Vision API calls may be expensive
- Rate limits may slow development
- API costs may exceed budget

**Mitigation:**
- Use cheaper models for iteration
- Use GPT-4 Vision only for final testing
- Cache evaluation results if possible
- Limit screenshot count per game

**Residual Risk:** Medium (20%) - API costs manageable with planning

---

#### Risk 3.3: Screenshot Storage and Processing
**Probability:** Low (20%)  
**Impact:** Low  
**Risk Score:** 40 (Low)

**Description:**
- Screenshots may be large files
- Storage may be limited
- Processing may be slow

**Mitigation:**
- Compress screenshots if needed
- Limit screenshot count (3-5 per game)
- Use efficient image formats (PNG)
- Clean up old screenshots periodically

**Residual Risk:** Low (5%) - Storage is manageable

---

### 4. Quality & Accuracy Risks

#### Risk 4.1: AI Evaluation Accuracy <80%
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- LLM may misclassify playable vs. broken games
- Confidence scores may not correlate with accuracy
- Evaluation may be inconsistent across games

**Mitigation:**
- Test with known good/bad games
- Tune evaluation prompts based on results
- Use structured prompts with specific criteria
- Accept 70-80% accuracy if reasonable
- Document accuracy limitations

**Residual Risk:** Medium (25%) - Accuracy depends on prompt quality

---

#### Risk 4.2: Interaction Detection False Positives/Negatives
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Agent may click wrong elements (false positive)
- Agent may miss interactive elements (false negative)
- Game may respond differently than expected

**Mitigation:**
- Use multiple selector strategies
- Verify element visibility before clicking
- Test interaction detection thoroughly
- Accept partial interaction if detection fails

**Residual Risk:** Medium (20%) - Detection requires robust logic

---

### 5. Resource & External Risks

#### Risk 5.1: Finding Suitable Test Games
**Probability:** Low (25%)  
**Impact:** Low  
**Risk Score:** 50 (Low)

**Description:**
- HTML5 games on itch.io may not be suitable
- Games may require login or payment
- Games may not work in headless mode

**Mitigation:**
- Find test games early (Phase 1.6)
- Use simple test games (tic-tac-toe, etc.)
- Create simple test games if needed
- Document test game selection

**Residual Risk:** Low (10%) - Multiple sources available

---

#### Risk 5.2: Scope Creep (Adding Too Many Features)
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Batch testing may be tempting but time-consuming
- GIF recording may take significant time
- Web dashboard may expand scope

**Mitigation:**
- Strict adherence to 3-5 day core
- No stretch features until core complete
- Prioritize required features only
- Document deferred features clearly

**Residual Risk:** Medium (25%) - Scope discipline required

---

## Risk Summary Table

| Risk Category | Count | Total Risk Score | Avg Risk Score |
|---------------|-------|------------------|----------------|
| Technical Risks | 4 | 640 | 160.0 (Medium-High) |
| Timeline & Scope Risks | 4 | 510 | 127.5 (Medium) |
| Integration & API Risks | 3 | 185 | 61.7 (Low-Medium) |
| Quality & Accuracy Risks | 2 | 240 | 120.0 (Medium) |
| Resource & External Risks | 2 | 170 | 85.0 (Low-Medium) |
| **Total** | **15** | **1,745** | **116.3 (Medium)** |

---

## Critical Path Risks

**Top 3 Highest Risk Items:**
1. **Agent Loops Infinitely** (180) - Mitigate with max action count and timeouts
2. **LLM Evaluation Inconsistency** (165) - Use structured prompts and thresholds
3. **Games Don't Load in Headless Mode** (160) - Test early, accept limitations

**Focus Areas:**
- Agent loop prevention (Phase 2.1 - critical)
- LLM evaluation prompt engineering (Phase 2.3)
- Error handling and robustness (Phase 3.2)

---

## Success Probability Breakdown

### Phase 1: MVP (Nov 4) - 85% Success Probability
**Confidence:** Medium-High
- Basic automation is achievable
- Screenshot capture is straightforward
- Browserbase free tier should work
- **Key Risk:** Browser automation reliability

### Phase 2: Early Submission (Nov 7) - 80% Success Probability
**Confidence:** Medium
- Interaction system adds complexity
- AI evaluation requires prompt engineering
- **Key Risk:** LLM evaluation accuracy

### Phase 3: Final Submission (Nov 9) - 75% Success Probability
**Confidence:** Medium
- Comprehensive testing may be tight
- Error handling needs thoroughness
- **Key Risk:** Testing diverse game types thoroughly

---

## Recommended Risk Mitigation Actions

### Immediate Actions (Day 1)
1. ✅ Setup Browserbase account and test basic automation
2. ✅ Test with simple game (tic-tac-toe or similar)
3. ✅ Validate screenshot capture works
4. ✅ Test LLM evaluation with mock screenshots

### Ongoing Actions (Throughout Project)
1. ✅ Test agent with various game types incrementally
2. ✅ Monitor for infinite loops (add logging)
3. ✅ Tune LLM evaluation prompts based on results
4. ✅ Test error handling after each feature

### Final Actions (Phase 3.6)
1. ✅ Comprehensive security assessment (API key security)
2. ✅ Code refactoring and cleanup
3. ✅ Performance optimization review
4. ✅ Final testing on 3-5 diverse game types

---

## Contingency Plans

### If Agent Loops Infinitely
- Add max action count (50 actions)
- Add total timeout (5 minutes)
- Detect stuck states with screenshot comparison
- Break loop if no state change detected

### If LLM Evaluation Inconsistent
- Use structured prompts with specific questions
- Set confidence thresholds (accept if >0.7)
- Use fallback heuristics
- Accept 70-80% accuracy if reasonable

### If Games Don't Load in Headless Mode
- Test with headed mode option (if available)
- Document games that don't work
- Accept that some games may not be testable
- Focus on games that work in headless

### If Time Runs Short
- **Priority 1:** Basic automation and screenshots (required)
- **Priority 2:** Simple interaction (clicks only)
- **Priority 3:** Basic LLM evaluation (simple prompts)
- **Priority 4:** Error handling (critical cases only)

---

## Overall Assessment

**Project Feasibility:** Medium-High ✅  
**Technical Complexity:** Medium-High  
**Timeline Adequacy:** Adequate (with prioritization)  
**Resource Availability:** High (all tools/APIs available)

**Key Strengths:**
- Well-defined requirements
- TypeScript familiarity
- Browserbase free tier available
- Clear agent flow defined

**Key Challenges:**
- Browser automation reliability
- LLM evaluation accuracy
- Time constraints for comprehensive testing
- Agent loop prevention requires careful logic

**Recommendation:** **Proceed with Medium-High Confidence**

The project is feasible with 80% success probability. Key risks are manageable through careful agent design and prompt engineering. Focus on preventing infinite loops and ensuring LLM evaluation accuracy.

---

**Document Version:** 1.0  
**Created:** November 3, 2025  
**Last Updated:** November 3, 2025

```

```markdown
# Risk Assessment: Varsity Tutors 10x K Factor

**Project:** Varsity Tutors - Viral Growth System  
**Date:** November 3, 2025  
**Project Start:** November 3, 2025  
**Project End:** November 9, 2025  
**Duration:** 7 Days

---

## **Probability of Success Rating**

### **Overall Probability of Success: 75% (Medium Confidence)**

**Breakdown:**
- **MVP Success (Nov 4)**: 80% - Core viral loops achievable but complex
- **Early Submission Success (Nov 7)**: 75% - Agentic actions add significant complexity
- **Final Submission Success (Nov 9)**: 70% - Compliance and analytics may be tight

**Confidence Level:** Medium  
**Key Success Factors:** Well-defined requirements, clear viral loop structure, multiple agent architecture  
**Primary Risk:** Agent complexity, K-factor measurement, compliance requirements, time constraints

---

## Risk Analysis

### 1. Technical Risks

#### Risk 1.1: Agent Architecture Complexity
**Probability:** High (65%)  
**Impact:** High  
**Risk Score:** 260 (High)

**Description:**
- 7+ agents (Orchestrator, Personalization, Incentives, Social Presence, Tutor Advocacy, Trust & Safety, Experimentation)
- MCP server pattern may require learning curve
- Agent coordination may have bugs
- Agent decision rationale logging adds complexity

**Mitigation:**
- Simplify agent architecture if needed (stub agents if necessary)
- Use custom agent framework (simpler than full MCP)
- Test agents incrementally (one at a time)
- Accept basic agent stubs for MVP if time-constrained
- Document agent architecture clearly

**Residual Risk:** High (35%) - Multiple agents require careful design

---

#### Risk 1.2: Session Transcription and Summarization Quality
**Probability:** Medium (45%)  
**Impact:** High  
**Risk Score:** 180 (Medium-High)

**Description:**
- Transcription accuracy may vary
- Summarization may miss key information (skill gaps, exam dates)
- Agentic action triggers may be unreliable
- Integration with viral loops may have bugs

**Mitigation:**
- Use OpenAI Whisper API (high accuracy)
- Use GPT-4 for summarization (better quality)
- Test transcription with known audio samples
- Validate summarization extracts key information
- Accept 80-90% accuracy if reasonable

**Residual Risk:** Medium (30%) - LLM quality is generally good

---

#### Risk 1.3: Smart Links and Attribution Complexity
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Signed smart links require cryptography
- Attribution tracking (last-touch, multi-touch) is complex
- Cross-device continuity adds complexity
- UTM parameter handling may have bugs

**Mitigation:**
- Use simple JWT signing for smart links
- Implement basic last-touch attribution (simpler)
- Defer multi-touch to "future improvements"
- Test attribution tracking thoroughly
- Accept basic attribution if time-constrained

**Residual Risk:** Medium (25%) - Attribution requires careful implementation

---

#### Risk 1.4: Real-time Presence and Activity Feed Performance
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Socket.io may have scalability issues
- Presence updates may be too frequent
- Activity feed may be slow with many events
- Real-time updates may exceed 50 events/sec

**Mitigation:**
- Use Socket.io efficiently (batch updates if needed)
- Limit presence update frequency (e.g., every 30 seconds)
- Optimize activity feed queries
- Accept reduced real-time performance if needed
- Document performance limitations

**Residual Risk:** Medium (20%) - Real-time requires careful optimization

---

### 2. Timeline & Scope Risks

#### Risk 2.1: Insufficient Time for All Viral Loops
**Probability:** High (60%)  
**Impact:** High  
**Risk Score:** 240 (High)

**Description:**
- ≥4 viral loops required (Buddy Challenge, Results Rally, Proud Parent, Tutor Spotlight)
- Each loop requires backend logic, UI, testing
- Integration between loops may take time
- Loop orchestration adds complexity

**Mitigation:**
- Prioritize 4 core loops (required minimum)
- Simplify loop implementation (MVP approach)
- Test loops incrementally (one at a time)
- Accept basic loop functionality if time-constrained
- Document loop limitations

**Residual Risk:** High (30%) - 4 loops require significant time

---

#### Risk 2.2: Agentic Actions Implementation Time
**Probability:** High (55%)  
**Impact:** High  
**Risk Score:** 220 (High)

**Description:**
- ≥4 agentic actions required (≥2 student, ≥2 tutor)
- Each action requires transcription → summarization → trigger → viral loop
- Integration complexity is high
- Testing each action flow takes time

**Mitigation:**
- Implement actions incrementally
- Use simple action triggers (keyword-based if needed)
- Accept basic actions (not perfect) if time-constrained
- Test actions one at a time
- Document action limitations

**Residual Risk:** High (30%) - Agentic actions are complex

---

#### Risk 2.3: K-Factor Measurement and Analytics Complexity
**Probability:** Medium (45%)  
**Impact:** High  
**Risk Score:** 180 (Medium-High)

**Description:**
- K-factor calculation requires accurate event tracking
- Attribution logic may be incomplete
- Analytics dashboard may be time-consuming
- Event schema design and implementation takes time

**Mitigation:**
- Implement basic event tracking early (Phase 2.4)
- Simplify K-factor calculation (basic formula)
- Use simple dashboard (charts.js or similar)
- Accept basic analytics if time-constrained
- Document analytics limitations

**Residual Risk:** Medium (30%) - Analytics requires careful implementation

---

#### Risk 2.4: Compliance Memo and COPPA/FERPA Implementation
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- Compliance requirements may be unclear
- Parental gating implementation may be complex
- Consent flows may require multiple iterations
- Compliance memo may require legal review

**Mitigation:**
- Use clear consent UI (explicit opt-in)
- Implement simple parental gating (age check)
- Document compliance approach clearly
- Accept basic compliance (prototype scope)
- Note that compliance memo is advisory, not legal advice

**Residual Risk:** Medium (25%) - Compliance requires careful attention

---

### 3. Integration & Complexity Risks

#### Risk 3.1: Multi-Agent Coordination Bugs
**Probability:** High (60%)  
**Impact:** High  
**Risk Score:** 240 (High)

**Description:**
- Agents may not communicate correctly
- Decision conflicts may occur
- Agent rationale logging may have bugs
- <150ms decision SLA may be difficult to achieve

**Mitigation:**
- Use simple agent coordination (sequential if needed)
- Test agent integration incrementally
- Accept longer decision times if needed (<500ms acceptable)
- Document agent coordination approach
- Stub agents if full implementation is too complex

**Residual Risk:** High (35%) - Multi-agent coordination is complex

---

#### Risk 3.2: Viral Loop Integration with Agentic Actions
**Probability:** High (55%)  
**Impact:** High  
**Risk Score:** 220 (High)

**Description:**
- Agentic actions must trigger viral loops correctly
- Loop eligibility checking may conflict with actions
- Integration testing may reveal bugs
- End-to-end flow may be fragile

**Mitigation:**
- Test integration early (Phase 2.2)
- Implement integration incrementally
- Test each action → loop flow separately
- Accept basic integration if time-constrained
- Document integration limitations

**Residual Risk:** High (30%) - Integration is complex

---

#### Risk 3.3: Results-Page Sharing Integration
**Probability:** Medium (40%)  
**Impact:** Medium  
**Risk Score:** 135 (Medium)

**Description:**
- Share card generation may be time-consuming
- Deep link integration may have bugs
- Cohort challenge implementation may be complex
- Integration with existing results pages may require changes

**Mitigation:**
- Simplify share card generation (basic templates)
- Use simple deep link format
- Accept basic sharing if time-constrained
- Document integration approach

**Residual Risk:** Medium (25%) - Sharing requires integration work

---

### 4. Quality & Performance Risks

#### Risk 4.1: K-Factor < 1.20 Target
**Probability:** High (60%)  
**Impact:** Medium  
**Risk Score:** 180 (Medium-High)

**Description:**
- Viral loops may not generate enough invites
- Invite conversion rate may be low
- K-factor calculation may be inaccurate
- Target K ≥ 1.20 may be difficult to achieve

**Mitigation:**
- Optimize loop personalization
- Test with seeded cohort early
- Accept lower K if justified (document reasons)
- Focus on one loop achieving K ≥ 1.20
- Document K-factor calculation clearly

**Residual Risk:** Medium (35%) - K-factor depends on loop quality

---

#### Risk 4.2: Presence UI Performance Issues
**Probability:** Low (25%)  
**Impact:** Low  
**Risk Score:** 50 (Low)

**Description:**
- Real-time updates may be slow
- Presence indicators may lag
- Activity feed may not update in real-time

**Mitigation:**
- Optimize Socket.io updates
- Batch updates if needed
- Accept periodic updates (every 30 seconds)
- Document performance limitations

**Residual Risk:** Low (10%) - Performance is manageable

---

### 5. Compliance & Legal Risks

#### Risk 5.1: COPPA/FERPA Compliance Gaps
**Probability:** Medium (40%)  
**Impact:** High  
**Risk Score:** 160 (Medium-High)

**Description:**
- Parental gating may miss edge cases
- Child data segregation may have bugs
- Consent flows may not meet requirements
- Compliance memo may be incomplete

**Mitigation:**
- Implement clear parental gating (age <13 check)
- Segregate child data in database
- Use explicit consent UI
- Document compliance approach thoroughly
- Note that compliance memo is prototype, not legal advice

**Residual Risk:** Medium (25%) - Compliance requires careful implementation

---

#### Risk 5.2: Trust & Safety Agent Effectiveness
**Probability:** Medium (35%)  
**Impact:** Medium  
**Risk Score:** 120 (Medium)

**Description:**
- Fraud detection may have false positives/negatives
- Rate limiting may be too restrictive or too loose
- Abuse detection may miss patterns

**Mitigation:**
- Use simple rule-based fraud detection
- Test rate limiting thresholds
- Document abuse patterns
- Accept basic trust & safety if time-constrained

**Residual Risk:** Medium (20%) - Trust & safety requires testing

---

## Risk Summary Table

| Risk Category | Count | Total Risk Score | Avg Risk Score |
|---------------|-------|------------------|----------------|
| Technical Risks | 4 | 695 | 173.8 (High) |
| Timeline & Scope Risks | 4 | 800 | 200.0 (High) |
| Integration & Complexity Risks | 3 | 595 | 198.3 (High) |
| Quality & Performance Risks | 2 | 230 | 115.0 (Medium) |
| Compliance & Legal Risks | 2 | 280 | 140.0 (Medium-High) |
| **Total** | **15** | **2,600** | **173.3 (High)** |

---

## Critical Path Risks

**Top 3 Highest Risk Items:**
1. **Agent Architecture Complexity** (260) - Mitigate by simplifying or stubbing agents
2. **Insufficient Time for All Viral Loops** (240) - Prioritize 4 core loops
3. **Multi-Agent Coordination Bugs** (240) - Test incrementally, use simple coordination

**Focus Areas:**
- Agent architecture simplification (Phase 1.2 - critical)
- Viral loop prioritization (Phase 1.4-1.7)
- Integration testing (Phase 2.5, 3.4)

---

## Success Probability Breakdown

### Phase 1: MVP (Nov 4) - 80% Success Probability
**Confidence:** Medium
- Core viral loops are achievable but require time
- Basic agents can be stubbed if needed
- Presence layer is straightforward
- **Key Risk:** Completing all 4 viral loops

### Phase 2: Early Submission (Nov 7) - 75% Success Probability
**Confidence:** Medium
- Agentic actions add significant complexity
- Smart links require careful implementation
- Analytics dashboard takes time
- **Key Risk:** Agentic action quality and integration

### Phase 3: Final Submission (Nov 9) - 70% Success Probability
**Confidence:** Medium-Low
- Compliance memo requires thoroughness
- Comprehensive testing may be tight
- K-factor measurement may be incomplete
- **Key Risk:** Completing all requirements on time

---

## Recommended Risk Mitigation Actions

### Immediate Actions (Day 1)
1. ✅ Simplify agent architecture (use stubs if needed)
2. ✅ Prioritize 4 core viral loops
3. ✅ Test basic agent coordination
4. ✅ Setup Socket.io for presence early

### Ongoing Actions (Throughout Project)
1. ✅ Test agents incrementally (one at a time)
2. ✅ Test viral loops independently
3. ✅ Validate agentic actions trigger loops correctly
4. ✅ Monitor K-factor calculation early

### Final Actions (Phase 3.6)
1. ✅ Comprehensive security assessment (COPPA/FERPA compliance)
2. ✅ Code refactoring and cleanup
3. ✅ Performance optimization review
4. ✅ Final compliance review

---

## Contingency Plans

### If Agents Are Too Complex
- Use simplified agent architecture
- Stub non-critical agents (basic implementations)
- Accept basic agent logic (not full MCP)
- Document agent limitations
- Focus on required agents only

### If Viral Loops Take Too Much Time
- Prioritize 4 core loops (required minimum)
- Simplify loop implementation (MVP approach)
- Accept basic loop functionality
- Document incomplete loops
- Focus on one high-quality loop over multiple incomplete ones

### If K-Factor < 1.20
- Optimize personalization for one loop
- Test with seeded cohort early
- Accept lower K if justified (document reasons)
- Focus on one loop achieving target
- Document K-factor calculation and limitations

### If Time Runs Short
- **Priority 1:** ≥4 viral loops (required)
- **Priority 2:** Basic agents (stubs acceptable)
- **Priority 3:** ≥4 agentic actions (basic triggers acceptable)
- **Priority 4:** Basic analytics (K-factor calculation required)

---

## Overall Assessment

**Project Feasibility:** Medium ⚠️  
**Technical Complexity:** High  
**Timeline Adequacy:** Tight (7 days is ambitious)  
**Resource Availability:** High (all tools/APIs available)

**Key Strengths:**
- Well-defined requirements
- Clear viral loop structure
- Familiar React/TypeScript stack

**Key Challenges:**
- Multiple agents require careful coordination
- 4+ viral loops require significant implementation time
- Agentic actions add complexity
- Compliance requirements require thoroughness
- K-factor target (≥1.20) may be difficult to achieve

**Recommendation:** **Proceed with Medium Confidence**

The project is feasible with 75% success probability. Key risk is complexity and time constraints. **Strongly recommend simplifying agent architecture and prioritizing core viral loops.** Consider using agent stubs if full implementation is too complex.

---

**Document Version:** 1.0  
**Created:** November 3, 2025  
**Last Updated:** November 3, 2025

```

Completed updates to all 5 PRDs and created risk assessment documents. Summary:

## Updates completed

### 1. PRD updates (all 5 projects)
- Updated start date: November 3, 2025 (was October 31)
- Updated duration: 7 Days (Nov 3 - Nov 9)
- Added "Code Evaluation, Refactoring & Security Assessment" task (Phase 3.6) before final testing in each PRD
- Updated document versions to 1.1

### 2. Risk assessment documents created

Created risk assessment documents for all 5 projects:

1. RISK-Peak6-ChainEquity.md — 85% success probability
2. RISK-Peak6-SpendSense.md — 88% success probability
3. RISK-SuperBuilders-AI_Math_Tutor.md — 82% success probability
4. RISK-SuperBuilders-DreamUp.md — 80% success probability
5. RISK-Varsity_Tutors-10x_K_Factor.md — 75% success probability

### 3. Risk assessment features

Each document includes:
- Probability of Success Rating highlighted on page 1
- Breakdown by phase (MVP, Early Submission, Final)
- Risk analysis with probability, impact, and risk scores
- Mitigation strategies for each risk
- Risk summary tables
- Critical path risks (top 3)
- Success probability breakdown
- Recommended mitigation actions
- Contingency plans
- Overall assessment and recommendation

### 4. Code evaluation/security assessment task

Added as Phase 3.6 in each PRD with:
- Code evaluation and refactoring (45 minutes)
- Security assessment (45 minutes)
- Project-specific security checks (smart contracts, data privacy, API keys, etc.)

All documents are ready and aligned with your success path methodology. Each risk assessment includes a probability of success rating prominently displayed on page 1.

